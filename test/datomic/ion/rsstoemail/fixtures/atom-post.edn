{:tag :rss, :attrs {:xmlns:atom "http://www.w3.org/2005/Atom", :version "2.0"}, :content [{:tag :channel, :attrs nil, :content [{:tag :title, :attrs nil, :content nil} {:tag :link, :attrs nil, :content ["https://danluu.com/atom/index.xml"]} {:tag :description, :attrs nil, :content ["Recent content on "]} {:tag :generator, :attrs nil, :content ["Hugo -- gohugo.io"]} {:tag :language, :attrs nil, :content ["en-us"]} {:tag :lastBuildDate, :attrs nil, :content ["Mon, 22 Nov 2021 00:00:00 +0000"]} {:tag :atom:link, :attrs {:type "application/rss+xml", :rel "self", :href "https://danluu.com/atom/index.xml"}, :content nil} {:tag :item, :attrs nil, :content [{:tag :title, :attrs nil, :content ["Major errors on this blog (and their corrections)"]} {:tag :link, :attrs nil, :content ["https://danluu.com/corrections/"]} {:tag :pubDate, :attrs nil, :content ["Mon, 22 Nov 2021 00:00:00 +0000"]} {:tag :guid, :attrs nil, :content ["https://danluu.com/corrections/"]} {:tag :description, :attrs nil, :content ["<p>Here's a list of errors on this blog that I think were fairly serious. While what I think of as serious is, of course, subjective, I don't think there's any reasonable way to avoid that because, e.g., I make a huge number of typos, so many that the majority of acknowledgements on many posts are for people who e-mailed or DM'ed me typo fixes.</p>\n\n<p>A list that included everything, including typos would both be uninteresting for other people to read as well as high overhead for me, which is why I've drawn the line somewhere. An example of an error I don't think of as serious is, <a href=\"https://danluu.com/learning-to-program/\">in this post on how I learned to program</a>, I originally had the dates wrong on when the competition programmers from my high school made money (it was a couple years after I thought it was). In that case, and many others, I don't think that the date being wrong changes anything significant about the post.</p>\n\n<p>Although I'm publishing the original version of this in 2021, I expect this list to grow over time. I hope that I've become more careful and that the list will grow more slowly in the future than it has in the past, but that remains to be seen. I view it as a good sign that a large fraction of the list is from my first three months of blogging, in 2013, but that's no reason to get complacent!</p>\n\n<p>I've added a classification below that's how I think of the errors, but that classification is also arbitrary and the categories aren't even mutually exclusive. If I ever collect enough of these that it's difficult to hold them all in my head at once, I might create a tag system and use that to classify them instead, but I hope to not accumulate so many major errors that I feel like I need a tag system for readers to easily peruse them.</p>\n\n<ul>\n<li>Insufficient thought\n\n<ul>\n<li><i>2013</i>: <a href=\"https://danluu.com/randomize-hn/\">Using random algorithms to decrease the probability that good stories get &quot;unlucky&quot; on HN</a>: this idea was tried and didn't work well as well as putting humans in the loop who decide which stories should be rescued from oblivion.\n\n<ul>\n<li>Since this was a proposal and not a claim, this technically wasn't an error since I didn't claim that this would definitely work, but my feeling is that I should've also considered solutions that put humans in the loop. I didn't because Digg famously got a lot of backlash for having humans influence their front page but, in retrospect, we can see that it's possible to do so in a way that doesn't generate backlash that effective kills the site and I think this could've been predicted with enough thought</li>\n</ul></li>\n</ul></li>\n<li>Naivete\n\n<ul>\n<li><i>2013</i>: <a href=\"https://danluu.com/hardware-unforgiving/\">The institution knowledge and culture that create excellence can take a long time to build up</a>: At this time, I hadn't worked in software and that thought that this wasn't as difficult for software because so many software companies are successful with new/young teams. But, in retrospect, the difference isn't that those companies don't produce bad (unreliable, buggy, slow, etc.) software, it's that product/market fit and network effects are important enough that it frequently doesn't matter that software is bad</li>\n<li><i>2015</i>: <a href=\"https://danluu.com/dunning-kruger/\">In this post on how people don't read citations</a>, I found it mysterious that type system advocates would cite non-existent strong evidence, which seems unlike the other examples, where people pass on a clever, contrarian, result without ever having read it. The thing I thought was mysterious was that, unlike the other examples, there isn't an incorrect piece of evidence being passed around; the assertion that there is evidence is disconnected from any evidence, even misinterpreted evidence. In retrospect, I was being naive in thinking that there was a link to evidence that people wouldn't just fabricate the idea that there is evidence supporting their belief and then pass that around.</li>\n</ul></li>\n<li>Insufficient verification of information\n\n<ul>\n<li><i>2016</i>: <a href=\"https://danluu.com/sounds-easy/\">Building a search engine isn't trivial</a>: although I think the overall point is true, one of the pieces of evidence I relied on came out of using numbers that someone who worked on a search engine told me about. But when I measured actual numbers, I found that the numbers I was told were off by multiple orders of magnitude</li>\n</ul></li>\n<li>Blunder\n\n<ul>\n<li><i>2015</i>: <a href=\"https://danluu.com/butler-lampson-1999/\">Checking out Butler Lampson's review of what worked in CS, 16 years later</a>: it was wrong to say that capabilities were a &quot;no&quot; in 2015 given their effectiveness on mobile and that seems so obviously wrong at the time that I would call this a blunder rather than something where I gave it a decent amount of thought but should've thought through it more deeply</li>\n</ul></li>\n<li>Pointlessly difficult to understand explanation\n\n<ul>\n<li><i>2013</i>: <a href=\"https://danluu.com/3c-conflict/\">How data alignment impacts memory latency</a>: the main plots in this post use a ratio of latencies, which adds a level of indirection that many people found confusing</li>\n<li><i>2017</i>: <a href=\"https://danluu.com/p95-skill/\">It is easy to achieve 95%-ile performance</a>: the most common objection people had to this post was something like &quot;False. You need to be very talented and/or it is hard to [play in the NBA / become a chess GM / achieve a 2200 chess rating]&quot;. <a href=\"https://twitter.com/JamesClear/status/1292574538912456707\">James Clear made an even weaker claim on Twitter</a> and also got similar responses. There isn't really space to do this on Twitter, but in my blog post, I should've included more concrete examples of what various levels of performance look like for people who have a difficult time estimating what performance looks like at various percentiles. To pick one of the less outlandish claims, <a href=\"https://lobste.rs/s/mwykjj/95_ile_isn_t_good#c_pudige\">here's a claim that a 2200 rating is 95%-ile for someone who's ever played chess online</a>, which <a href=\"https://lobste.rs/s/mwykjj/95_ile_isn_t_good#c_iyoegc\">appears to be off by perhaps four orders of magnitude, plus or minus one</a>.</li>\n</ul></li>\n<li>Errors in retrospect\n\n<ul>\n<li><i>2015</i>: <a href=\"https://danluu.com/blog-ads/\">Blog monetization</a>: I grossly underestimated how much <a href=\"https://www.patreon.com/danluu\">I could make on Patreon</a> by looking at how much Casey Muratori, Eric Raymond, and eevee were making on Patreon at the time. I thought that all three of them would out-earn me based for a variety of reasons and that was incorrect. A major reason that was incorrect was that boring, long-form, writing monetizes much better than I exepected, which means that I monetarily undervalued that compared to what other tech folks are doing.\n\n<ul>\n<li>A couple weeks ago, I added a link to Patreon at the top of posts (instead of just having one hidding at the bottom) and mentioned having a Patreon on Twitter. Since then, my earnings have increased by about as much as Eric Raymond makes in total and the amount seems to be increasing at a decent rate, which is a result I wouldn't have expected before the rise of substack. But anyone who realized how well individual writers can monetize their writing could've created substack and no one did until Chris Best, Hamish McKenzie, and Jairaj Sethi created substack, so I'd say that this one was somewhat non-obvious</li>\n</ul></li>\n</ul></li>\n</ul>\n\n<p>Thanks to Anja Boskovic and Ville Sundberg for comments/corrections/discussion.</p>\n"]}]} {:tag :item, :attrs nil, :content [{:tag :title, :attrs nil, :content ["Individuals matter"]} {:tag :link, :attrs nil, :content ["https://danluu.com/people-matter/"]} {:tag :pubDate, :attrs nil, :content ["Mon, 15 Nov 2021 00:00:00 +0000"]} {:tag :guid, :attrs nil, :content ["https://danluu.com/people-matter/"]} {:tag :description, :attrs nil, :content ["\n\n<p>One of the most common mistakes I see people make when looking at data is incorrectly using an overly simplified model. A specific variant of this that has derailed the majority of work roadmaps I've looked at is treating people as interchangeable, as if it doesn't matter who is doing what, as if individuals don't matter.</p>\n\n<p>Individuals matter.</p>\n\n<p>A pattern I've repeatedly seen during the roadmap creation and review process is that people will plan out the next few quarters of work and then assign some number of people to it, one person for one quarter to a project, two people for three quarters to another, etc. Nominally, this process enables teams to understand what other teams are doing and plan appropriately. I've never worked in an organization where this actually worked, where this actually enabled teams to effectively execute with dependencies on other teams.</p>\n\n<p>What I've seen happen instead is, when work starts on the projects, people will ask who's working the project and then will make a guess at whether or not the project will be completed on time or in an effective way or even be completed at all based on who ends up working on the project. &quot;Oh, Joe is taking feature X? He never ships anything reasonable. Looks like we can't depend on it because that's never going to work. Let's do Y instead of Z since that won't require X to actually work&quot;. The roadmap creation and review process maintains the polite fiction that people are interchangeable, but everyone knows this isn't true and teams that are effective and want to ship on time can't play along when the rubber hits the road even if they play along with the managers, directors, and VPs, who create roadmaps as if people can be generically abstracted over.</p>\n\n<p>Another place the non-fungibility of people causes predictable problems is with how managers operate teams. Managers who want to create effective teams<sup class=\"footnote-ref\" id=\"fnref:P\"><a rel=\"footnote\" href=\"#fn:P\">1</a></sup> end up fighting the system in order to do so. Non-engineering orgs mostly treat people as fungible, and the finance org at a number of companies I've worked for forces the engineering org to treat people as fungible by requiring the org to budget in terms of headcount. The company, of course, spends money and not &quot;heads&quot;, but internal bookkeeping is done in terms of &quot;heads&quot;, so $X of budget will be, for some team, translated into something like &quot;three staff-level heads&quot;. There's no way to convert that into &quot;two more effective and better-paid staff level heads&quot;<sup class=\"footnote-ref\" id=\"fnref:E\"><a rel=\"footnote\" href=\"#fn:E\">2</a></sup>. If you hire two staff engineers and not a third, the &quot;head&quot; and the associated budget will eventually get moved somewhere else.</p>\n\n<p>One thing I've repeatedly seen is that a hiring manager will <a href=\"https://twitter.com/danluu/status/1452701799417143296\">want to hire someone who they think will be highly effective or even just someone who has specialized skills and then not be able to hire because the company has translated budget into &quot;heads&quot; at a rate that doesn't allow for hiring some kind of heads</a>. There will be a &quot;comp team&quot; or other group in HR that will object because the comp team has no concept of &quot;an effective engineer&quot; or &quot;a specialty that's hard to hire for&quot;; for a person, role, level, and location defines them and someone who's paid too much for their role and level is therefore a bad hire. If anyone reasonable had power over the process that they were willing to use, this wouldn't happen but, by design, the bureaucracy is set up so that few people have power<sup class=\"footnote-ref\" id=\"fnref:B\"><a rel=\"footnote\" href=\"#fn:B\">3</a></sup>.</p>\n\n<p>A similar thing happens with retention. A great engineer I know who was regularly creating $x0M/yr<sup class=\"footnote-ref\" id=\"fnref:Q\"><a rel=\"footnote\" href=\"#fn:Q\">4</a></sup> of additional profit for the company per year wanted to move home to Portugal, so the company cut the person's cash comp by a factor of four. The company also offered to only cut his cash comp by a factor of two if he moved to Spain instead of Portugal. He left for a company that doesn't have location-based pay. This was escalated up to the director level, but that wasn't sufficient to override HR, so they left. HR didn't care that the person made the company more money than HR saves by doing location adjustments for all international employees combined because HR at the company had no notion of the value of an employee, only the cost, title, level, and location<sup class=\"footnote-ref\" id=\"fnref:L\"><a rel=\"footnote\" href=\"#fn:L\">5</a></sup>.</p>\n\n<p>Relatedly, a &quot;move&quot; I've seen twice, once from a distance and once from up close, is when HR decides <a href=\"https://twitter.com/danluu/status/1277591470162104321\">attrition is too low</a>. In one case, the head of HR thought that the company's ~5% attrition was &quot;unhealthy&quot; because it was too low and in another, HR thought that the company's attrition sitting at a bit under 10% was too low. In both cases, the company made some moves that resulted in attrition moving up to what HR thought was a &quot;healthy&quot; level. In the case I saw from a distance, folks I know at the company agree that the majority of the company's best engineers left over the next year, many after only a few months. In the case I saw up close, I made a list of the most effective engineers I was aware of (like the person mentioned above who increased the company's revenue by 0.7% on his paternity leave) and, when the company successfully pushed attrition to over 10% overall, the most effective engineers left at over double that rate (which understates the impact of this because they tended to be long-tenured and senior engineers, where the normal expected attrition would be less than half the average company attrition).</p>\n\n<p>Some people seem to view companies like a game of SimCity, where if you want more money, you can turn a knob, increase taxes, and get more money, uniformly impacting the city. But companies are not a game of SimCity. If you want more attrition and turn a knob that cranks that up, you don't get additional attrition that's sampled uniformly at random. People, as a whole, cannot be treated as an abstraction where the actions company leadership takes impacts everyone in the same way. The people who are most effective will be disproportionately likely to leave if you turn a knob that leads to increased attrition.</p>\n\n<p>So far, we've talked about how treating individual people as fungible doesn't work for corporations but, of course, it also doesn't work in general. For example, a complaint from a friend of mine who's done a fair amount of &quot;on the ground&quot; development work in Africa is that a lot of people who are looking to donate want, clear, simple criteria to guide their donations (e.g., an RCT showed that the intervention was highly effective). But many effective interventions cannot have their impact demonstrated ex ante in any simple way because, among other reasons, the composition of the team implementing the intervention is important, resulting in a randomized trial or other experiment not being applicable to team implementing the intervention other than the teams from the trial in the context they were operating in during the trial.</p>\n\n<p>An example of this would be an intervention they worked on that, among other things, helped wipe out guinea worm in a country. Ex post, we can say that was a highly effective intervention since it was a team of three people operating on a budget of $12/(person-day)<sup class=\"footnote-ref\" id=\"fnref:T\"><a rel=\"footnote\" href=\"#fn:T\">6</a></sup> for a relatively short time period, making it a high ROI intervention, but there was no way to make a quantitative case for the intervention ex ante, nor does it seem plausible that there could've been a set of randomized trials or experiments that would've justified the intervention.</p>\n\n<p>Their intervention wasn't wiping out guinea worm, that was just a side effect. The intervention was, basically, travelling around the country and embedding in regional government offices in order to understand their problems and then advise/facilitate better decision making. In the course of talking to people and suggesting improvements/changes, they realized that guinea worm could with better distribution of clean water (guinea worm can come from drinking unfiltered water; giving people clean water can solve that problem) and that aid money flowing into the country specifically for water-related projects, like building wells, was already sufficient if the it was distributed to places in the country that had high rates of guinea worm due to contaminated water instead of to the places aid money was flowing to (which were locations that had a lot of aid money flowing to them for a variety of reasons, such as being near a local &quot;office&quot; that was doing a lot of charity work). The specific thing this team did to help wipe out guinea worm was to give powerpoint presentations to government officials on how the government could advise organizations receiving aid money on how those organizations could more efficiently place wells. At the margin, wiping out guinea worm in a country would probably be sufficient for the intervention to be high ROI, but that's a very small fraction of the &quot;return&quot; from this three person team. I only mention it because it's a self-contained easily-quantifiable change. Most of the value of &quot;leveling up&quot; decision making in regional government offices is very difficult to quantify (and, to the extent that it can be quantified, will still have very large error bars).</p>\n\n<p>Many interventions that seem the same ex ante, probably even most, produce little to no impact. My friend has a lot of comments on organizations that send a lot of people around to do similar sounding work but that produce little value, such as the Peace Corps.</p>\n\n<p>A major difference between my friend's team and most teams is that my friend's team was composed of people who had a track record of being highly effective across a variety of contexts. In an earlier job, my friend started a job at a large-ish ($5B/yr revenue) government-run utility company and was immediately assigned a problem that, unbeknownst to her, had been an open problem for years that was considered to be unsolvable. No one was willing to touch the problem, so they hired her because they wanted a scapegoat to blame and fire when the problem blew up. Instead, she solved the problem she was assigned to as well as a number of other problems that were considered unsolvable. A team of three such people will be able to get a lot of mileage out of potentially high ROI interventions that most teams would not succeed at, such as going to a foreign country and improving governmental decision making in regional offices across the country enough that the government is able to solve serious open problems that had been plaguing the country for decades.</p>\n\n<p>Many of the highest ROI interventions are similarly skill intensive and not amenable to simple back-of-the-envelope calculations, but most discussions I see on the topic, both in person and online, rely heavily on simplistic but irrelevant back-of-the-envelope calculations. This is not just a problem limited to cocktail-party conversations. My friend's intervention was almost killed by the organization she worked for because the organization was infested with what she thinks of &quot;overly simplistic <a href=\"https://en.wikipedia.org/wiki/Effective_altruism\">EA</a> thinking&quot;, which caused leadership in the organization to try to redirect resources to projects where the computation of expected return was simpler because those projects were thought to be higher impact even though they were, ex post, lower impact. Of course, we shouldn't judge interventions on how they performed ex post since that will overly favor high variance interventions, but I think that someone thinking it through, who was willing to exercise their judgement instead of outsourcing their judgement to a simple metric, could and should say that the intervention in question was a good choice ex ante.</p>\n\n<p>This issue of projects which are more <a href=\"https://amzn.to/3qGiucN\">legible</a> getting more funding is an issue across organizations as well as within them. For example, my friend says that, back when GiveWell was mainly or only recommending charities that had simply quantifiable return, she basically couldn't get her friends who worked in other fields to put resources towards efforts that weren't endorsed by GiveWell. People who didn't know about her aid background would say things like &quot;haven't you heard of GiveWell?&quot; when she suggested putting resources towards any particular cause, project, or organization.</p>\n\n<p>I talked to a friend of mine who worked at GiveWell during that time period about this and, according to him, the reason GiveWell initially focused on charities that had easily quantifiable value wasn't that they thought those were the highest impact charities. Instead, it was because, as a young organization, they needed to be credible and it's easier to make a credible case for charities whose value is easily quantifiable. He would not, and he thinks GiveWell would not, endorse donors funnelling all resources into charities endorsed by GiveWell and neglecting other ways to improve the world. But many people want the world to be simple and apply the algorithm &quot;charity on GiveWell list = good; not on GiveWell list = bad&quot; because it makes the world simple for them.</p>\n\n<p>Unfortunately for those people, as well as for the world, the world is not simple.</p>\n\n<p>Coming back to the tech company examples, Laurence Tratt notes something that I've also observed:</p>\n\n<blockquote>\n<p>One thing I've found very interesting in large organisations is when they realise that they need to do something different (i.e. they're slowly failing and want to turn the ship around). The obvious thing is to let a small team take risks on the basis that they might win big. Instead they tend to form endless committees which just perpetuate the drift that caused the committees to be formed in the first place! I think this is because they really struggle to see people as anything other than fungible, even if they really want to: it's almost beyond their ability to break out of their organisational mould, even when it spells long-term doom.</p>\n</blockquote>\n\n<p>One lens we can use to look at what's going on is <a href=\"https://www.ribbonfarm.com/2010/07/26/a-big-little-idea-called-legibility/\">legibility</a>. When you have a complex system, whether that's a company with thousands of engineers or a world with many billions of dollars going to aid work, the system is too complex for any decision maker to really understand, whether that's an exec at a company or a potential donor trying to understand where their money should go. One way to address this problem is by reducing the perceived complexity of the problem via imagining that individuals are fungible, making the system more legible. That produces relatively inefficient outcomes but, unlike <a href=\"https://twitter.com/benskuhn/status/1458948982059593730\">trying to understand the issues at hand</a>, it's highly scalable, and if there's one thing that tech companies like, it's doing things that scale, and treating a complex system like it's SimCity or Civilization is highly scalable. When returns are relatively evenly distributed, losing out on potential outlier returns in the name of legibility is a good trade-off. But when ROI is a heavy-tailed distribution, when the right person can, on their paternity leave, increase company revenue of a giant tech company by 0.7% and then much more when they work on that full-time, then severely tamping down on the right side of the curve to improve legibility is very costly and can cost you the majority of your potential returns.</p>\n\n<p>Thanks to Laurence Tratt, Pam Wolf, Ben Kuhn, Peter Bhat Harkins, John Hergenroeder, Andrey Mishchenko, Joseph Kaptur, and Sophia Wisdom for comments/corrections/discussion.</p>\n\n<h3 id=\"appendix-re-orgs\">Appendix: re-orgs</h3>\n\n<p>A friend of mine recently told me a story about a trendy tech company where they tried to move six people to another project, one that the people didn't want to work on that they thought didn't really make sense. The result was that two senior devs quit, the EM retired, one PM was fired (long story), and three people left the team. The team for both the old project and the new project had to be re-created from scratch.</p>\n\n<p>It could be much worse. In that case, at least there were some people who didn't leave the company. I once asked someone why feature X, which had been publicly promised, hadn't been implemented yet and also the entire sub-product was broken. The answer was that, after about a year of work, when shipping the feature was thought to be weeks away, leadership decided that the feature, which was previously considered a top priority, was no longer a priority and should be abandoned. The team argued that the feature was very close to being done and they just wanted enough runway to finish the feature. When that was denied, the entire team quit and the sub-product has slowly decayed since then. After many years, there was one attempted reboot of the team but, for reasons beyond the scope of this story, it was done with a new manager managing new grads and didn't really re-create what the old team was capable of.</p>\n\n<p>As we've previously seen, <a href=\"https://danluu.com/hardware-unforgiving/\">an effective team is difficult to create, due to the institutional knowledge that exists on a team</a>, as well as <a href=\"https://danluu.com/culture/\">the team's culture</a>, but destroying a team is very easy.</p>\n\n<p>I find it interesting that so many people in senior management roles persist in thinking that they can re-direct people as easily as opening up the city view in Civilization and assigning workers to switch from one task to another when the senior ICs I talk to have high accuracy in predicting when these kinds of moves won't work out.</p>\n\n<h3 id=\"appendix-related-posts\">Appendix: related posts</h3>\n\n<ul>\n<li><a href=\"https://yosefk.com/blog/compensation-rationality-and-the-projectperson-fit.html\">Yossi Kreinin on compensation and project/person fit</a></li>\n<li><a href=\"https://danluu.com/hardware-unforgiving/\">Me on the difficulty of obtaining institutional knowledge</a></li>\n<li><a href=\"https://amzn.to/3qGiucN\">James C. Scott on legibility</a></li>\n</ul>\n<div class=\"footnotes\">\n\n<hr />\n\n<ol>\n<li id=\"fn:P\"><p>On the flip side, there are managers who want to maximize the return to their career. At every company I've worked at that wasn't a startup, doing that involves moving up the ladder, which is easiest to do by collecting as many people as possible. At one company I've worked for, the explicitly stated promo criteria are basically &quot;how many people report up to this person&quot;.</p>\n\n<p>Tying promotions and compensation to the number of people managed could make sense if you think of people as mostly fungible, but is otherwise an obviously silly idea.</p>\n <a class=\"footnote-return\" href=\"#fnref:P\"><sup>[return]</sup></a></li>\n<li id=\"fn:E\">This isn't quite this simple when you take into account retention budgets (money set aside from a pool that doesn't come out of the org's normal budget, often used to match offers from people who are leaving), etc., but adding this nuance doesn't really change the fundamental point.\n <a class=\"footnote-return\" href=\"#fnref:E\"><sup>[return]</sup></a></li>\n\n<li id=\"fn:B\"><p>There are advantages to a system where people don't have power, such as mitigating abuses of power, various biases, nepotism, etc. One can argue that reducing variance in outcomes by making people powerless is the preferred result, but in winner-take-most markets, which many tech markets are, forcing everyone lowest-common-denominator effectiveness is a recipe for being an also ran.</p>\n\n<p>A specific, small-scale, example of this is the massive advantage <a href=\"https://danluu.com/corp-eng-blogs/\">companies that don't have a bureaucratic comms/PR approval process for technical blog posts have</a>. The theory behind having the onerous process that most companies have is that the company is protected from downside risk of a bad blog post, but examples of bad engineering blog posts  that would've been mitigated by having an onerous process are few and far between, whereas the companies that have good processes for writing publicly get a lot of value that's easy to see.</p>\n\n<p>A larger scale example of this is that the large, now &gt;= $500B companies, all made aggressive moves that wouldn't have been possible at their bureaucracy laden competitors, which allowed them to wipe the floor with their competitors. Of course, many other companies that made serious bets instead of playing it safe failed more quickly than companies trying to play it safe, but those companies at least had a chance, unlike the companies that played it safe.</p>\n <a class=\"footnote-return\" href=\"#fnref:B\"><sup>[return]</sup></a></li>\n\n<li id=\"fn:Q\"><p>I'm generally skeptical of claims like this. At multiple companies that I've worked for, if you tally up the claimed revenue or user growth wins and compare them to actual revenue or user growth, you can see that there's some funny business going on since the total claimed wins are much larger than the observed total.</p>\n\n<p>Just because <a href=\"https://danluu.com/why-benchmark/\">I'm generally curious about measurements</a>, I sometimes did my own analysis of people's claimed wins and I almost always came up with an estimate that was much lower than the original estimate. Of course, I generally didn't publish these results internally since that would, in general, be a good way to make a lot of enemies without causing any change. In one extreme case, I found that the experimental methodology one entire org used was broken, causing them to get spurious wins in their A/B tests. I quietly informed them and they did nothing about it, which was the only reasonable move for them since having experiments that systematically showed improvement when none existed was a cheap and effective way for the org to gain more power by having its people get promoted and having more headcount allocated to it. And if anyone with power over the bureaucracy cared about accuracy of results, such a large discrepancy between claimed wins and actual results couldn't exist in the first place.</p>\n\n<p>Anyway, despite my general skepticism of claimed wins in general, I found this person's claimed wins highly credible after checking them myself. A project of theirs, done on their paternity leave (done while on leave because their manager and, really, the organization as well as the company, didn't support the kind of work they were doing) increased the company's revenue by 0.7%, robust and actually increasing in value through a long-term holdback, and they were able to produce wins of that magnitude after leadership was embarrassed into allowing them to do valuable work.</p>\n\n<p>P.S. If you'd like to play along at home, another fun game you can play after figuring out which teams and orgs hit their roadmap goals. For bonus points, plot the percentage of roadmap goals a team hits vs. their headcount growth as well as how predictive hitting last quarter's goals are for hitting next quarter's goals across teams.</p>\n <a class=\"footnote-return\" href=\"#fnref:Q\"><sup>[return]</sup></a></li>\n\n<li id=\"fn:L\"><p>I've seen quite a few people leave their employers due to location adjustments during the pandemic. In one case, HR insisted the person was actually very well compensated because, even though it might appear as if the person isn't highly paid because they were paid significantly less than many people who were one level below them, according to HR's formula, which included a location-based pay adjustment, the person was one of the highest paid people for their level at the entire company in terms of normalized pay. Putting aside abstract considerations about fairness, <a href=\"https://yosefk.com/blog/compensation-rationality-and-the-projectperson-fit.html\">for an employee</a>, HR telling them that they're highly paid given their location is like HR having a formula that pays based on height telling an employee that they're well paid for their height. That may be true according to whatever formula HR has but, practically speaking, that means nothing to the employee, who can go work somewhere that has a smaller height-based pay adjustment.</p>\n\n<p>Companies were able to get away with severe location-based pay adjustments with no cost to themselves before the pandemic. But, since the pandemic, a lot of companies have ramped up remote hiring and some of those companies have relatively small location-based pay adjustments, which has allowed them to disproportionately hire away who they choose from companies that still maintain severe location-based pay adjustments.</p>\n <a class=\"footnote-return\" href=\"#fnref:L\"><sup>[return]</sup></a></li>\n<li id=\"fn:T\">Technically, their budget ended up being higher than this because one team member contracted typhoid and paid for some medical expenses from their personal budget and not from the organization's budget, but $12/(person-day), the organizational funding, is a pretty good approximation.\n <a class=\"footnote-return\" href=\"#fnref:T\"><sup>[return]</sup></a></li>\n</ol>\n</div>\n"]}]} {:tag :item, :attrs nil, :content [{:tag :title, :attrs nil, :content ["Culture matters"]} {:tag :link, :attrs nil, :content ["https://danluu.com/culture/"]} {:tag :pubDate, :attrs nil, :content ["Mon, 08 Nov 2021 00:00:00 +0000"]} {:tag :guid, :attrs nil, :content ["https://danluu.com/culture/"]} {:tag :description, :attrs nil, :content ["\n\n<p>Three major tools that companies have to influence behavior are incentives, process, and culture. People often mean different things when talking about these, so I'll provide an example of each so we're on the same page (if you think that I should be using a different word for the concept, feel free to mentally substitute that word).</p>\n\n<ul>\n<li><p>Getting people to show up to meetings on time</p>\n\n<ul>\n<li>Incentive: dock pay for people who are late</li>\n<li>Process: don't allow anyone who's late into the meeting</li>\n<li>Culture: people feel strongly about showing up on time</li>\n</ul></li>\n\n<li><p>Getting people to build complex systems</p>\n\n<ul>\n<li>Incentive: require complexity in promo criteria</li>\n<li>Process: make process for creating or executing on a work item so heavyweight that people stop doing simple work</li>\n<li>Culture: people enjoy building complex systems and/or building complex systems results in respect from peers and/or prestige</li>\n</ul></li>\n\n<li><p>Avoiding manufacturing defects</p>\n\n<ul>\n<li>Incentive: pay people per good item created and/or dock pay for bad items</li>\n<li>Process: have QA check items before shipment and discard bad items</li>\n<li>Culture: people value excellence and try very hard to avoid defects</li>\n</ul></li>\n</ul>\n\n<p>If you read &quot;old school&quot; thought leaders, many of them advocate for a culture-only approach, e.g., <a href=\"https://mobile.twitter.com/danluu/status/885214004649615360\">Ken Thompson saying, to reduce bug rate, that tools (which, for the purposes of this post, we'll call process) aren't the answer, having people care to and therefore decide to avoid writing bugs is the answer</a> or Bob Martin saying &quot;<a href=\"https://www.hillelwayne.com/post/uncle-bob/\">The solution to the software apocalypse is not more tools. The solution is better programming discipline</a>.&quot;</p>\n\n<p>The emotional reaction those kinds of over-the-top statements evoke combined with the ease of rebutting them has led to a backlash against cultural solutions, leading people to say things like &quot;you should never say that people need more discipline and you should instead look at the incentives of the underlying system&quot;, in the same way that the 10x programmer meme and the associated comments have caused a backlash that's led to people to say things like <a href=\"https://danluu.com/productivity-velocity/\">velocity doesn't matter at all</a> or there's absolutely no difference in velocity between programmers (<a href=\"https://scattered-thoughts.net/writing/moving-faster/\">as Jamie Brandon has noted, a lot of velocity comes down to caring about and working on velocity</a>, so this is also part of the backlash against culture).</p>\n\n<p>But if we look at quantifiable output, we can see that, even if processes and incentives are the first-line tools a company should reach for, culture also has a large impact. For example, if we look at manufacturing defect rate, some countries persistently have lower defect rates than others on a timescale of decades<sup class=\"footnote-ref\" id=\"fnref:C\"><a rel=\"footnote\" href=\"#fn:C\">1</a></sup>, generally robust across companies, even when companies are operating factories in multiple countries and importing the same process and incentives to each factory to the extent that's possible, due to cultural differences that impact how people work.</p>\n\n<p>Coming back to programming, Jamie's post on &quot;moving faster&quot; notes:</p>\n\n<blockquote>\n<p>The main thing that helped is actually wanting to be faster.</p>\n\n<p>Early on I definitely cared more about writing 'elegant' code or using fashionable tools than I did about actually solving problems. Maybe not as an explicit belief, but those priorities were clear from my actions.</p>\n\n<p>I probably also wasn't aware how much faster it was possible to be. I spent my early career working with people who were as slow and inexperienced as I was.</p>\n\n<p>Over time I started to notice that some people are producing projects that are far beyond what I could do in a single lifetime. I wanted to figure out how to do that, which meant giving up my existing beliefs and trying to discover what actually works.</p>\n</blockquote>\n\n<p>I was lucky to have the opposite experience starting out since my first full-time job was at Centaur, a company that, at the time, had very high velocity/productivity. I'd say that I've only ever worked on one team with a similar level of productivity, and that's my current team, but my current team is fairly unusual for a team at a tech company (e.g., the median level on my team is &quot;senior staff&quot;)<sup class=\"footnote-ref\" id=\"fnref:S\"><a rel=\"footnote\" href=\"#fn:S\">2</a></sup>. A side effect of having started my career at such a high velocity company is that I generally find the pace of development slow at big companies and I see no reason to move slowly just because that's considered normal. I often hear similar comments from people I talk to at big companies who've previously worked at non-dysfunctional but not even particularly fast startups. A regular survey at one of the trendiest companies around asks &quot;Do you feel like your dev speed is faster or slower than your previous job?&quot; and the responses are bimodal, depending on whether the respondent came from a small company or a big one (with dev speed at TrendCo being slower than at startups and faster than at larger companies).</p>\n\n<p>There's a story that, <a href=\"https://amzn.to/3EZXykS\">IIRC, was told by Brian Enos</a>, where he was practicing timed drills with the goal of practicing until he could complete a specific task at or under his usual time. He was having a hard time hitting his normal time and was annoyed at himself because he was slower than usual and kept at it until he hit his target, at which point he realized he misremembered the target and was accidentally targeting a new personal best time that was better than he thought was possible. While it's too simple to say that we can achieve anything if we put our minds to it, <a href=\"https://twitter.com/nickcammarata/status/1362261305357393920\">almost none of us are operating at anywhere near our capacity and what we think we can achieve is often a major limiting factor</a>. Of course, at the limit, there's a tradeoff between velocity and quality and you can't get velocity &quot;for free&quot;, but, when it comes to programming, <a href=\"https://danluu.com/p95-skill/\">we're so far from the Pareto frontier that there are free wins</a> if you just <a href=\"https://twitter.com/danluu/status/1442945072144678914\">realize that they're available</a>.</p>\n\n<p>One way in which culture influences this is that people often absorb their ideas of what's possible from the culture they're in. For a non-velocity example, one thing I noticed after attending <a href=\"https://www.recurse.com/scout/click?t=b504af89e87b77920c9b60b2a1f6d5e8\">RC</a> was that a lot of speakers at the well-respected non-academic non-enterprise tech conferences, like Deconstruct and Strange Loop, also attended RC. Most people hadn't given talks before attending RC and, when I asked people, a lot of people had wanted to give talks but didn't realize how straightforward the process for becoming a speaker at &quot;big&quot; conferences is (have an idea, write it down, and then submit what you wrote down as a proposal). It turns out that giving talks at conferences is easy to do and a major blocker for many folks is just knowing that it's possible. In an environment where lots of people give talks and, where people who hesitantly ask how they can get started are told that it's straightforward, a lot of people will end up giving talks. The same thing is true of blogging, which is why a disproportionately large fraction of widely read programming bloggers started blogging seriously after attending RC. For many people, the barrier to starting a blog is some combination of realizing it's feasible to start a blog and that, from a technical standpoint, it's very easy to start a blog if you just pick any semi-reasonable toolchain and go through the setup process. And then, because people give talks and write blog posts, they get better at giving talks and writing blog posts so, on average, RC alums are probably better speakers and writers than random programmers even though there's little to no skill transfer or instruction at RC.</p>\n\n<p>Another kind of thing where culture can really drive skills are skills that are highly attitude dependent. An example of this is debugging. As Julia Evans has noted, <a href=\"https://twitter.com/b0rk/status/1249715842708844544\">having a good attitude is a major component of debugging effectiveness</a>. This is something Centaur was very good at instilling in people, to the point that nearly everyone in my org at Centaur would be considered a very strong debugger by tech company standards.</p>\n\n<p>At big tech companies, it's common to see people give up on bugs after trying a few random things that didn't work. In one extreme example, someone I know at a mid-10-figure tech company said that it never makes sense to debug a bug that takes more than a couple hours to debug because engineer time is too valuable to waste on bugs that take longer than that to debug, an attitude this person picked up from the first team they worked on. Someone who picks up that kind of attitude about debugging is unlikely to become a good debugger until they change their attitude, and many people, including this person, carry the attitudes and habits they pick up at their first job around for quite a long time<sup class=\"footnote-ref\" id=\"fnref:N\"><a rel=\"footnote\" href=\"#fn:N\">3</a></sup>.</p>\n\n<p>By tech standards, Centaur is an extreme example in the other direction. If you're designing a CPU, it's not considered ok to walk away from a bug that you don't understand. Even if the symptom of the bug isn't serious, it's possible that the underlying cause is actually serious and you won't observe the more serious symptom until you've shipped a chip, so you have to go after even seemingly trivial bugs. Also, it's pretty common for there to be no good or even deterministic reproduction of a bug. The repro is often something like &quot;run these programs with these settings on the system and then the system will hang and/or corrupt data after some number of hours or days&quot;. When debugging a bug like that, there will be numerous wrong turns and dead ends, some of which can eat up weeks or months. As a new employee watching people work on those kinds of bugs, what I observed was that people would come in day after day and track down bugs like that, not getting frustrated and not giving up. When that's the culture and everyone around you has that attitude, it's natural to pick up the same attitude. Also, a lot of practical debugging skill is applying tactical skills picked up from having debugged a lot of problems, which naturally falls out of spending a decent amount of time debugging problems with a positive attitude, especially with exposure to hard debugging problems.</p>\n\n<p>Of course, most bugs at tech companies don't warrant months of work, but there's a big difference between intentionally leaving some bugs undebugged because some bugs aren't worth fixing and having poor debugging skills from never having ever debugged a serious bug and then not being able to debug any bug that isn't completely trivial.</p>\n\n<p>Cultural attitudes can drive a lot more than individual skills like debugging. Centaur had, per capita, by far the lowest serious production bug rate of any company I've worked for, at well under one per year with ~100 engineers. By comparison, I've never worked on a team 1/10th that size that didn't have at least 10x the rate of serious production issues. Like most startups, Centaur was very light on process and it was also much lighter on incentives than the big tech companies I've worked for.</p>\n\n<p>One component of this was that there was a culture of owning problems, regardless of what team you were on. If you saw a problem, you'd fix it, or, if there was a very obvious owner, you'd tell them about the problem and they'd fix it. There weren't roadmaps, standups, kanban, or anything else to get people to work on important problems. People did it without needed to be reminded or prompted.</p>\n\n<p>That's the opposite of what I've seen at two of the three big tech companies I've worked for, where the median person avoids touching problems outside of their team's mandate like the plague, and someone who isn't politically savvy who brings up a problem to another team will get a default answer of &quot;sorry, this isn't on our roadmap for the quarter, perhaps we can put this on the roadmap in [two quarters from now]&quot;, with the same response repeated to anyone naive enough to bring up the same issue two quarters later. At every tech company I've worked for, huge, extremely costly, problems slip through the cracks all the time because no one wants to pick them up. I never observed that happening at Centaur.</p>\n\n<p>A side effect of big company tech culture is that someone who wants to actually do the right thing can easily do very high (positive) impact work by just going around and fixing problems that any intern could solve, if they're willing to ignore organizational processes and incentives. You can't shake a stick without <a href=\"https://twitter.com/danluu/status/802971209176477696\">hitting a problem that's worth more to the company than my expected lifetime earnings</a> and it's easy to knock off multiple such problems per year. <a href=\"https://danluu.com/algorithms-interviews/#appendix-misaligned-incentive-hedgehog-defense-part-3\">Of course, the same forces that cause so many trivial problems to not get solved mean that people who solve those problems don't get rewarded for their work</a><sup class=\"footnote-ref\" id=\"fnref:B\"><a rel=\"footnote\" href=\"#fn:B\">4</a></sup>.</p>\n\n<p>Conversely, in eight years at Centaur, I only found one trivial problem whose fix was worth more than I'll earn in my life because, in general, problems would get solved before they got to that point. I've seen various big company attempts to fix this problem using incentives (e.g., monetary rewards for solving important problems) and process (e.g., making a giant list of all projects/problems and having a single person order them, along with a bureaucratic system where everyone has to constantly provide updates on their progress via JIRA so that PMs can keep sending progress updates to the person who's providing a total order over the work of thousands of engineers<sup class=\"footnote-ref\" id=\"fnref:R\"><a rel=\"footnote\" href=\"#fn:R\">5</a></sup>), but none of those attempts have worked even half as well as having a culture of ownership (to be fair to incentives, I've heard that FB uses monetary rewards to good effect, but <a href=\"https://twitter.com/danluu/status/1447268693075841024\">I've failed FB's interview three times</a>, so I haven't been able to observe how that works myself).</p>\n\n<p>Another component that resulted in a relatively low severe bug rate was that, across the company at Centaur, people cared about quality in a way that I've never seen at a team level let alone at an org level at a big tech company. When you have a collection of people who care about quality and feel that no issue is off limits, you'll get quality. And when you onboard people, as long as you don't do it so quickly that the culture is overwhelmed by the new hires, they'll also tend to pick up the same habits and values, especially when you hire new grads. While it's not exactly common, there are plenty of small firms out there with a culture of excellence that generally persists without heavyweight processes or big incentives, but this doesn't work at big tech companies since they've all gone through a hypergrowth period where it's impossible to maintain such extreme (by mainstream standards) cultural values.</p>\n\n<p>So far, we've mainly discussed companies transmitting culture to people, but something that I think is no less important is how people then carry that culture with them when they leave. I've been <a href=\"https://twitter.com/danluu/status/1444034823329177602\">reasonably successful since changing careers from hardware to software</a> and I think that, among the factors that are under my control, one of the biggest ones is that I picked up effective cultural values from the first place I worked full-time and continue to operate as in the same way, which is highly effective. I've also seen this in other people who, career-wise, &quot;grew up&quot; in a culture of excellence and then changed to a different field where there's even less direct skill transfer, e.g., from skiing to civil engineering. Relatedly, if you read books from people who discuss the reasons why they were very effective in their field, e.g., <a href=\"https://amzn.to/3EZXykS\">Practical Shooting by Brian Enos</a>, <a href=\"https://www.sirlin.net/ptw\">Playing to Win by Dan Sirlin</a>, etc., the books tend to contain the same core ideas (serious observation and improvement of skills, the importance of avoiding emotional self-sabotage, the importance of intuition, etc.).</p>\n\n<p>Anyway, I think that cultural transmission of values and skills is an underrated part of choosing a job (some things I would consider overrated are <a href=\"https://www.patreon.com/posts/25835707\">prestige</a> and <a href=\"https://twitter.com/danluu/status/1275191896097189888\">general reputation</a> and that people should be thoughtful about what cultures they spend time in because not many people are able to avoid at least somewhat absorbing the cultural values around them<sup class=\"footnote-ref\" id=\"fnref:L\"><a rel=\"footnote\" href=\"#fn:L\">6</a></sup>.</p>\n\n<p>Although this post is oriented around tech, there's nothing specific to tech about this. A classic example is how idealistic students will go to law school with the intention of doing &quot;save the world&quot; type work and then absorb the <a href=\"https://www.patreon.com/posts/25835707\">prestige-transmitted cultural values</a> of students around then go into the most prestigious job they can get which, when it's not a clerkship, will be a &quot;BIGLAW&quot; job that's the opposite of &quot;save the world&quot; work. To first approximation, everyone thinks &quot;that will never happen to me&quot;, but from having watched many people join organizations where they <a href=\"https://danluu.com/wat/\">initially find the values and culture very wrong</a>, almost no one is able to stay without, to some extent, absorbing the values around them; <a href=\"https://danluu.com/look-stupid/\">very few people are ok with everyone around them looking at them like they're an idiot for having the wrong values</a>.</p>\n\n<h3 id=\"appendix-bay-area-culture\">Appendix: Bay area culture</h3>\n\n<p>One thing I admire about the bay area is how infectious people's attitudes are with respect to trying to change the world. Everywhere I've lived, people gripe about problems (the mortgage industry sucks, selling a house is high friction, etc.). Outside of the bay area, it's just griping, but in the bay, when I talk to someone who was griping about something a year ago, there's a decent chance they've started a startup to try to address one of the problems they're complaining about. I don't think that people in the bay area are fundamentally different from people elsewhere, it's more that when you're surrounded by people who are willing to walk away from their jobs to try to disrupt an entrenched industry, it seems pretty reasonable to do the same thing (which also leads to network effects that make it easier from a &quot;technical&quot; standpoint, e.g., easier fundraising). <a href=\"https://slatestarcodex.com/2017/05/11/silicon-valley-a-reality-check/\">There's a kind of earnestness in these sorts of complaints and attempts to fix them that's easy to mock</a>, but <a href=\"https://danluu.com/look-stupid/\">that earnestness is something I really admire</a>.</p>\n\n<p>Of course, <a href=\"https://twitter.com/pushcx/status/1442860058660913166\">not all of bay area culture is positive</a>. The bay has, among other things, <a href=\"https://devonzuegel.com/post/why-is-flaking-so-widespread-in-san-francisco\">a famously flaky culture</a> to an extent I found shocking when I moved there. Relatively early on in my time there, I met some old friends for dinner and texted them telling them I was going to be about 15 minutes late. They were shocked when I showed up because they thought that saying that I was going to be late actually meant that I wasn't going to show up (another norm that surprised me that's an even more extreme version was that, for many people, not confirming plans shortly before their commencement means that the person has cancelled, i.e., plans are cancelled by default).</p>\n\n<p>A related norm that I've heard people complain about is how management and leadership will say yes to everything in a &quot;people pleasing&quot; move to avoid conflict, which actually increases conflict as people who heard &quot;yes&quot; as a &quot;yes&quot; and not as &quot;I'm saying yes to avoid saying no but don't actually mean yes&quot; are later surprised that &quot;yes&quot; meant &quot;no&quot;.</p>\n\n<h3 id=\"appendix-centaur-s-hiring-process\">Appendix: Centaur's hiring process</h3>\n\n<p>One comment people sometimes have when I talk about Centaur is that they must've had some kind of incredibly rigorous hiring process that resulted in hiring elite engineers, but the hiring process was much less selective than any &quot;brand name&quot; big tech company I've worked for (Google, MS, and Twitter) and not obviously more selective than boring, old school, companies I've worked for (IBM and Micron). The &quot;one weird trick&quot; was onboarding, not hiring.</p>\n\n<p>For new grad hiring (and, proportionally, we hired a lot of new grads), recruiting was more difficult than at any other company I'd worked for. Senior hiring wasn't difficult because Centaur had a good reputation locally, in Austin, but among new grads, no one had heard of us and no one wanted to work for us. When I recruited at career fairs, I had to stand out in front of our booth and flag down people who were walking by to get anyone to talk to us. This meant that we couldn't be picky about who we interviewed. We really ramped up hiring of new grads around the time that Jeff Atwood popularized the idea that there are a bunch of fake programmers out there applying for jobs and that you'd end up with programmers who can't program if you don't screen people out with basic coding questions in his very influential post, <a href=\"https://blog.codinghorror.com/why-cant-programmers-program/\" rel=\"nofollow\">Why Can't Programmers.. Program?</a> (the bolding below is his):</p>\n\n<blockquote>\n<p><strong>I am disturbed and appalled that any so-called programmer would apply for a job without being able to write the simplest of programs</strong>. That's a slap in the face to anyone who writes software for a living.\n...\nIt's a shame you have to do so much pre-screening to <strong>have the luxury of interviewing programmers who can actually program</strong>. It'd be funny if it wasn't so damn depressing</p>\n</blockquote>\n\n<p>Since we were a relatively coding oriented hardware shop (verification engineers primarily wrote software and design engineers wrote a lot of tooling), we tried asking a simple coding question where people were required to code up a function to output Fibonacci numbers given a description of how to compute them (the naive solution was fine; a linear time or faster solution wasn't necessary). We dropped that question because no one got it without being walked through the entire thing in detail, which meant that the question had zero discriminatory power for us.</p>\n\n<p>Despite not really asking a coding question, people did things like write hairy concurrent code (internal processor microcode, which often used barriers as the concurrency control mechanism) and create tools at a higher velocity and lower bug rate than I've seen anywhere else I've worked.</p>\n\n<p>We were much better off avoiding hiring the way everyone else was because that meant we tried to and did hire people that other companies weren't competing over. That wouldn't make sense if other companies were using techniques that were highly effective but other companies were doing things like asking people to code FizzBuzz and then whiteboard some algorithms. While <a href=\"https://danluu.com/algorithms-interviews/\">one might expect that doing algorithms interviews would result in hiring people who can solve the exact problems people ask about in interviews, but this turns out not to be the case</a>. <a href=\"https://twitter.com/danluu/status/1425514112642080773\">The other thing we did was have much less of a prestige filter than most companies</a>, which also let us hire great engineers that other companies wouldn't even consider.</p>\n\n<p>We did have some people who didn't work out, but it was never because they were &quot;so-called programmers&quot; who couldn't &quot;write the simplest of programs&quot;. I do know of two cases of &quot;fake programmers being hired who literally couldn't program, but both were at prestigious companies that have among the most rigorous coding interviews done at tech companies. In one case, it was discovered pretty quickly that the person couldn't code and people went back to review security footage from the interview and realized that the person who interviewed wasn't the person who showed up to do the job. In the other, the person was able to sneak under the radar at Google for multiple years before someone realized that the person never actually wrote any code and tasks only got completed when they got someone else to do the task. The person who realized eventually scheduled a pair programming session, where they discovered that the person wasn't able to write a loop, didn't know the difference between <code>=</code> and <code>==</code>, etc., despite being a &quot;senior SWE&quot; (L5/T5) at Google for years.</p>\n\n<p>I'm not going to say that having coding questions will never save you from hiring a fake programmer, but the rate of fake programmers appears to be very low enough that a small company can go a decade without hiring a fake programmer without asking a coding question and larger companies that are targeted by scammers still can't really avoid them even after asking coding questions.</p>\n\n<h3 id=\"appendix-importing-culture\">Appendix: importing culture</h3>\n\n<p>Although this post is about how company culture impacts employees, of course employees impact company culture as well. Something that seems underrated in hiring, especially of senior leadership and senior ICs, is how they'll impact culture. Something I've repeatedly seen, both up close, and from a distance, is the hiring of a new senior person who manages to import their culture, which isn't compatible with the existing company's culture, causing serious problems and, frequently, high attrition, as things settle down.</p>\n\n<p>Now that I've been around for a while, I've been in the room for discussions on a number of very senior hires and I've never seen anyone else bring up whether or not someone will import incompatible cultural values other than really blatant issues, like the person being a jerk or making racist or sexist comments in the interview.</p>\n\n<p>Thanks to Peter Bhat Harkins, Laurence Tratt, Julian Squires, Anja Boskovic, Tao L., Justin Blank, Ben Kuhn, V. Buckenham, Mark Papadakis, and Jamie Brandon for comments/corrections/discussion.</p>\n<div class=\"footnotes\">\n\n<hr />\n\n<ol>\n<li id=\"fn:C\">What countries actually have low defect rate manufacturing is often quite different from the general public reputation. To see this, you really need to look at the data, which is often NDA'd and generally only spread in &quot;bar room&quot; discussions.\n <a class=\"footnote-return\" href=\"#fnref:C\"><sup>[return]</sup></a></li>\n<li id=\"fn:S\">: Centaur had what I sometimes called &quot;the world's stupidest business model&quot;, competing with Intel on x86 chips starting in 1995, so it needed an extremely high level of productivity to survive. Through the bad years, AMD survived by selling off pieces of itself to fund continued x86 development and every other competitor (Rise, Cyrix, TI, IBM, UMC, NEC, and Transmeta) got wiped out. If you compare Centaur to the longest surviving competitor that went under, Transmeta, Centaur just plain shipped more quickly, which is a major reason that Centaur was able to survive until 2021 (when it was pseudo-acqui-hired by Intel) and Transmeta went in 2009 under after burning through ~$1B of funding (including payouts from lawsuits). Transmeta was founded in 1995 and shipped its first chip in 2000, which was considered a normal tempo for the creation of a new CPU/microarchitecture at the time; Centaur shipped its first chip in 1997 and continued shipping at a high cadence until 2010 or so (how things got slower and slower until the company stalled out and got acqui-hired is a topic for another post).\n <a class=\"footnote-return\" href=\"#fnref:S\"><sup>[return]</sup></a></li>\n<li id=\"fn:N\">This person initially thought the processes and values on their first team were absurd before <a href=\"https://danluu.com/wat/\">the cognitive dissonance got to them and they became a staunch advocate of the company's culture, which is typical for folks joining a company that has obviously terrible practices</a>.\n <a class=\"footnote-return\" href=\"#fnref:N\"><sup>[return]</sup></a></li>\n<li id=\"fn:B\">This illustrates one way in which incentives and culture are non-independent. What I've seen in places where this kind of work isn't rewarded is that, due to the culture, making these sorts of high-impact changes frequently requires burnout inducing slogs, at the end of which there is no reward, which causes higher attrition among people who have a tendency to own problems and do high-impact work. What I've observed in environments like this is that the environment differentially retains people who don't want to own problems, which then makes make more difficult and more burnout inducing for new people who join who attempt to fix serious problems.\n <a class=\"footnote-return\" href=\"#fnref:B\"><sup>[return]</sup></a></li>\n<li id=\"fn:R\">I'm adding this note because, when I've described this to people, many people thought that this must be satire. It is not satire.\n <a class=\"footnote-return\" href=\"#fnref:R\"><sup>[return]</sup></a></li>\n\n<li id=\"fn:L\"><p>As with many other qualities, there can be high variance within a company as well as across companies. For example, there's a team I sometimes encountered at a company I've worked for that has a very different idea of customer service than most of the company and people who join that team and don't quickly bounce usually absorb their values.</p>\n\n<p>Much of the company has a pleasant attitude towards internal customers, but this team has a &quot;the customer is always wrong&quot; attitude. A funny side effect of this is that, when I dealt with the team, I got the best support when a junior engineer who hadn't absorbed the team's culture was on call, and sometimes a senior engineer would say something was impossible or infeasible only to have a junior engineer follow up and trivially solve the problem.</p>\n <a class=\"footnote-return\" href=\"#fnref:L\"><sup>[return]</sup></a></li>\n</ol>\n</div>\n"]}]} {:tag :item, :attrs nil, :content [{:tag :title, :attrs nil, :content ["Willingness to look stupid"]} {:tag :link, :attrs nil, :content ["https://danluu.com/look-stupid/"]} {:tag :pubDate, :attrs nil, :content ["Thu, 21 Oct 2021 00:00:00 +0000"]} {:tag :guid, :attrs nil, :content ["https://danluu.com/look-stupid/"]} {:tag :description, :attrs nil, :content ["\n\n<p>People frequently<sup class=\"footnote-ref\" id=\"fnref:F\"><a rel=\"footnote\" href=\"#fn:F\">1</a></sup> think that I'm very stupid. I don't find this surprising, since I don't mind if other people think I'm stupid, which means that I don't adjust my behavior to avoid seeming stupid, which results in people thinking that I'm stupid. Although there are some downsides to people thinking that I'm stupid, e.g., failing interviews where the interviewer very clearly thought I was stupid, I think that, overall, the upsides of being willing to look stupid have greatly outweighed the downsides.</p>\n\n<p>I don't know why this one example sticks in my head but, for me, the most memorable example of other people thinking that I'm stupid was from college. I've had numerous instances where more people thought I was stupid and also where people thought the depths of my stupidity was greater, but this one was really memorable for me.</p>\n\n<p>Back in college, there was one group of folks that, for whatever reason, stood out to me as people who really didn't understand the class material. When they talked, they said things that didn't make any sense, they were struggling in the classes and barely passing, etc. I don't remember any direct interactions but, one day, a friend of mine who also knew them remarked to me, &quot;did you know [that group] thinks you're really dumb?&quot;. I found that really delightful and asked why. It turned out the reason was that I asked really stupid sounding questions.</p>\n\n<p>In particular, it's often the case that there's a seemingly obvious but actually incorrect reason something is true, a slightly less obvious reason the thing seems untrue, and then a subtle and complex reason that the thing is actually true<sup class=\"footnote-ref\" id=\"fnref:T\"><a rel=\"footnote\" href=\"#fn:T\">2</a></sup>. I would regularly figure out that the seemingly obvious reason was wrong and then ask a question to try to understand the subtler reason, which sounded stupid to someone who thought the seemingly obvious reason was correct or thought that the refutation to the obvious but incorrect reason meant that the thing was untrue.</p>\n\n<p>The benefit from asking a stupid sounding question is small in most particular instances, but the compounding benefit over time is quite large and I've observed that people who are willing to ask dumb questions and think &quot;stupid thoughts&quot; end up understanding things much more deeply over time. Conversely, when I look at people who have a very deep understanding of topics, many of them frequently ask naive sounding questions and continue to apply one of the techniques that got them a deep understanding in the first place.</p>\n\n<p>I think I first became sure of something that I think of as a symptom of the underlying phenomenon via playing competitive video games when I was in high school. There were few enough people playing video games online back then that you'd basically recognize everyone who played the same game and could see how much everyone improved over time. Just like <a href=\"https://danluu.com/p95-skill/\">I saw when I tried out video games again a couple years ago</a>, most people would blame external factors (lag, luck, a glitch, teammates, unfairness, etc.) when they &quot;died&quot; in the game. The most striking thing about that was that people who did that almost never became good and never became great. I got pretty good at the game<sup class=\"footnote-ref\" id=\"fnref:B\"><a rel=\"footnote\" href=\"#fn:B\">3</a></sup> and my &quot;one weird trick&quot; was to think about what went wrong every time something went wrong and then try to improve. But most people seemed more interested in making an excuse to avoid looking stupid (or maybe feeling stupid) in the moment than actually improving, which, of course, resulted in them having many more moments where they looked stupid in the game.</p>\n\n<p>In general, I've found willingness to look stupid to be very effective. Here are some more examples:</p>\n\n<ul>\n<li>Going into an Apple store and asking for (and buying) the computer that comes in the smallest box, which I had a good reason to want at the time\n\n<ul>\n<li>The person who helped me, despite being very polite, also clearly thought I was a bozo and kept explaining things like &quot;the size of the box and the size of the computer aren't the same&quot;. Of course I knew that, but I didn't want to say something like &quot;I design CPUs. I understand the difference between the size of the box the computer comes and in the size of the computer and I know it's very unusual to care about the size of the box, but I really want the one that comes in the smallest box&quot;. Just saying the last bit without establishing any kind of authority didn't convince the person</li>\n<li>I eventually asked them to humor me and just bring out the boxes for the various laptop models so I could see the boxes, which they did, despite clearly thinking that my decision making process made no sense (<a href=\"https://twitter.com/altluu/status/1452704171447123969\">I also tried explaining why I wanted the smallest box but that didn't work</a>)</li>\n</ul></li>\n<li>Covid: I took this seriously relatively early on and bought a half mask respirator on 2020-01-26 and was using N95s I'd already had on hand for the week before (IMO, the case that covid was airborne and that air filtration would help was very strong based on the existing literature on SARS contact tracing, filtration of viruses from air filters, and viral load)\n\n<ul>\n<li>It wasn't until many months later that people didn't generally look at me like I was an idiot, and even as late 2020-08, I would sometimes run into people who would verbally make fun me</li>\n<li>On the flip side, the person I was living with at the time didn't want to wear the mask I got her since she found it too embarrassing to wear a mask when no one else was and became one of the early bay area covid cases, which gave her a case of long covid that floored her for months</li>\n<li>A semi-related one is that, when Canada started doing vaccines, I wanted to get Moderna even though the general consensus online and in my social circles was that Pfizer was preferred\n\n<ul>\n<li>One reason for this was it wasn't clear if the government was going to allow mixing vaccines and the delivery schedule implied that there would be a very large shortage of Pfizer for 2nd doses as well as a large supply of Moderna</li>\n<li>Another thought that had crossed my mind was that Moderna is basically &quot;more stuff&quot; than Pfizer and might convey better immunity in some cases, in the same way that some populations get high-dose flu shots to get better immunity</li>\n</ul></li>\n</ul></li>\n<li>Work: I generally don't worry about proposals or actions looking stupid\n\n<ul>\n<li>I can still remember the first time I explicitly ran into this. This was very early on my career, when I was working on chip verification. Shortly before tape-out, the head of verification wanted to use our compute resources to re-run a set of tests that had virtually no chance of finding any bugs (they'd been run thousands of times before) instead of running the usual mix of tests, which would include a lot of new generated tests that had a much better chance of finding a bug (this was both logically and empirically true). I argued that we should run the tests that reduced the odds of shipping with a show stopping bug (which would cost us millions of dollars and delay shipping by three months), but the head of the group said that we would look stupid and incompetent if there was a bug that could've been caught by one of our old &quot;golden&quot; tests that snuck in since the last time we'd run those tests\n\n<ul>\n<li>At the time, I was shocked that somebody would deliberately do the wrong thing in order to reduce the odds of potentially looking stupid (and, really, only looking stupid to people who wouldn't understand the logic of running the best available mix of tests; since there weren't non-technical people anywhere in the management chain, anyone competent should understand the reasoning) but now that I've worked at various companies in multiple industries, I see that most people would choose to do the wrong thing to avoid potentially looking stupid to people who are incompetent. I see the logic, but I think that it's self-sabotaging to behave that way and that the gains to my career for standing up for what I believe are the right thing have been so large that, even if the next ten times I do so, I get unlucky and it doesn't work out, that still won't erase the gains I've made from having done the right thing many times in the past</li>\n</ul></li>\n</ul></li>\n<li>Air filtration: I did a bit of looking into the impact of air quality on health and bought air filters for my apartment in 2012\n\n<ul>\n<li>Friends have been chiding about this for years and strangers, dates, and acquaintances, will sometimes tell me, with varying levels of bluntness, that I'm being paranoid and stupid</li>\n<li>I added more air filtration capacity when I moved to a wildfire risk area <a href=\"https://mobile.twitter.com/altluu/status/1409762306452459520\">after looking into wildfire risk</a> which increased the rate and bluntness of people telling me that I'm weird for having air filters\n\n<ul>\n<li>I've been basically totally unimpacted by wildfire despite living through a fairly severe wildfire season twice</li>\n<li>Other folks I know experienced some degree of discomfort, with a couple people developing persistent issues after the smoke exposure (in one case, persistent asthma, which they didn't have before or at least hadn't noticed before)</li>\n</ul></li>\n</ul></li>\n<li>Learning things that are hard for me: this is a &quot;feeling stupid&quot; thing and not a &quot;looking stupid&quot; thing, but when I struggle with something, I feel really dumb, as in, I have a feeling/emotion that I would verbally describe as &quot;feeling dumb&quot;\n\n<ul>\n<li>When I was pretty young, I think before I was a teenager, I noticed that this happened when I learned things that were hard for me and tried to think of this feeling as &quot;the feeling of learning something&quot; instead of &quot;feeling dumb&quot;, which half worked (I now associate that feeling with the former as well as the latter)</li>\n</ul></li>\n<li>Asking questions: covered above, but I frequently ask questions when there's something I don't understand or know, from basic stuff, &quot;what does [some word] mean?&quot; to more subtle stuff.\n\n<ul>\n<li>On the flip side, one of the most common failure modes I see with junior engineers is when someone will be too afraid to look stupid to ask questions and then learn very slowly as a result; in some cases, this is so severe it results in them being put on a PIP and then getting fired\n\n<ul>\n<li>I'm sure there are other reasons this can happen, like not wanting to bother people, but in the cases where I've been close enough to the situation to ask, it was always embarrassment and fear of looking stupid</li>\n<li>I try to be careful to avoid this failure mode when onboarding interns and junior folks and have generally been sucessful, but it's taken me up to six weeks to convince people that it's ok for them to ask questions and, until that happens, I have to constantly ask them how things are going to make sure they're not stuck. That works fine if someone is my intern, but I can observe that many intern and new hire mentors do not do this and that often results in a bad outcome for all parties\n\n<ul>\n<li>In almost every case, the person had at least interned at other companies, but they hadn't learned that it was ok to ask questions. P.S. if you're a junior engineer at a place where it's not ok to ask questions, you should look for another job if circumstances permit</li>\n</ul></li>\n</ul></li>\n</ul></li>\n<li>Not making excuses for failures: covered above for video games, but applies a lot more generally</li>\n<li>When learning, deliberately playing around in the area between success and failure (this applies to things like video games and sports as well as abstract intellectual pursuits)\n\n<ul>\n<li>An example would be, when learning to climb, repeatedly trying the same easy move over and over again in various ways to understand what works better and what works worse. I've had strangers make fun of me and literally point at me and make snide comments to their friends while I'm doing things like this</li>\n<li>When learning to drive, I wanted to set up some cones and drive so that I barely hit them, to understand where the edge of the car is. My father thought this idea was very stupid and I should just not hit things like curbs or cones</li>\n</ul></li>\n<li>Car insurance: the last time I bought car insurance, I had to confirm three times that I only wanted coverage for damage I do to others with no coverage for damage to my own vehicle if I'm at fault. The insurance agent was unable to refrain from looking at me like I'm an idiot and was more incredulous each time they asked if I was really sure</li>\n<li>The styling and content on this website: I regularly get design folks and typographers telling me how stupid the design is, frequently in ways that become condescending very quickly if I engage with them\n\n<ul>\n<li>But, when I tested out switching to the current design from the generally highly lauded Octopress design, this one got much better engagement when a user landed on the site and also appeared to get passed around a lot more as well</li>\n<li>When I've compared my traffic numbers to major coprorate blogs, my blog completely dominates most &lt; $100B companies (e.g., it gets an order of magnitude more traffic than my employer's blog and my employer is a $50B company)</li>\n<li>When I started my blog (and this is still true today), writing advice for programming blogs <a href=\"https://twitter.com/danluu/status/1437539076324790274\">was to keep it short, maybe 500 to 1000 words</a>. Most of my blog posts are 5000 to 10000 words</li>\n</ul></li>\n<li>Taking my current job, which almost everyone thought was a stupid idea\n\n<ul>\n<li>Closely related: quitting my job at Centaur to attend <a href=\"https://www.recurse.com/scout/click?t=b504af89e87b77920c9b60b2a1f6d5e8\">RC</a> and then eventually changing fields into software (I don't think this would be considered as stupid now, but it was thought to be a very stupid thing to do in 2013)</li>\n</ul></li>\n<li>Learning a sport or video game: I try things out to understand what happens when you do them, which often results in other people thinking that I'm a complete idiot when the thing looks stupid, but being willing to look stupid helps me improve relatively quickly</li>\n<li>Medical care: I've found that a lot of doctors are very confident in their opinion and get condescending pretty fast if you disagree\n\n<ul>\n<li>And yet, in the most extreme case, I would have died if I listened to my doctor; in the next most extreme case, I would have gone blind</li>\n<li>When getting blood draws, I explain to people that I'm deceptively difficult to draw from and tell them what's worked in the past\n\n<ul>\n<li>About half the time, the nurse or phlebotomist takes my comments seriously, generally resulting in a straightforward and painless or nearly painless blood draw</li>\n<li>About half the time, the nurse or phlebotomist looks at me like I'm an idiot and makes angry and/or condescending comments towards me; so far, everyone who's done this has failed to draw blood and/or given me a hematoma</li>\n<li>I've had people tell me that I'm probably stating my preferences an offensive way and that I should be more polite; I've then invited them along with me to observe and no one has ever had a suggestion on how I could state things different to elicit a larger fraction of positive responses; in general, people are shocked and upset when they see how nurses and phlebotomists respond</li>\n<li>In retrospect, I should probably just get up and leave when someone has the &quot;bad&quot; response, which will probably increase the person's feeling that I'm stupid</li>\n<li>One issue I have (and not the main one that makes it hard to &quot;get a stick&quot;) is that, during a blood draw, the blood will slow down and then usually stop. Some nurses like to wiggle the needle around to see if that starts things up again, which sometimes works (maybe 50/50) and will generally leave me with a giant bruise or a hematoma or both. After this happened a few times, I asked if getting my blood flowing (e.g., by moving around a lot before a blood draw) could make a difference and every nurse or phlebotomist I talked to said that was silly and that it wouldn't make any difference. I tried it anyway and that solved this problem, although I still have the problem of being hard to stick properly</li>\n</ul></li>\n</ul></li>\n<li>Interviews: I'm generally not (perhaps not ever?) adversarial in interviews, but I try to say things that I think are true and try to avoid saying things that I think are false and this frequently <a href=\"https://twitter.com/danluu/status/1447268693075841024\">causes interviews to think that I'm stupid</a></li>\n<li>Generally trying to improve at things as well as being earnest\n\n<ul>\n<li>Even before &quot;tryhard&quot; was an insult, a lot of people in my extended social circles thought that being a tryhard was idiotic and that one shouldn't try and should instead play it cool (this was before I worked as an engineer; as an engineer, I think that effort is more highly respected than among my classmates from school as well as internet folks I knew back when I was in school)</li>\n</ul></li>\n<li>Generally admitting when I'm bad or untalented at stuff, e.g., <a href=\"https://danluu.com/learning-to-program/\">mentioning that I struggled to learn to program in this post</a>; an interviewer at Jane Street really dug into what I'd written in that post and tore me a new one for that post (it was the most hostile interview I've ever experienced by a very large margin), which is the kind of thing that sometimes happens when you're earnest and put yourself out there, but I still view the upsides as being greater than the downsides</li>\n<li>Recruiting: I have an unorthodox recruiting pitch which candidly leads with the downsides, often causing people to say that I'm a terrible recruiter (or sarcastically say that I'm a great recruiter); I haven't publicly written up the pitch (yet?) because it's negative enough that I'm concerned that I'd be fired for putting it on the internet\n\n<ul>\n<li>I have never failed to close a full-time candidate (I once failed to close an intern candidate) and have brought in a lot of people who never would've considered working for us otherwise. My recruiting pitch sounds comically stupid, but it's much more effective than the standard recruiting speil most people give</li>\n</ul></li>\n<li>Posting things on the internet: self explanatory</li>\n</ul>\n\n<p>Although most of the examples above are &quot;real life&quot; examples, being willing to look stupid is also highly effective at work. Besides the obvious reason that it allows you to learn faster and become more effective, it also makes it much easier to find high ROI ideas. If you go after trendy or reasonable sounding ideas, to do something really extraordinary, you have to have better ideas/execution than everyone else working on the same problem. But if you're thikning about ideas that most people consider too stupid to consider, you'll often run into ideas that are both very high ROI as well as simple and easy that anyone could've done had they not dismissed the idea out of hand. It may still technically be true that you need to have better execution than anyone else who's trying the same thing, but if no one else trying the same thing, that's easy to do!</p>\n\n<p>I don't actually have to be nearly as smart or work nearly as hard as most people to get good results. If I try to solve some a problem by doing what everyone else is doing and go looking for problems where everyone else is looking, if I want to do something valuable, I'll have to do better than a lot of people, maybe even better than everybody else if the problem is really hard. If the problem is considered trendy, a lot of very smart and hardworking people will be treading the same ground and doing better than that is very difficult. But I have a dumb thought, one that's too stupid sounding for anyone else to try, I don't necessarily have to be particularly smart or talented or hardworking to come up with valuable solutions. Often, the dumb solution is something any idiot could've come up with and the reason the problem hasn't been solved is because no one was willing to think the dumb thought until an idiot like me looked at the problem.</p>\n\n<p>Overall, I view the upsides of being willing to look stupid as much larger than the downsides. When it comes to things that aren't socially judged, like winning a game, understanding something, or being able to build things due to having a good understanding, it's all upside. There can be downside for things that are &quot;about&quot; social judgement, like interviews and dates but, even there, I think a lot of things that might seem like downsides are actually upsides.</p>\n\n<p>For example, if a date thinks I'm stupid because I ask them what a word means, so much so that they show it in their facial expression and/or tone of voice, I think it's pretty unlikely that we're compatible, so I view finding that out sooner rather than later as upside and not downside.</p>\n\n<p>Interviews are the case where I think there's the most downside since, at large companies, the interviewer likely has no connection to the job or your co-workers, so them having a pattern of interaction that I would view as a downside has no direct bearing on the work environment I'd have if I were offered the job and took it. There's probably some correlation but I can probably get much more signal on that elsewhere. But I think that being willing to say things that I know have a good chance of causing people to think I'm stupid is a deeply ingrained enough habit that it's not worth changing just for interviews and I can't think of another context where the cost is nearly as high as it is in interviews. In principle, I could probably change how I filter what I say only in interviews, but I think that would be a very large amount of work and not really worth the cost. An easier thing to do would be to change how I think so that I reflexively avoid thinking and saying &quot;stupid&quot; thoughts, which a lot of folks seem to do, but that seems even more costly.</p>\n\n<h3 id=\"appendix-do-you-try-to-avoid-looking-stupid\">Appendix: do you try to avoid looking stupid?</h3>\n\n<p>On reading a draft of this, Ben Kuhn remarked,</p>\n\n<blockquote>\n<p>[this post] caused me to realize that I'm actually very bad at this, at least compared to you but perhaps also just bad in general.</p>\n\n<p>I asked myself &quot;why can't Dan just avoid saying things that make him look stupid specifically in interviews,&quot; then I started thinking about what the mental processes involved must look like in order for that to be impossible, and realized they must be extremely different from mine. Then tried to think about the last time I did something that made someone think I was stupid and realized I didn't have a readily available example)</p>\n</blockquote>\n\n<p>One problem I expect this post to have is that most people will read this and decide that they're very willing to look stupid. This reminds me of how most people, when asked, think that they're creative, innovative, and take big risks. I think that feels true since people often operate at the edge of their comfort zone, but there's a difference between feeling like you're taking big risks and taking big risks, e.g., when asked, someone I know who is among the most conservative people I know thinks that they take a lot of big risks and names things like sometimes jaywalking as risk that they take.</p>\n\n<p>This might sound ridiculous, <a href=\"https://danluu.com/everything-is-broken/\">as ridiculous as saying that I run into hundreds to thousands of software bugs per week</a>, but I think I run into someone who thinks that I'm an idiot in a way that's obvious to me around once a week. The car insurance example is from a few days ago, and if I wanted to think of other recent examples, there's a long string of them.</p>\n\n<p>If you don't regularly have people thinking that you're stupid, I think it's likely that at least one of the following is true</p>\n\n<ul>\n<li>You have extremely filtered interactions with people and basically only interact with people of your choosing and you have filtered out any people who have the reactions describe in this post\n\n<ul>\n<li>If you count internet comments, then you do not post things to the internet or do not read internet comments</li>\n</ul></li>\n<li>You are avoiding looking stupid</li>\n<li>You are not noticing when people think you're stupid</li>\n</ul>\n\n<p>I think the last one of those is unlikely because, while I sometimes have interactions like the school one described, where the people were too nice to tell me that they think I'm stupid and I only found out via a third party, just as often, the person very clearly wants me to know that they think I'm stupid. The way it happens reminds me of being a pedestrian in NYC, where, when a car tries to cut you off when you have right of way and fails (e.g., when you're crossing a crosswalk and have the walk signal and the driver guns it to try to get in front of you to turn right), the driver will often scream at you and gesture angrily until you acknowledge them and, if you ignore them, will try very hard to get your attention. In the same way that it seems very important to some people who are angry that you know they're angry, many people seem to think it's very important that you know that they think that you're stupid and will keep increasing the intensity of their responses until you acknowledge that they think you're stupid.</p>\n\n<p>One thing that might be worth noting is that I don't go out of my way to sound stupid or otherwise be non-conformist. If anything, it's the opposite. I generally try to conform in areas that aren't important to me when it's easy to conform, e.g., I dressed more casually in the office on the west coast than on the east coast since it's not important to me to convey some particular image based on how I dress and I'd rather spend my &quot;weirdness points&quot; on pushing radical ideas than on dressing unusually. After I changed how I dressed, one of the few people in the office who dressed really sharply in a way that would've been normal in the east coast office jokingly said to me, &quot;so, the west coast got to you, huh?&quot; and a few other people remarked that I looked a lot less stuffy/formal.</p>\n\n<p>Another thing to note is that &quot;avoiding looking stupid&quot; seems to usually go beyond just filtering out comments or actions that might come off as stupid. Most people I talk to (and Ben is an exception here) have a real aversion evaluating stupid thoughts and (I'm guessing) also to having stupid thoughts. When I have an idea that sounds stupid, it's generally (and again, Ben is an exception here) extremely difficult to get someone to really consider the idea. Instead, most people reflexively reject the idea without really engaging with it at all and (I'm guessing) the same thing happens inside their heads when a potentially stupid sounding thought might occur to them. I think the danger here is not having a concious process that lets you decide to broadcast or not broadcast stupid sounding thoughts (that seems great if it's low overhead), and instead it's having some non-concious process automatically reject thinking about stupid sounding things.</p>\n\n<p>Of course, stupid-sounding thoughts are frequently wrong, so, if you're not going to rely on social proof to filter out bad ideas, you'll have to hone your intuition or find trusted friends/colleagues who are able to catch your stupid-sounding ideas that are actually stupid. That's beyond the scope of this post. but I'll note that <a href=\"https://danluu.com/p95-skill/\">because almost no one attempts to hone their intuition for this kind of thing, it's very easy to get relatively good at it by just trying to do it at all</a>.</p>\n\n<h3 id=\"appendix-stories-from-other-people\">Appendix: stories from other people</h3>\n\n<p>A disproportionate fraction of people whose work I really respect operate in a similar way to me with respect to looking stupid and also have a lot of stories about looking stupid.</p>\n\n<p>One example from Laurence Tratt is from when he was job searching:</p>\n\n<blockquote>\n<p>I remember being rejected from a job at my current employer because a senior person who knew me told other people that I was &quot;too stupid&quot;. For a long time, I found this bemusing (I thought I must be missing out on some deep insights), but eventually I found it highly amusing, to the point I enjoy playing with it.</p>\n</blockquote>\n\n<p>Another example: the other day, when I was talking to Gary Bernhardt, he told me a story about a time when he was chatting with someone who specialized in microservices on Kubernetes for startups and Gary said that he thought that most small (by transaction volume) startups could get away with being on a managed platform like Heroku or Google App Engine. The more Gary explained about his opinion, the more sure the person was that Gary was stupid.</p>\n\n<h3 id=\"appendix-context\">Appendix: context</h3>\n\n<p>There are a lot of contexts that I'm not exposed to where it may be much more effective to train yourself to avoid looking stupid or incompetent, e.g., <a href=\"https://twitter.com/apartovi/status/1449856639331340289\">see this story by Ali Partovi about how his honesty led to Paul Graham's company being acquired by Yahoo instead of his own, which eventually led to Paul Graham founding YC and becoming one of the most well-known and influential people in the valley</a>. If you're in a context where it's more important to look competent than to be competent then this post doesn't apply to you. Personally, I've tried to avoid such contexts, although they're probably more lucrative than the contexts I operate in.</p>\n\n<p>Thanks to Ben Kuhn, Laurence Tratt, Jeshua Smith, Niels Olson, Justin Blank, Tao L., Colby Russell, Anja Boskovic, David Coletta, and Ahmad Jarara for comments/corrections/discussion.</p>\n<div class=\"footnotes\">\n\n<hr />\n\n<ol>\n<li id=\"fn:F\">This happens in a way that I notice something like once a week and it seems like it must happen much more frequently in ways that I don't notice.\n <a class=\"footnote-return\" href=\"#fnref:F\"><sup>[return]</sup></a></li>\n\n<li id=\"fn:T\"><p>A semi-recent example of this from my life is when I wanted to understand why wider tires have better grip. A naive reason one might think this is true is that wider tire = larger contact patch = more friction, and a lot of people seem to believe the naive reason. A reason the naive reason is wrong is because, as long as the tire is inflated semi-reasonably, given a fixed vehicle weight and tire pressure, the total size of the tire's contact patch won't change when tire width is changed. Another naive reason that the original naive reason is wrong is that, at a &quot;spherical cow&quot; level of detail, the level of grip is unrelated to the contact patch size.</p>\n\n<p>Most people I talked who don't race cars (e.g., autocross, drag racing, etc.) and <a href=\"https://twitter.com/danluu/status/1304093800474636288\">the top search results online used the refutation to the naive reason plus an incorrect application of high school physics to incorrectly conclude that varying tire width has no effect on grip</a>.</p>\n\n<p>But there is an effect and the reason is subtler than more width = larger contact patch.</p>\n <a class=\"footnote-return\" href=\"#fnref:T\"><sup>[return]</sup></a></li>\n<li id=\"fn:B\">I was arguably #1 in the world one season, when I put up a statistically dominant performance and my team won every game I played even though I disproportionately played in games against other top teams (and we weren't undefeated and other top players on the team played in games we lost).\n <a class=\"footnote-return\" href=\"#fnref:B\"><sup>[return]</sup></a></li>\n</ol>\n</div>\n"]}]} {:tag :item, :attrs nil, :content [{:tag :title, :attrs nil, :content ["What to learn"]} {:tag :link, :attrs nil, :content ["https://danluu.com/learn-what/"]} {:tag :pubDate, :attrs nil, :content ["Mon, 18 Oct 2021 00:00:00 +0000"]} {:tag :guid, :attrs nil, :content ["https://danluu.com/learn-what/"]} {:tag :description, :attrs nil, :content ["\n\n<p>It's common to see people advocate for learning skills that they have or using processes that they use. For example, Steve Yegge has a set of blog posts where he recommends reading compiler books and learning about compilers. His reasoning is basically that, if you understand compilers, you'll see compiler problems everywhere and will recognize all of the cases where people are solving a compiler problem without using compiler knowledge. Instead of hacking together some half-baked solution that will never work, you can apply a bit of computer science knowledge to solve the problem in a better way with less effort. That's not untrue, but it's also not a reason to study compilers in particular because you can say that about many different areas of computer science and math. Queuing theory, computer architecture, mathematical optimization, operations research, etc.</p>\n\n<p>One response to that kind of objection is to say that <a href=\"https://twitter.com/danluu/status/899141882760110081\">one should study everything</a>. While being an extremely broad generalist can work, it's gotten much harder to &quot;know a bit of everything&quot; and be effective because there's more of everything over time (in terms of both breadth and depth). And even if that weren't the case, I think saying should is too strong; whether or not someone enjoys having that kind of breadth is a matter of taste. Another approach that can also work, one that's more to my taste, is to, <a href=\"https://alumni.media.mit.edu/~cahn/life/gian-carlo-rota-10-lessons.html\">as Gian Carlo Rota put it</a>, learn a few tricks:</p>\n\n<blockquote>\n<p>A long time ago an older and well known number theorist made some disparaging remarks about Paul Erdos' work. You admire contributions to mathematics as much as I do, and I felt annoyed when the older mathematician flatly and definitively stated that all of Erdos' work could be reduced to a few tricks which Erdos repeatedly relied on in his proofs. What the number theorist did not realize is that other mathematicians, even the very best, also rely on a few tricks which they use over and over. Take Hilbert. The second volume of Hilbert's collected papers contains Hilbert's papers in invariant theory. I have made a point of reading some of these papers with care. It is sad to note that some of Hilbert's beautiful results have been completely forgotten. But on reading the proofs of Hilbert's striking and deep theorems in invariant theory, it was surprising to verify that Hilbert's proofs relied on the same few tricks. Even Hilbert had only a few tricks!</p>\n</blockquote>\n\n<p>If you look at how people succeed in various fields, you'll see that this is a common approach. For example, <a href=\"https://judoinfo.com/weers1/\">this analysis of world-class judo players found that most rely on a small handful of throws</a>, concluding<sup class=\"footnote-ref\" id=\"fnref:J\"><a rel=\"footnote\" href=\"#fn:J\">1</a></sup></p>\n\n<blockquote>\n<p>Judo is a game of specialization. You have to use the skills that work best for you. You have to stick to what works and practice your skills until they become automatic responses.</p>\n</blockquote>\n\n<p>If you watch an anime or a TV series &quot;about&quot; fighting, people often improve by increasing the number of techniques they know because that's an easy thing to depict but, in real life, getting better at techniques you already know is often more effective than having a portfolio of hundreds of &quot;moves&quot;.</p>\n\n<p><a href=\"https://staffeng.com/stories/joy-ebertz\" rel=\"nofollow\">Relatedly, Joy Ebertz says</a>:</p>\n\n<blockquote>\n<p>One piece of advice I got at some point was to amplify my strengths. All of us have strengths and weaknesses and we spend a lot of time talking about areas of improvement. It can be easy to feel like the best way to advance is to eliminate all of those. However, it can require a lot of work and energy to barely move the needle if its truly an area were weak in. Obviously, you still want to make sure you dont have any truly bad areas, but assuming youve gotten that, instead focus on amplifying your strengths. How can you turn something youre good at into your superpower?</p>\n</blockquote>\n\n<p>I've personally found this to be true in a variety of disciplines. While it's really difficult to measure programmer effectiveness in anything resembling an objective manner, this isn't true of some things I've done, like competitive video games (a very long time ago at this point, back before there was &quot;real&quot; money in competitive gaming), the thing that took me from being a pretty decent player to a <a href=\"https://danluu.com/look-stupid/#fn:B\">very good player</a> was abandoning practicing things I wasn't particularly good at and focusing on increasing the edge I had over everybody else at the few things I was unusually good at.</p>\n\n<p>This can work for games and sports because you can get better maneuvering yourself into positions that take advantage of your strengths as well as avoiding situations that expose your weaknesses. I think this is actually more effective at work than it is in sports or gaming since, unlike in competitive endeavors, you don't have an opponent who will try to expose your weaknesses and force you into positions where your strengths are irrelevant. If I study queuing theory instead of compilers, a rival co-worker isn't going to stop me from working on projects where queuing theory knowledge is helpful and leave me facing a field full of projects that require compiler knowledge.</p>\n\n<p>One thing that's worth noting is that skills don't have to be things people would consider fields of study or discrete techniques. For the past three years, the main skill I've been applying and improving is something you might call &quot;looking at data&quot;; the term is in quotes because I don't know of a good term for it. I don't think it's what most people would think of as &quot;statistics&quot;, in that I don't often need to do anything as sophisticated as logistic regression, let alone actually sophisticated. Perhaps one could argue that this is something data scientists do, but if I look at what I do vs. what data scientists we hire do as well as what we screen for in data scientist interviews, we don't appear to want to hire data scientists with the skill I've been working on nor do they do what I'm doing (this is a long enough topic that I might turn it into its own post at some point).</p>\n\n<p>Unlike Matt Might or Steve Yegge, I'm not going to say that you should take a particular approach, but I'll say that working on a few things and not being particularly well rounded has worked for me in multiple disparate fields and it appears to work for a lot of other folks as well.</p>\n\n<p>If you want to take this approach, this still leaves the question of what skills to learn. This is one of the most common questions I get asked and I think my answer is probably not really what people are looking for and not very satisfying since it's both <a href=\"https://twitter.com/danluu/status/1428445465662603272\">obvious and difficult to put into practice</a>.</p>\n\n<p>For me, two ingredients for figuring out what to spend time learning are having a relative aptitude for something (relative to other things I might do, not relative to other people) and also having a good environment in which to learn. To say that someone should look for those things is so vague that's it's nearly useless, but it's still better than the usual advice, which boils down to &quot;learn what I learned&quot;, which results in advice like &quot;Career pro tip: if you want to get good, REALLY good, at designing complex and stateful distributed systems at scale in real-world environments, learn functional programming. It is an almost perfectly identical skillset.&quot; or the even more extreme claims from some language communities, like Chuck Moore's claim that Forth is <a href=\"https://danluu.com/boring-languages/\">at least 100x as productive as boring languages</a>.</p>\n\n<p>I took generic internet advice early in my career, including language advice (this was when much of this kind of advice was relatively young and it was not yet possible to easily observe that, despite many people taking advice like this, people who took this kind of advice were not particularly effective and people who are particularly effective were not likely to have taken this kind of advice). I learned <a href=\"https://twitter.com/sc13ts/status/1448003352655060997\">Haskell, Lisp, Forth</a>, <a href=\"https://malisper.me/there-is-more-to-programming-than-programming-languages/\">etc</a>. At one point in my career, I was on a two person team that implemented what might still be, a decade later, the highest performance Forth processor in existence (it was a 2GHz IPC-oriented processor) and I programmed it as well (there were good reasons for this to be a stack processor, so Forth seemed like as good a choice as any). <a href=\"https://yosefk.com/blog/my-history-with-forth-stack-machines.html\">Like Yossi Kreinin, I think I can say that I spent more effort than most people have becoming proficient in Forth, and like him, not only did I not find it find it to be a 100x productivity tool, it wasn't clear that it would, in general, even be 1x on productivity</a>. To be fair, a number of other tools did better than 1x on productivity but, overall, I think following internet advice was very low ROI and the things that I learned that were high ROI weren't things people were recommending.</p>\n\n<p>In retrospect, when people said things like &quot;Forth is very productive&quot;, what I suspect they really meant was &quot;Forth makes me very productive and I have not considered how well this generalizes to people with different aptitudes or who are operating in different contexts&quot;. I find it totally plausible that Forth (or Lisp or Haskell or any other tool or technique) does work very well for some particular people, but I think that people tend to overestimate how much something working for them means that it works for other people, <a href=\"https://twitter.com/danluu/status/1355661542155378688\">making advice generally useless because it doesn't distinguish between advice that's aptitude or circumstance specific and generalizable advice, which is in stark contrast to fields where people actually discuss the pros and cons of particular techniques</a><sup class=\"footnote-ref\" id=\"fnref:F\"><a rel=\"footnote\" href=\"#fn:F\">2</a></sup>.</p>\n\n<p>While a coach can give you advice that's tailored to you 1 on 1 or in small groups, that's difficult to do on the internet, which is why the best I can do here is the uselessly vague &quot;pick up skills that are suitable for you&quot;. Just for example, two skills that clicked for me are &quot;having an adversarial mindset&quot; and &quot;looking at data&quot;. A perhaps less useless piece of advice is that, if you're having a hard time identifying what those might be, you can ask people who know you very well, e.g., my manager and Ben Kuhn independently named coming up with solutions that span many levels of abstraction as a skill of mine that I frequently apply (and I didn't realize I was doing that until they pointed it out).</p>\n\n<p>Another way to find these is to look for things you can't help but do that most other people don't seem to do, which is true for me of both &quot;looking at data&quot; and &quot;having an adversarial mindset&quot;. Just for example, on having an adversarial mindset, when a company I was working for was beta testing a new custom bug tracker, I filed some of the first bugs on it and put unusual things into the fields to see if it would break. Some people really didn't understand why anyone would do such a thing and were baffled, disgusted, or horrified, but a few people (including the authors, who I knew wouldn't mind), really got it and were happy to see the system pushed past its limits. Poking at the limits of a system to see where it falls apart doesn't feel like work to me; it's something that I'd have to stop myself from doing if I wanted to not do it, which made spending a decade getting better at testing and verification techniques felt like something hard not to do and not work. Looking deeply into data is one I've spent more than a decade on at this point and it's another one that, to me, emotionally feels almost wrong to not improve at.</p>\n\n<p>That these things are suited to me is basically due to my personality, and not something inherent about human beings. Other people are going to have different things that really feel easy/right for them, which is great, since if everyone was into looking at data and no one was into building things, that would be very problematic (although, IMO, looking at data is, on average, underrated).</p>\n\n<p>The other major ingredient in what I've tried to learn is finding environments that are conducive to learning things that line up with my skills that make sense for me. Although suggesting that other people do the same sounds like advice that's so obvious that it's useless, based on how I've seen people select what team and company to work on, I think that almost nobody does this and, as a result, discussing this may not be completely useless.</p>\n\n<p>An example of not doing this which typifies what I usually see is a case I just happened to find out about because I chatted with a manager about why their team had lost their new full-time intern conversion employee. I asked them about it since it was unusual for that manager to lose anyone since they're very good at retaining people and have low turnover on their teams. It turned out that their intern had wanted to work on infra, but had joined this manager's product team because they didn't know that they could ask to be on a team that matched their preferences. After the manager found out, the manager wanted the intern to be happy and facilitated a transfer to an infra team. In this case, this was a double whammy since the new hire doubly didn't consider working in an environment conducive for learning the skills they wanted. They made no attempt to work in the area they were interested in and then they joined a company that has a dysfunctional infra org that generally has poor design and operational practices, making the company a relatively difficult place to learn about infra on top of not even trying to land on an infra team. While that's an unusually bad example, in the median case that I've seen, people don't make decisions that result in particularly good outcomes with respect to learning even though good opportunities to learn are one of the top things people say that they want.</p>\n\n<p>For example, Steve Yegge has noted:</p>\n\n<blockquote>\n<p>The most frequently-asked question from college candidates is: &quot;what kind of training and/or mentoring do you offer?&quot;\n...\nOne UW interviewee just told me about Ford Motor Company's mentoring program, which Ford had apparently used as part of the sales pitch they do for interviewees. [I've elided the details, as they weren't really relevant. -stevey 3/1/2006] The student had absorbed it all in amazing detail. That doesn't really surprise me, because it's one of the things candidates care about most.</p>\n</blockquote>\n\n<p>For myself, I was lucky that my first job, Centaur, was a great place to develop having an adversarial mindset with respect to testing and verification. When I compare what the verification team there accomplished, it's comparable to peer projects at other companies that employed much larger teams to do very similar things with similar or worse effectiveness, implying that the team was highly productive, which made that a really good place to learn.</p>\n\n<p>Moreover, I don't think I could've learned as quickly on my own or by trying to follow advice from books or the internet. I think that <a href=\"https://danluu.com/hardware-unforgiving/\">people who are really good at something have too many bits of information in their head about how to do it for that information to really be compressible into a book, let alone a blog post</a>. In sports, good coaches are able to convey that kind of information over time, but I don't know of anything similar for programming, so I think the best thing available for learning rate is to find an environment that's full of experts<sup class=\"footnote-ref\" id=\"fnref:M\"><a rel=\"footnote\" href=\"#fn:M\">3</a></sup>.</p>\n\n<p>For &quot;looking at data&quot;, while I got a lot better at it from working on that skill in environments where people weren't really taking data seriously, the rate of improvement during the past few years, where I'm in an environment where I can toss ideas back and forth with people who are very good at understanding the limitations of what data can tell you as well as good at informing data analysis with deep domain knowledge, has been much higher. I'd say that I improved more at this in each individual year at my current job than I did in the decade prior to my current job.</p>\n\n<p>One thing to perhaps note is that the environment, how you spend your day-to-day, is inherently local. My current employer is probably the least data driven of the three large tech companies I've worked for, but my vicinity is a great place to get better at looking at data because I spend a relatively large fraction of my time working with people who are great with data, like Rebecca Isaacs, and a relatively small fraction of the time working with people who don't take data seriously.</p>\n\n<p>This post has discussed some strategies with an eye towards why they can be valuable, but I have to admit that my motivation for learning from experts wasnt to create value. It's more that I find learning to be fun and there are some areas where I'm motivated enough to apply the skills regardless of the environment, and learning from experts is such a great opportunity to have fun that it's hard to resist. Doing this for a couple of decades has turned out to be useful, but that's not something I knew would happen for quite a while (and I had no idea that this would effectively transfer to a new industry until I changed from hardware to software).</p>\n\n<p>A lot of career advice I see is oriented towards career or success or growth. That kind of advice often tells people to have a long-term goal or strategy in mind. It will often have some argument that's along the lines of &quot;a random walk will only move you sqrt(n) in some direction whereas a directed walk will move you n in some direction&quot;. I don't think that's wrong, but I think that, for many people, that advice implicitly underestimates the difficulty of finding an area that's suited to you<sup class=\"footnote-ref\" id=\"fnref:S\"><a rel=\"footnote\" href=\"#fn:S\">4</a></sup>, which I've basically <a href=\"https://twitter.com/jeanqasaur/status/1074528356324892672\">done by trial and error</a>.</p>\n\n<h3 id=\"appendix-parts-of-the-problem-this-post-doesn-t-discuss-in-detail\">Appendix: parts of the problem this post doesn't discuss in detail</h3>\n\n<p>One major topic not discussed is how to balance what &quot;level&quot; of skill to work on, which could be something high level, like &quot;looking at data&quot;, to something lower level, like &quot;Bayesian multilevel models&quot;, to something even lower level, like &quot;typing speed&quot;. That's a large enough topic that it deserves its own post that I'd expect to be longer than this one but, for now, <a href=\"https://danluu.com/productivity-velocity/#appendix-one-way-to-think-about-what-to-improve\">here's a comment from Gary Bernhardt about something related that I believe also applies to this topic</a>.</p>\n\n<p>Another major topic that's not discussed here is picking skills that are relatively likely to be applicable. It's a little too naive to just say that someone should think about learning skills they have an aptitude for without thinking about applicability.</p>\n\n<p>But while it's pretty easy to pick out skills where it's very difficult to either have an impact on the world or make a decent amount of money or achieve whatever goal you might want to achieve, like &quot;basketball&quot; or &quot;boxing&quot;, it's harder to pick between plausible skills, like computer architecture vs. PL.</p>\n\n<p>But I think semi-reasonable sounding skills are likely enough to be high return if they're a good fit for someone that trial and error among semi-reasonable sounding skills is fine, although it probably helps <a href=\"https://danluu.com/productivity-velocity/\">to be able to try things out quickly</a></p>\n\n<h3 id=\"appendix-related-posts\">Appendix: related posts</h3>\n\n<ul>\n<li>Ben Kuhn on, in some sense, <a href=\"https://www.benkuhn.net/conviction/\">what it's like to really learn something</a></li>\n<li>Holden Karnofsky on <a href=\"https://80000hours.org/podcast/episodes/holden-karnofsky-building-aptitudes-kicking-ass/\">having an aptitude-first approach to careers instead of a career-path-first approach</a>, which is sort of analogous to thinking about cross cutting skills like &quot;looking at data&quot; or &quot;having an adversarial mindset&quot; and not just thinking about skills like &quot;compilers&quot; or &quot;queuing theory&quot;</li>\n<li>Peter Drucker on <a href=\"https://www.csub.edu/~ecarter2/CSUB.MKTG%20490%20F10/DRUCKER%20HBR%20Managing%20Oneself.pdf\">how to understand one's strengths and weaknesses and do work that compatible with ones own inclinations</a></li>\n<li>Alexy Guzey on <a href=\"https://guzey.com/advice/\">the effectiveness of advice</a></li>\n<li>Edward Kmett with <a href=\"https://www.youtube.com/watch?v=Z8KcCU-p8QA\">another perspective on how to think about learning</a></li>\n<li>Patrick Collison <a href=\"https://patrickcollison.com/advice\">on how to maximize useful learning and find what you'll enjoy</a></li>\n</ul>\n\n<p><small>Thanks to Ben Kuhn, Alexey Guzey, Marek Majkowski, Nick Bergson-Shilcock, @bekindtopeople2, Aaron Levin, Milosz Danczak, Anja Boskovic, John Doty, Justin Blank, Mark Hansen, &quot;wl&quot;, and Jamie Brandon for comments/corrections/discussion.</small></p>\n<div class=\"footnotes\">\n\n<hr />\n\n<ol>\n<li id=\"fn:J\">This is an old analysis. If you were to do one today, you'd see a different mix of throws, but it's still the case that you see specialists having a lot of success, e.g., Riner with osoto gari\n <a class=\"footnote-return\" href=\"#fnref:J\"><sup>[return]</sup></a></li>\n<li id=\"fn:F\">To be fair to blanket, context free, advice, to learn a particular topic, functional programming really clicked for me and I could imagine that, if that style of thinking wasn't already natural for me (as a result of coming from a hardware background), the advice that one should learn functional programming because it will change how you think about problems might've been useful for me, but on the other hand, that means that the advice could've just as easily been to learn hardware engineering.\n <a class=\"footnote-return\" href=\"#fnref:F\"><sup>[return]</sup></a></li>\n\n<li id=\"fn:M\"><p>I don't have a large enough sample nor have I polled enough people to have high confidence that this works as a general algorithm but, for finding groups of world-class experts, what's worked for me is finding excellent managers. The two teams I worked on with the highest density of world-class experts have been teams under really great management. I have a higher bar for excellent management than most people and, from having talked to many people about this, almost no one I've talked to has worked for or even knows a manager as good as one I would consider to be excellent (and, general, both the person I'm talking to agrees with me on this, indicating that it's not the case that they have a manager who's excellent in dimensions I don't care about and vice versa); from discussions about this, I would guess that a manager I think of as excellent is at least 99.9%-ile. How to find such a manager is a long discussion that I might turn into another post.</p>\n\n<p>Anyway, despite having a pretty small sample on this, I think the mechanism for this is plausible, in that the excellent managers I know have very high retention as well as a huge queue of people who want to work for them, making it relatively easy for them to hire and retain people with world-class expertise since <a href=\"https://danluu.com/hiring-lemons/\">the rest of the landscape is so bleak</a>.</p>\n\n<p>A more typical strategy, one that I don't think generally works and also didn't work great for me when I tried it is to work on the most interesting sounding and/or hardest problems around. While I did work with some really great people while trying to <a href=\"https://www.benkuhn.net/hard/\">work on interesting / hard problems</a>, including one of the best engineers I've ever worked with, I don't think that worked nearly as well as looking for good management w.r.t. working with people I really want to learn from. I believe the general problem with this algorithm is the same problem with going to work in video games because video games are cool and/or interesting. The fact that so many people want to work on exciting sounding problems leads to dysfunctional environments that can persist indefinitely.</p>\n\n<p>In one case, I was on a team that had 100% turnover in nine months and it would've been six if it hadn't taken so long for one person to find a team to transfer to. In the median case, my cohort (people who joined around when I joined, ish) had about 50% YoY turnover and I think that people had pretty good reasons for leaving. Not only is this kind of turnover a sign that the environment is often a pretty unhappy one, these kinds of environments often differentially cause people who I'd want to work with and/or learn from to leave. For example, on the team I was on where the TL didn't believe in using version control, automated testing, or pipelined designs, I worked with Ikhwan Lee, who was great. Of course, Ikhwan left pretty quickly while the TL stayed and is still there six years later.</p>\n <a class=\"footnote-return\" href=\"#fnref:M\"><sup>[return]</sup></a></li>\n<li id=\"fn:S\">Something I've seen many times among my acquaintances is that people will pick a direction before they have any idea whether or not it's suitable for them. Often, after quite some time (more than a decade in some cases), they'll realize that they're actually deeply unhappy with the direction they've gone, sometimes because it doesn't match their temperament, and sometimes because it's something they're actually bad at. In any case, wandering around randomly and finding yourself sqrt(n) down a path you're happy with doesn't seem so bad compared to having made it n down a path you're unhappy with.\n <a class=\"footnote-return\" href=\"#fnref:S\"><sup>[return]</sup></a></li>\n</ol>\n</div>\n"]}]} {:tag :item, :attrs nil, :content [{:tag :title, :attrs nil, :content ["Some reasons to work on productivity and velocity"]} {:tag :link, :attrs nil, :content ["https://danluu.com/productivity-velocity/"]} {:tag :pubDate, :attrs nil, :content ["Fri, 15 Oct 2021 00:00:00 +0000"]} {:tag :guid, :attrs nil, :content ["https://danluu.com/productivity-velocity/"]} {:tag :description, :attrs nil, :content ["\n\n<p>A common topic of discussion among my close friends is where the bottlenecks are in our productivity and how we can execute more quickly. This is very different from what I see in my extended social circles, where people commonly say that <a href=\"https://twitter.com/danluu/status/1440106603093495810\">velocity doesn't matter</a>. In online discussions about this, I frequently see people go a step further and assign moral valence to this, saying that it is actually bad to try to increase velocity or be more productive or work hard (see appendix for more examples).</p>\n\n<p>The top reasons I see people say that productivity doesn't matter (or is actually bad) fall into one of three buckets:</p>\n\n<ul>\n<li>Working on the right thing is more important than working quickly</li>\n<li>Speed at X doesn't matter because you don't spend much time doing X</li>\n<li>Thinking about productivity is bad and you should &quot;live life&quot;</li>\n</ul>\n\n<p>I certainly agree that working on the right thing is important, but increasing velocity doesn't stop you from working on the right thing. If anything, each of these is a force multiplier for the other. Having strong execution skills becomes more impactful if you're good at picking the right problem and vice versa.</p>\n\n<p>It's true that the gains from picking the right problem can be greater than the gains from having better tactical execution because the gains from picking the right problem can be unbounded, but it's also much easier to improve tactical execution and doing so also helps with picking the right problem because having faster execution lets you experiment more quickly, which helps you find the right problem.</p>\n\n<p>A concrete example of this is a project I worked on to quantify the machine health of the fleet. The project discovered a number of serious issues (a decent fraction of hosts were actively corrupting data or had a performance problem that would increase tail latency by &gt; 2 orders of magnitude, or both). This was considered serious enough that a new team was created to deal with the problem.</p>\n\n<p>In retrospect, my first attempts at quantifying the problem were doomed and couldn't have really worked (or not in a reasonable amount of time, anyway). I spent a few weeks cranking through ideas that couldn't work and a critical part of getting to the idea that did work after &quot;only&quot; a few weeks was being able to quickly try out and discard ideas that didn't work. In part of a previous post, I described how long a tiny part of that process took and multiple people objected to that being impossibly fast in internet comments.</p>\n\n<p>I find this a bit funny since I'm not a naturally quick programmer. <a href=\"https://danluu.com/learning-to-program/\">Learning to program was a real struggle for me</a> and I was pretty slow at it for a long time (and I still am in aspects that I haven't practiced). My &quot;one weird trick&quot; is that I've explicitly worked on speeding up things that I do frequently and most people have not. I view the situation as somewhat analogous to sports before people really trained. For a long time, many athletes didn't seriously train, and then once people started trying to train, the training was often misguided by modern standards. For example, if you read commentary on baseball from the 70s, you'll see people saying that baseball players shouldn't weight train because it will make them &quot;muscle bound&quot; (many people thought that weight lifting would lead to &quot;too much&quot; bulk, causing people to be slower, have less explosive power, and be less agile). But today, players get a huge advantage from using performance-enhancing drugs that increase their muscle-bound-ness, which implies that players could not get too &quot;muscle bound&quot; from weight training alone. An analogous comment to one discussed above would be saying that athletes shouldn't worry about power/strength and should increase their skill, but power increases returns to skill and vice versa.</p>\n\n<p>Coming back to programming, if you explicitly practice and train and almost no one else does, you'll be able to do things relatively quickly compared to most people even if, like me, you don't have much talent for programming and getting started at all was a real struggle. Of course, there's always going to be someone more talented out there who's executing faster after having spent less time improving. But, luckily for me, <a href=\"https://danluu.com/p95-skill/\">relatively few people seriously attempt to improve</a>, so I'm able to do ok.</p>\n\n<p>Anyway, despite operating at a rate that some internet commenters thought was impossible, it took me weeks of dead ends to find something that worked. If I was doing things at a speed that people thought was normal, I suspect it would've taken long enough to find a feasible solution that I would've dropped the problem after spending maybe one or two quarters on it. The number of plausible-ish seeming dead ends was probably not unrelated to why the problem was still an open problem despite being a critical issue for years. Of course, someone who's better at having ideas than me could've solved the problem without the dead ends, but as we discussed earlier, it's fairly easy to find low hanging fruit on &quot;execution speed&quot; and not so easy to find low hanging fruit on &quot;having better ideas&quot;. However, it's possible to, to a limited extent, simulate someone who has better ideas than me by being able to quickly try out and discard ideas (I also work on having better ideas, but I think it makes sense to go after the easier high ROI wins that are available as well). Being able to try out ideas quickly also improves the rate at which I can improve at having better ideas since a key part of that is building intuition by getting feedback on what works.</p>\n\n<p>The next major objection is that speed at a particular task doesn't matter because time spent on that task is limited. At a high level, I don't agree with this objection because, while this may hold true for any particular kind of task, the solution to that is to try to improve each kind of task and not to reject the idea of improvement outright. A sub-objection people have is something like &quot;but I spend 20 hours in unproductive meetings every week, so it doesn't matter what I do with my other time&quot;. I think this is doubly wrong, in that if you then only have 20 hours of potentially productive time, whatever productivity multiplier you have on that time still holds for your general productivity. Also, it's generally possible to drop out of meetings that are a lost cause and increase the productivity of meetings that aren't a lost cause<sup class=\"footnote-ref\" id=\"fnref:M\"><a rel=\"footnote\" href=\"#fn:M\">1</a></sup>.</p>\n\n<p>More generally, when people say that optimizing X doesn't help because they don't spend time on X and are not bottlenecked on X, that doesn't match my experience as I find I spend plenty of time bottlenecked on X for commonly dismissed Xs. I think that part of this is because getting faster at X can actually increase time spent on X due to a sort of virtuous cycle feedback loop of where it makes sense to spend time. Another part of this is illustrated in this comment by Fabian Giesen:</p>\n\n<blockquote>\n<p>It is commonly accepted, verging on a cliche, that you have no idea where your program spends time until you actually profile it, but the corollary that you also don't know where <em>you</em> spend your time until you've measured it is not nearly as accepted.</p>\n</blockquote>\n\n<p>When I've looked how people spend time vs. how people think they spend time, it's wildly inaccurate and I think there's a fundamental reason that, unless they measure, people's estimates of how they spend their time tends to be way off, which is nicely summed in by another Fabian Giesen quote, which happens to be about solving rubik's cubes but applies to other cognitive tasks:</p>\n\n<blockquote>\n<p>Paraphrasing a well-known cuber, &quot;your own pauses never seem bad while you're solving, because your brain is busy and you know what you're thinking about, but once you have a video it tends to become blindingly obvious what you need to improve&quot;. Which is pretty much the usual &quot;don't assume, profile&quot; advice for programs, but applied to a situation where you're concentrated and busy for the entire time, whereas the default assumption in programming circles seems to be that as long as you're actually doing work and not distracted or slacking off, you can't possibly be losing a lot of time</p>\n</blockquote>\n\n<p>Unlike most people who discuss this topic online, I've actually looked at where my time goes and a lot of it goes to things that are canonical examples of things that you shouldn't waste time improving because people don't spend much time doing them.</p>\n\n<p>An example of one of these, the most commonly cited bad-thing-to-optmize example that I've seen, is typing speed (when discussing this, people usually say that typing speed doesn't matter because more time is spent thinking than typing). But, when I look at where my time goes, a lot of it is spent typing.</p>\n\n<p>A specific example is that I've written a number of influential docs at my current job and when people ask how long some doc took to write, they're generally surprised that the doc only took a day to write. As with the machine health example, a thing that velocity helps with is figuring out which docs will be influential. If I look at the docs I've written, I'd say that maybe 15% were really high impact (caused a new team to be created, changed the direction of existing teams, resulted in significant changes to the company's bottom line, etc.). Part of it is that I don't always know which ideas will resonate with other people, but part of it is also that I often propose ideas that are long shots because the ideas sound too stupid to be taken seriously (e.g., one of my proposed solutions to a capacity crunch was to, for each rack, turn off 10% of it, thereby increasing effective provisioned capacity, which is about as stupid sounding an idea as one could come up with). If I was much slower at writing docs, it wouldn't make sense to propose real long shot ideas. As things are today, if I think an idea has a 5% chance of success, in expectation, I need to spend ~20 days writing docs to have one of those land.</p>\n\n<p>I spend roughly half my writing time typing. If I typed at what some people say median typing speed is (40 WPM) instead of the rate some random typing test clocked me at (110 WPM), this would be a 0.5 + 0.5 * 110/40 = 1.875x slowdown, putting me at nearly 40 days of writing before a longshot doc lands, which would make that a sketchier proposition. If I hadn't optimized the non-typing part of my writing workflow as well, I think I would be, on net, maybe 10x slower<sup class=\"footnote-ref\" id=\"fnref:T\"><a rel=\"footnote\" href=\"#fn:T\">2</a></sup>, which would put me at more like ~200 days per high impact longshot doc, which is enough that I think that I probably wouldn't write longshot docs<sup class=\"footnote-ref\" id=\"fnref:S\"><a rel=\"footnote\" href=\"#fn:S\">3</a></sup>.</p>\n\n<p>More generally, Fabian Giesen has noted that this kind of non-linear impact of velocity is common:</p>\n\n<blockquote>\n<p>There are &quot;phase changes&quot; as you cross certain thresholds (details depend on the problem to some extent) where your entire way of working changes.\n...\nThere's a lot of things I could in theory do at any speed but in practice cannot, because as iteration time increases it first becomes so frustrating that I can't do it for long and eventually it takes so long that it literally drops out of my short-term memory, so I need to keep notes or otherwise organize it or I can't do it at all.</p>\n\n<p>Certainly if I can do an experiment in an interactive UI by dragging on a slider and see the result in a fraction of a second, at that point it's very &quot;no filter&quot;, if you want to try something you just do it.</p>\n\n<p>Once you're at iteration times in the low seconds (say a compile-link cycle with a statically compiled lang) you don't just try stuff anymore, you also spend time thinking about whether it's gonna tell you anything because it takes long enough that you'd rather not waste a run.</p>\n\n<p>Once you get into several-minute or multi-hour iteration times there's a lot of planning to not waste runs, and context switching because you do other stuff while you wait, and note-taking/bookkeeping; also at this level mistakes are both more expensive (because a wasted run wastes more time) and more common (because your attention is so divided).</p>\n\n<p>As you scale that up even more you might now take significant resources for a noticeable amount of time and need to get that approved and budgeted, which takes its own meetings etc.</p>\n</blockquote>\n\n<p>A specific example of something moving from one class of item to another in my work was <a href=\"https://danluu.com/metrics-analytics/\">this project on metrics analytics</a>. There were a number of proposals on how to solve this problem. There was broad agreement that the problem was important with no dissenters, but the proposals were all the kinds of things you'd allocate a team to work on through multiple roadmap cycles. Getting a project that expensive off the ground requires a large amount of organizational buy-in, enough that many important problems don't get solved, including this one. But it turned out, if scoped properly and executed reasonably, the project was actually something a programmer could create an MVP of in a day, which takes no organizational buy-in to get off the ground. Instead of needing to get multiple directors and a VP to agree that the problem is among the org's most important problems, you just need a person who thinks the problem is worth solving.</p>\n\n<p>Going back to Xs where people say velocity doesn't matter because they don't spend a lot time on X, another one I see frequently is coding, and it is also not my personal experience that coding speed doesn't matter. For the machine health example discussed above, after I figured out something that would work, I spent one month working on basically nothing but that, coding, testing, and debugging. I think I had about 6 hours of meetings during that month, but other than that plus time spent eating, etc., I would go in to work, code all day, and then go home. I think it's much more difficult to compare coding speed across people because it's rare to see people do the same or very similar non-trivial tasks, so I won't try to compare to anyone else, but if I look at my productivity before I worked on improving it as compared to where I'm at now, the project probably would have been infeasible without the speedups I've found by looking at my velocity.</p>\n\n<p><a href=\"https://en.wikipedia.org/wiki/Amdahl%27s_law\">Amdahl's law</a> based arguments can make sense when looking for speedups in a fixed benchmark, like a sub-task of SPECint, but when you have a system where getting better at a task increases returns to doing that task and can increase time spent on the task, it doesn't make sense to say that you shouldn't work on something because you spend a lot of time doing it. I spend time on things that are high ROI, but those things are generally only high ROI because I've spent time improving my velocity, which reduces the &quot;I&quot; in ROI.</p>\n\n<p>The last major argument I see against working on velocity assigns negative moral weight to the idea of thinking about productivity and working on velocity at all. This kind of comment often assigns positive moral weight to various kinds of leisure, such as spending time with friends and family. I find this argument to be backwards. If someone thinks it's important to spend time with friends and family, an easy way to do that is to be more productive at work and spend less time working.</p>\n\n<p>Personally, I deliberately avoid working long hours and I suspect I don't work more than the median person at my company, which is a company where I think work-life balance is pretty good overall. A lot of my productivity gains have gone to leisure and not work. Furthermore, deliberately working on velocity has <a href=\"https://twitter.com/danluu/status/1444034823329177602\">allowed me to get promoted relatively quickly</a><sup class=\"footnote-ref\" id=\"fnref:P\"><a rel=\"footnote\" href=\"#fn:P\">4</a></sup>, which means that I make more money than I would've made if I didn't get promoted, which gives me more freedom to spend time on things that I value.</p>\n\n<p>For people that aren't arguing that you shouldn't think about productivity because it's better to focus on leisure and instead argue that you simply shouldn't think about productivity at all because it's unnatural and one should live a natural life, that ultimately comes down to personal preference, but for me, I value the things I do outside of work too much to not explicitly work on productivity at work.</p>\n\n<p>As with <a href=\"https://danluu.com/why-benchmark/\">this post on reasons to measure</a>, while this post is about practical reasons to improve productivity, the main reason I'm personally motivated to work on my own productivity isn't practical. The main reason is that I enjoy the process of getting better at things, whether that's some nerdy board game, a sport I have zero talent at that will never have any practical value to me, or work. For me, a secondary reason is that, given that  my lifespan is finite, I want to allocate my time to things that I value, and increasing productivity allows me to do more of that, but that's not a thought i had until I was about 20, at which point I'd already been trying to improve at most things I spent significant time on for many years.</p>\n\n<p>Another common reason for working on productivity is that mastery and/or generally being good at something seems satisfying for a lot of people. That's not one that resonates with me personally, but when I've asked other people about why they work on improving their skills, that seems to be a common motivation.</p>\n\n<p>A related idea, one that Holden Karnofsky has been talking about for a while, is that if you ever want to make a difference in the world in some way, it's useful to work on your skills even in jobs where it's not obvious that being better at the job is useful, because the developed skills will give you more leverage on the world when you switch to something that's more aligned with you want to achieve.</p>\n\n<h3 id=\"appendix-one-way-to-think-about-what-to-improve\">Appendix: one way to think about what to improve</h3>\n\n<p>Here's a framing I like from Gary Bernhardt (not set off in a quote block since this entire section, other than this sentence, is his).</p>\n\n<p>People tend to fixate on a single granularity of analysis when talking about efficiency. E.g., &quot;thinking is the most important part so don't worry about typing speed&quot;. If we step back, the response to that is &quot;efficiency exists at every point on the continuum from year-by-year strategy all the way down to millisecond-by-millisecond keystrokes&quot;. I think it's safe to assume that gains at the larger scale will have the biggest impact. But as we go to finer granularity, it's not obvious where the ROI drops off. Some examples, moving from coarse to fine:</p>\n\n<ol>\n<li>The macro point that you started with is: programming isn't just thinking; it's thinking plus tactical activities like editing code. Editing faster means more time for thinking.</li>\n<li>But editing code costs more than just the time spent typing! Programming is highly dependent on short-term memory. Every pause to edit is a distraction where you can forget the details that you're juggling. Slower editing effectively weakens your short-term memory, which reduces effectiveness.</li>\n<li>But editing code isn't just hitting keys! It's hitting keys plus the editor commands that those keys invoke. A more efficient editor can dramatically increase effective code editing speed, even if you type at the same WPM as before.</li>\n<li>But each editor command doesn't exist in a vacuum! There are often many ways to make the same edit. A Vim beginner might type &quot;hhhhxxxxxxxx&quot; when &quot;bdw&quot; is more efficient. An advanced Vim user might use &quot;bdw&quot;, not realizing that it's slower than &quot;diw&quot; despite having the same number of keystrokes. (In QWERTY keyboard layout, the former is all on the left hand, whereas the latter alternates left-right-left hands. At 140 WPM, you're typing around 14 keystrokes per second, so each finger only has 70 ms to get into position and press the key. Alternating hands leaves more time for the next finger to get into position while the previous finger is mid-keypress.)</li>\n</ol>\n\n<p>We have to choose how deep to go when thinking about this. I think that there's clear ROI in thinking about 1-3, and in letting those inform both tool choice and practice. I don't think that (4) is worth a lot of thought. It seems like we naturally find &quot;good enough&quot; points there. But that also makes it a nice fence post to frame the others.</p>\n\n<h3 id=\"appendix-more-examples\">Appendix: more examples</h3>\n\n<ul>\n<li><a href=\"https://news.ycombinator.com/item?id=10529064\">In the comments on a post where Ben Kuhn notes that he got 50% more productive by allocating his time better, people are nearly uniformly negative about the post and say that he works too much</a>. Although Ben clarified in multiple comments as well as in the post that not all time tracked was worked, the commenters are too busy taking the moral high ground to actually respond to the contents of the post</li>\n<li><a href=\"https://news.ycombinator.com/item?id=28879240\">Comments on Jamie Brandon's &quot;Speed Matters&quot;</a>\n\n<ul>\n<li><a href=\"https://news.ycombinator.com/item?id=28880190\">Working quickly is pointless because you will be forced to do more work</a></li>\n<li><a href=\"https://news.ycombinator.com/item?id=28881360\">Speed doesn't matter if you're doing the right thing, and also, if such a thing as speed did exist, it would be unmeasurable and therefore pointless to discuss</a></li>\n<li><a href=\"https://news.ycombinator.com/item?id=28879823\">Thinking about productivity is unhealthy. One should relax instead</a></li>\n<li><a href=\"https://news.ycombinator.com/item?id=28881320\">You can only choose 2 of &quot;good, fast, cheap&quot;, therefore it is counterproductive to work on speed</a></li>\n<li><a href=\"https://news.ycombinator.com/item?id=28880653\">A large speedup is impossible</a></li>\n<li><a href=\"https://news.ycombinator.com/item?id=28880173\">&quot;The author mistakes coding for typing&quot;</a></li>\n<li>etc.</li>\n<li>As with Ben's post, virtually all of these comments are addressed in the post itself. I'm going to stop noting when this is true because it is generally true of the posts referred to here.</li>\n</ul></li>\n<li><a href=\"https://news.ycombinator.com/item?id=22255996\">The #3 comment on a post by Michael Malis on &quot;How to Improve Your Productivity as a Working Programmer\n&quot;</a>: &quot;Fuck it, the entire work environment seems designed to decrease productivity . . . Why should I bother . . .&quot;\n\n<ul>\n<li>#4 comment: &quot;What if I don't want to improve my productivity ? Just take time.&quot;\n\n<ul>\n<li>After the initial indignation, this comment goes on and proves that the commenter missed the point entirely, as the rest of the comment explains how the commenter works productively, which the commenter apparently is ok with as long as it's not phrased as a way to work productively, because one is supposed to be morally outraged by someone wanting to be productive and sharing techniques about how to be productive with other people who might be interested in being productive</li>\n<li>In the responses, someone points out that someone who's more productive would be able to spend more time on leisure; that comment is uniformly panned because &quot;work expands so as to fill the time available for its completion&quot;, as if how one spends time is some sort of immutable law of nature and not something under anyone's control</li>\n</ul></li>\n<li>Another comment: &quot;Alright. What are we optimizing for? Productivity? Or the end-goals of any of: achieving more, climbing the corporate ladder, making more money, etc..?&quot;</li>\n</ul></li>\n<li><a href=\"https://news.ycombinator.com/item?id=13752887\">Comments on a post by antirez about productivity</a>\n\n<ul>\n<li><a href=\"https://news.ycombinator.com/item?id=13753443\">The article is talking about the 10x programmer universe, not the normal universe most people live in</a></li>\n<li><a href=\"https://news.ycombinator.com/item?id=13753611\">It's pointless to work on productivity since your environment determines productivity</a></li>\n<li><a href=\"https://news.ycombinator.com/item?id=13753465\">Productive programmers are selfish, don't mentor, etc., and are bad for their teams</a>\n\n<ul>\n<li>If you read the entire comments to the post, you'll see that this is a common theme</li>\n</ul></li>\n</ul></li>\n<li><a href=\"https://news.ycombinator.com/item?id=20737304\">Comments on Alexy Guezy's thoughts on productivity</a>\n\n<ul>\n<li><a href=\"https://news.ycombinator.com/item?id=20737854\">&quot;Serious question: Is anything less productive than reading other people's productivity thoughts? It's a combination of procrastination and finding out what works for someone who is presumably more productive than you (ie: guilt).&quot;</a></li>\n<li><a href=\"https://news.ycombinator.com/item?id=20737854\">An anti-productivity article titled &quot;Against Productivity</a></li>\n</ul></li>\n<li><a href=\"https://twitter.com/b0rk/status/1367172498954059791\">Velocity doesn't matter</a>, from the person who I believe has been the most widely read programming blogger since about 2015</li>\n</ul>\n\n<p>etc.</p>\n\n<p>Some positive examples of people who have used their productivity to &quot;fund&quot; things that they value include Andy Kelley (Zig), Jamie Brandon (various), Andy Matuschak (mnemonic medium, various), Saul Pwanson (VisiData), Andy Chu (Oil Shell). I'm drawing from programming examples, but you can find plenty of others, e.g., Nick Adnitt (<a href=\"https://darksidecanoes.wordpress.com/\">Darkside Canoes</a>) and, of course, numerous people who've retired to pursue interests that aren't work-like at all.</p>\n\n<h3 id=\"appendix-another-reason-to-avoid-being-productive\">Appendix: another reason to avoid being productive</h3>\n\n<p>An idea that's become increasingly popular in my extended social circles at major tech companies is that one should avoid doing work and <a href=\"https://www.reddit.com/r/antiwork/comments/pvjc6f/they_dont_give_a_fuck_about_you/\">waste as much time as possible</a>, often called &quot;antiwork&quot;, which seems like a natural extension of &quot;tryhard&quot; becoming an insult. The reason given is often something like, work mainly enriches upper management at your employer and/or shareholders, who are generally richer than you.</p>\n\n<p>I'm sympathetic to the argument and <a href=\"https://twitter.com/danluu/status/802971209176477696\">agree that upper management and shareholders capture most of the value from work</a>. But as much as I sympathize with the idea of deliberately being unproductive to &quot;stick it to the man&quot;, I value spending my time on things that I want enough that I'd rather get my work done quickly so I can do things I enjoy more than work. Additionally, having been productive in the past has given me good options for jobs, so I have work that I enjoy a lot more than my acquaintances in tech who have embraced the &quot;antiwork&quot; movement.</p>\n\n<p>The less control you have over your environment, the more it makes sense to embrace &quot;antiwork&quot;. Programmers at major tech companies have, relatively speaking, a lot of control over their environment, which is why I'm not &quot;antiwork&quot; even though I'm sympathetic to the cause.</p>\n\n<p>Although it's about a different topic, a related comment <a href=\"https://twitter.com/PracheeAC/status/1448789430488092672\">from Prachee Avasthi about avoiding controversial work and avoiding pushing for necessary changes when pre-tenure ingrains habits that are hard break post-tenure</a>. If one wants to be &quot;antiwork&quot; forever, that's not a problem, but if one wants to move the needle on something at some point, building &quot;antiwork&quot; habits while working for a major tech company will instill counterproductive habits.</p>\n\n<p><small> Thanks to Fabian Giesen, Gary Bernhardt, Ben Kuhn, David Turner, Marek Majkowski, Anja Boskovic, Aaron Levin, Lifan Zeng, Justin Blank, Heath Borders, Tao L., Nehal Patel, and Jamie Brandon for comments/corrections/discussion</small></p>\n<div class=\"footnotes\">\n\n<hr />\n\n<ol>\n<li id=\"fn:M\">When I look at the productiveness of meetings, there are some people who are very good at keeping meetings on track and useful. For example, one person who I've been in meetings with who is extraordinarily good at ensuring meetings are productive is Bonnie Eisenman. Early on in my current job, I asked her how she was so effective at keeping meetings productive and have been using that advice since then (I'm not nearly as good at it as she is, but even so, improving at this was a significant win for me).\n <a class=\"footnote-return\" href=\"#fnref:M\"><sup>[return]</sup></a></li>\n\n<li id=\"fn:T\"><p>10x might sound like an implausibly large speedup on writing, but in a discussion on writing speed on a private slack, a well-known newsletter author mentioned that their net writing speed for a 5k word newsletter was a little under 2 words per minute (WPM). My net rate (including time spent editing, etc.) is over 20 WPM per doc.</p>\n\n<p>With a measured typing speed of 110 WPM, that might sound like I spend a small fraction of my time typing, but it turns out it's roughly half the time. If I look at my writing speed, it's much slower than my typing test speed and it seems that it's perhaps half the rate. If I look at where the actual time goes, roughly half of it goes to typing and have goes to thinking, semi-serially, which creates long pauses in my typing.</p>\n\n<p>If I look at where the biggest win here could come, it would be from thinking and typing in parallel, which is something I'd try to achieve by practicing typing more, not less. But even without being able to do that, and with above average typing speed, I still spend half of my time typing!</p>\n\n<p>The reason my net speed is well under the speed that I write is that I do multiple passes and re-write. Some time is spent reading as I re-write, but I read much more quickly than I write, so that's a pretty small fraction of time. In principle, I could adopt an approach that involves less re-writing, but I've tried a number of things that one might expect would lead to that goal and haven't found one that works for me (yet?).</p>\n\n<p>Although the example here is about work, this also holds for my personal blog, where my velocity is similar. If I wrote ten times slower than I do, I don't think I'd have much of a blog. My guess is that I would've written a few posts or maybe even a few drafts and not gotten to the point where I'd post and then stop.</p>\n\n<p>I enjoy writing and get a lot of value out of it in a variety of ways, but I value the other things in my life enough that I don't think writing would have a place in my life if my net writing speed were 2 WPM.</p>\n <a class=\"footnote-return\" href=\"#fnref:T\"><sup>[return]</sup></a></li>\n\n<li id=\"fn:S\"><p>Another strategy would be to write shorter docs. There's a style of doc where that works well, but I frequently write docs where I leverage my writing speed to discuss a problem that would be difficult to convincingly discuss without a long document.</p>\n\n<p>One example of a reason that my docs is that I frequently work on problems that span multiple levels of the stack, which means that I end up presenting data from multiple levels of the stack as well as providing enough context about why the problem at some level drives a problem up or down the stack for people who aren't deeply familiar with that level of the stack, which is necessary since few readers will have strong familiarity with every level needed to understand the problem.</p>\n\n<p>In most cases, there have been previous attempts to motivate/fund work on the problem that didn't get traction because there wasn't a case linking an issue at one level of the stack to important issues at other levels of the stack. I could avoid problems that span many levels of the stack, but there's a lot of low hanging fruit among those sorts of problems for technical and organizational reasons, so I don't think it makes sense to ignore them just because it takes a day to write a document explaining the problem (although it might make sense if it took ten days, at least in cases where people might be skeptical of the solution).</p>\n <a class=\"footnote-return\" href=\"#fnref:S\"><sup>[return]</sup></a></li>\n<li id=\"fn:P\">Of course, promotions are highly unfair and being more productive doesn't guarantee promotion. If I just look at what things are correlated with level, <a href=\"https://twitter.com/altluu/status/1448012821854257155\">it's not even clear to me that productivity is more strongly correlated with level than height</a>, but among factors that are under my control, productivity is one of the easiest to change.\n <a class=\"footnote-return\" href=\"#fnref:P\"><sup>[return]</sup></a></li>\n</ol>\n</div>\n"]}]} {:tag :item, :attrs nil, :content [{:tag :title, :attrs nil, :content ["The value of in-house expertise"]} {:tag :link, :attrs nil, :content ["https://danluu.com/in-house/"]} {:tag :pubDate, :attrs nil, :content ["Wed, 29 Sep 2021 00:00:00 +0000"]} {:tag :guid, :attrs nil, :content ["https://danluu.com/in-house/"]} {:tag :description, :attrs nil, :content ["<p>An alternate title for this post might be, &quot;Twitter has a kernel team!?&quot;. At this point, I've heard that surprised exclamation enough that I've lost count of the number times that's been said to me (I'd guess that it's more than ten but less than a hundred). If we look at trendy companies that are within a couple factors of two in size of Twitter (in terms of either market cap or number of engineers), they mostly don't have similar expertise, often as a result of path dependence  because they &quot;grew up&quot; in the cloud, they didn't need kernel expertise to keep the lights on the way an on prem company does. While that makes it socially understandable that people who've spent their career at younger, trendier, companies, are surprised by Twitter having a kernel team, I don't think there's a technical reason for the surprise.</p>\n\n<p>Whether or not it has kernel expertise, a company Twitter's size is going to regularly run into kernel issues, from major production incidents to papercuts. Without a kernel team or the equivalent expertise, the company will muddle through the issues, running into unnecessary problems as well as taking an unnecessarily long time to mitigate incidents. As an example of a critical production incident, just because it's already been written up publicly, I'll cite <a href=\"https://blog.twitter.com/engineering/en_us/topics/open-source/2020/hunting-a-linux-kernel-bug\">this post</a>, which dryly notes:</p>\n\n<blockquote>\n<p>Earlier last year, we identified a firewall misconfiguration which accidentally dropped most network traffic. We expected resetting the firewall configuration to fix the issue, but resetting the firewall configuration exposed a kernel bug</p>\n</blockquote>\n\n<p>What this implies but doesn't explicitly say is that this firewall misconfiguration was the most severe incident that's occured during my time at Twitter and I believe it's actually the most severe outage that Twitter has had since 2013 or so. As a company, we would've still been able to mitigate the issue without a kernel team or another team with deep Linux expertise, but it would've taken longer to understand why the initial fix didn't work, which is the last thing you want when you're debugging a serious outage. Folks on the kernel team were already familiar with the various diagnostic tools and debugging techniques necessary to quickly understand why the initial fix didn't work, which is not common knowledge at some peer companies (I polled folks at a number of similar-scale peer companies to see if they thought they had at least one person with the knowledge necessary to quickly debug the bug and the answer was no at many companies).</p>\n\n<p>Another reason to have in-house expertise in various areas is that they easily pay for themselves, which is a special case of <a href=\"https://danluu.com/sounds-easy/\">the generic argument that large companies should be larger than most people expect because tiny percentage gains are worth a large amount in absolute dollars</a>. If, in the lifetime of the specialist team like the kernel team, a single person found something that persistently reduced <a href=\"https://en.wikipedia.org/wiki/Total_cost_of_ownership\">TCO</a> by 0.5%, that would pay for the team in perpetuity, and Twitters kernel team has found many such changes. In addition to <a href=\"https://patchwork.ozlabs.org/project/netdev/list/?submitter=211&amp;state=*&amp;archive=both&amp;param=4&amp;page=1\">kernel patches</a> that sometimes have that kind of impact, people will also find configuration issues, etc., that have that kind of impact.</p>\n\n<p>So far, I've only talked about the kernel team because that's the one that most frequently elicits surprise from folks for merely existing, but I get similar reactions when people find out that Twitter has a bunch of ex-Sun JVM folks who worked on HotSpot, like Ramki Ramakrishna, Tony Printezis, and John Coomes. People wonder why a social media company would need such deep JVM expertise. As with the kernel team, companies our size that use the JVM run into weird issues and JVM bugs and it's helpful to have people with deep expertise to debug those kinds of issues. And, as with the kernel team, individual optimizations to the JVM can pay for the team in perpetuity. A concrete example is <a href=\"https://github.com/oracle/graal/pull/636\">this patch by Flavio Brasil, which virtualizes compare and swap calls</a>.</p>\n\n<p>The context for this is that Twitter uses a lot of Scala. Despite a lot of claims otherwise, Scala uses more memory and is significantly slower than Java, which has a significant cost if you use Scala at scale, enough that it makes sense to do optimization work to reduce the performance gap between idiomatic Scala and idiomatic Java.</p>\n\n<p>Before the patch, if you profiled our Scala code, you would've seen an unreasonably large amount of time spent in Future/Promise, including in cases where you might naively expect that the compiler would optimize the work away. One reason for this is that Futures use a <a href=\"https://en.wikipedia.org/wiki/Compare-and-swap\">compare-and-swap</a> (CAS) operation that's opaque to JVM optimization. The patch linked above avoids CAS operations when the Future doesn't escape the scope of the method. <a href=\"https://github.com/twitter/util/commit/3245a8e1a98bd5eb308f366678528879d7140f5e\">This companion patch</a> removes CAS operations in some places that are less amenable to compiler optimization. The two patches combined reduced the cost of typical major Twitter services using idiomatic Scala by 5% to 15%, paying for the JVM team in perpetuity many times over and that wasn't even the biggest win Flavio found that year.</p>\n\n<p>I'm not going to do a team-by-team breakdown of teams that pay for themselves many times over because there are so many of them, even if I limit the scope to &quot;teams that people are surprised that Twitter has&quot;.</p>\n\n<p>A related topic is how people talk about &quot;buy vs. build&quot; discussions. I've seen a number of discussions where someone has argued for &quot;buy&quot; because that would obviate the need for expertise in the area. This can be true, but I've seen this argued for much more often than it is true. An example where I think this tends to be untrue is with distributed tracing. <a href=\"https://danluu.com/tracing-analytics/\">We've previously looked at some ways Twitter gets value out of tracing</a>, which came out of the vision Rebecca Isaacs put into place. On the flip side, when I talk to people at peer companies with similar scale, most of them have not (yet?) succeeded at getting significant value from distributed tracing. This is so common that I see a viral Twitter thread about how useless distributed tracing is more than once a year. Even though we went with the more expensive &quot;build&quot; option, just off the top of my head, I can think of multiple uses of tracing that have returned between 10x and 100x the cost of building out tracing, whereas people at a number of companies that have chosen the cheaper &quot;buy&quot; option commonly complain that tracing isn't worth it.</p>\n\n<p>Coincidentally, I was just talking about this exact topic to Pam Wolf, a civil engineering professor with experience in (civil engineering) industry on multiple continents, who had a related opinion. For large scale systems (projects), you need an in-house expert (owner's-side engineer) for each area that you don't handle in your own firm. While it's technically possible to hire yet another firm to be the expert, that's more expensive than developing or hiring in-house expertise and, in the long run, also more risky. That's pretty analogous to my experience working as an electrical engineer as well, where orgs that outsource functions to other companies without retaining an in-house expert pay a very high cost, and not just monetarily. They often ship sub-par designs with long delays on top of having high costs. &quot;Buying&quot; can and often does reduce the amount of expertise necessary, but it often doesn't remove the need for expertise.</p>\n\n<p>This related to another common abstract argument that's commonly made, that companies should concentrate on &quot;their area of comparative advantage&quot; or &quot;most important problems&quot; or &quot;core business need&quot; and outsource everything else. We've already seen a couple of examples where this isn't true because, at a large enough scale, it's more profitable to have in-house expertise than not regardless of whether or not something is core to the business (one could argue that all of the things that are moved in-house are core to the business, but that would make the concept of coreness useless). Another reason this abstract advice is too simplistic is that businesses can somewhat arbitrarily choose what their comparative advantage is. A large<sup class=\"footnote-ref\" id=\"fnref:L\"><a rel=\"footnote\" href=\"#fn:L\">1</a></sup> example of this would be Apple bringing CPU design in-house. Since acquiring PA Semi (formerly the team from SiByte and, before that, a team from DEC) for $278M, Apple has been producing <a href=\"https://twitter.com/danluu/status/1433297089866383364\">the best chips in the phone and laptop power envelope by a pretty large margin</a>. But, before the purchase, there was nothing about Apple that made the purchase inevitable, that made CPU design an inherent comparative advantage of Apple. But if a firm can pick an area and make it an area of comparative advantage, saying that the firm should choose to concentrate on its comparative advantage(s) isn't very helpful advice.</p>\n\n<p>$278M is a lot of money in absolute terms, but as a fraction of Apple's resources, that was tiny and much smaller companies also have the capability to do cutting edge work by devoting a small fraction of their resources to it, e.g., Twitter, for a cost that any $100M company could afford, <a href=\"https://twitter.com/danluu/status/1381687511362138113\">created novel cache algorithms and data structures</a> and is doing other cutting edge cache work. Having great cache infra isn't any more core to Twitter's business than creating a great CPU is to Apple's, but it is a lever that Twitter can use to make more money than it could otherwise.</p>\n\n<p>For small companies, it doesn't make sense to have in-house experts for everything the company touches, but companies don't have to get all that large before it starts making sense to have in-house expertise in their operating system, language runtime, and other components that people often think of as being fairly specialized. Looking back at Twitter's history, Yao Yue has noted that when she was working on cache in Twitter's early days (when we had ~100 engineers), she would regularly go to the kernel team for help debugging production incidents and that, in some cases, debugging could've easily taken 10x longer without help from the kernel team. Social media companies tend to have relatively high scale on a per-user and per-dollar basis, so not every company is going to need the same kind of expertise when they have 100 engineers, but there are going to be other areas that aren't obviously core business needs where expertise will pay off even for a startup that has 100 engineers.</p>\n\n<p>Thanks to Ben Kuhn, Yao Yue, Pam Wolf, John Hergenroeder, Julien Kirch, Tom Brearley, and Kevin Burke for comments/corrections/discussion.</p>\n<div class=\"footnotes\">\n\n<hr />\n\n<ol>\n<li id=\"fn:L\"><p>Some other large examples of this are Korean chaebols, like Hyundai. Looking at how Hyundai Group's companies are connected to Hyundai Motor Company isn't really the right lens with which to examine Hyundai, but I'm going to use that lens anyway since most readers of this blog are probably already familiar with Hyundai Motor and will not be familiar with how Korean chaebols operate.</p>\n\n<p>Speaking very roughly, with many exceptions, American companies have tended to take the advice to specialize and concentrate on their competencies, at least since the 80s. This is the opposite of the direction that Korean chaebols have gone. Hyundai not only makes cars, they make the steel their cars use, the robots they use to automate production, the cement used for their factories, the construction equipment used to build their factories, the containers and ships used to ship cars (which they also operate), the transmissions for their cars, etc.</p>\n\n<p>If we look at a particular component, say, their 8-speed transmission vs. the widely used and lauded ZF 8HP transmission, reviewers typically slightly prefer the ZF transmission. But even so, having good-enough in-house transmissions, as well as many other in-house components that companies would typically buy, doesn't exactly seem to be a disadvantage for Hyundai.</p>\n <a class=\"footnote-return\" href=\"#fnref:L\"><sup>[return]</sup></a></li>\n</ol>\n</div>\n"]}]} {:tag :item, :attrs nil, :content [{:tag :title, :attrs nil, :content ["Some reasons to measure"]} {:tag :link, :attrs nil, :content ["https://danluu.com/why-benchmark/"]} {:tag :pubDate, :attrs nil, :content ["Fri, 27 Aug 2021 00:00:00 +0000"]} {:tag :guid, :attrs nil, :content ["https://danluu.com/why-benchmark/"]} {:tag :description, :attrs nil, :content ["\n\n<p>A question I get asked with some frequency is: why bother measuring X, why not build something instead? More bluntly, in a recent conversation with a newsletter author, his comment on some future measurement projects I wanted to do (in the same vein as other projects like <a href=\"https://danluu.com/keyboard-v-mouse/\">keyboard vs. mouse</a>, <a href=\"https://danluu.com/keyboard-latency/\">keyboard</a>, <a href=\"https://danluu.com/term-latency/\">terminal</a> and <a href=\"https://danluu.com/input-lag/\">end-to-end</a> latency measurements) was, &quot;so you just want to get to the top of Hacker News?&quot;\nThe implication for the former is that measuring is less valuable than building and for the latter that measuring isn't valuable at all (perhaps other than for fame), but I don't see measuring as lesser let alone worthless. If anything, because measurement is, <a href=\"https://twitter.com/danluu/status/1082321431109795840\">like writing</a>, not generally valued, it's much easier to find high ROI measurement projects than high ROI building projects.</p>\n\n<p>Let's start by looking at a few examples of high impact measurement projects. My go-to example for this is Kyle Kingsbury's work with <a href=\"https://jepsen.io\">Jepsen</a>. Before Jepsen, a handful of huge companies (the now $1T+ companies that people are calling &quot;hyperscalers&quot;) had decently tested distributed systems. They mostly didn't talk about testing methods in a way that really caused the knowledge to spread to the broader industry. Outside of those companies, most distributed systems were, <a href=\"https://danluu.com/testing/\">by my standards</a>, not particularly well tested.</p>\n\n<p>At the time, a common pattern in online discussions of distributed correctness was:</p>\n\n<p><strong>Person A</strong>: Database X corrupted my data.<br>\n<strong>Person B</strong>: It works for me. It's never corrupted my data.<br>\n<strong>A</strong>: How do you know? Do you ever check for data corruption?<br>\n<strong>B</strong>: What do you mean? I'd know if we had data corruption (alternate answer: <a href=\"https://mobile.twitter.com/danluu/status/918845240240410624\">sure, we sometimes have data corruption, but it's probably a hardware problem and therefore not our fault</a>)</p>\n\n<p>Kyle's early work found critical flaws in nearly everything he tested, despite Jepsen being much less sophisticated then than it is now:</p>\n\n<ul>\n<li><a href=\"https://aphyr.com/posts/283-call-me-maybe-redis\">Redis Cluster / Redis Sentinel</a>: &quot;we demonstrate Redis losing 56% of writes during a partition&quot;</li>\n<li><a href=\"https://aphyr.com/posts/284-call-me-maybe-mongodb\">MongoDB</a>: &quot;In this post, well see MongoDB drop a phenomenal amount of data&quot;</li>\n<li><a href=\"https://aphyr.com/posts/285-call-me-maybe-riak\">Riak</a>: &quot;well see how last-write-wins in Riak can lead to unbounded data loss&quot;</li>\n<li><a href=\"https://aphyr.com/posts/292-call-me-maybe-nuodb\">NuoDB</a>: &quot;If you are considering using NuoDB, be advised that the projects marketing and documentation may exceed its present capabilities&quot;</li>\n<li><a href=\"https://aphyr.com/posts/291-call-me-maybe-zookeeper\">Zookeeper</a>: the one early Jepsen test of a distributed system that didn't find a catastrophic bug</li>\n<li><a href=\"https://aphyr.com/posts/315-call-me-maybe-rabbitmq\">RabbitMQ clustering</a>: &quot;RabbitMQ lost ~35% of acknowledged writes ... This is not a theoretical problem. I know of at least two RabbitMQ deployments which have hit this in production.&quot;</li>\n<li><a href=\"https://aphyr.com/posts/316-call-me-maybe-etcd-and-consul\">etcd &amp; Consul</a>: &quot;etcds registers are not linearizable . . . 'consistent' reads in Consul return the local state of any node that considers itself a leader, allowing stale reads.&quot;</li>\n<li><a href=\"https://aphyr.com/posts/317-call-me-maybe-elasticsearch\">ElasticSearch</a>: &quot;the health endpoint will lie. Its happy to report a green cluster during split-brain scenarios . . . 645 out of 1961 writes acknowledged then lost.&quot;</li>\n</ul>\n\n<p>Many of these problems had existed for quite a while</p>\n\n<blockquote>\n<p>Whats really surprising about this problem is that its gone unaddressed for so long. The original issue was reported in July 2012; almost two full years ago. Theres no discussion on the website, nothing in the documentation, and users going through Elasticsearch training have told me these problems werent mentioned in their classes.</p>\n</blockquote>\n\n<p>Kyle then quotes a number of users who ran into issues into production and then dryly notes</p>\n\n<blockquote>\n<p>Some people actually advocate using Elasticsearch as a primary data store; I think this is somewhat less than advisable at present</p>\n</blockquote>\n\n<p>Although we don't have an A/B test of universes where Kyle exists vs. not and can't say how long it would've taken for distributed systems to get serious about correctness in a universe where Kyle didn't exist, from having spent many years looking at how developers treat correctness bugs, I would bet on distributed systems having rampant correctness problems until someone like Kyle came along. The typical response that I've seen when a catastrophic bug is reported is that the project maintainers will assume that the bug report is incorrect (and you can see many examples of this if you look at responses from the first few years of Kyle's work). When the reporter doesn't have a repro for the bug, which is quite common when it comes to distributed systems, the bug will be written off as non-existent.</p>\n\n<p>When the reporter does have a repro, the next line of defense is to argue that the behavior is fine (you can also see many examples of these from looking at responses to Kyle's work). Once the bug is acknowledged as real, the next defense is to argue that the bug doesn't need to be fixed because it's so uncommon (e.g., &quot;<a href=\"https://news.ycombinator.com/item?id=5913610\">It can be tempting to stand on an ivory tower and proclaim theory, but what is the real world cost/benefit? Are you building a NASA Shuttle Crawler-transporter to get groceries?</a>&quot;). And then, after it's acknowledged that the bug should be fixed, the final line of defense is to argue that the project takes correctness very seriously and there's really nothing more that could have been done; development and test methodology doesn't need to change because it was just a fluke that the bug occurred, and analogous bugs won't occur in the future without changes in methodology.</p>\n\n<p>Kyle's work blew through these defenses and, without something like it, my opinion is that we'd still see these as the main defense used against distributed systems bugs (as opposed to test methodologies that can actually produce pretty reliable systems).</p>\n\n<p>That's one particular example, but I find that it's generally true that, in areas where no one is publishing measurements/benchmarks of products, the products are generally sub-optimal, often in ways that are relatively straightforward to fix once measured. Here are a few examples:</p>\n\n<ul>\n<li>Keyboards: after I published <a href=\"https://danluu.com/keyboard-latency/\">this post on keyboard latency</a>, at least one major manufacturer that advertises high-speed gaming devices actually started optimizing input device latency; most users probably don't care much about input device latency, but it would be nice if manufacturers lived up to their claims</li>\n<li>Computers: after I published some other posts on <a href=\"https://danluu.com/input-lag/\">computer</a> <a href=\"https://danluu.com/term-latency/\">latency</a>, an engineer at a major software company that wasn't previously doing serious UI latency work told me that some engineers had started measuring and optimizing UI latency; also, the author of alacritty <a href=\"https://github.com/alacritty/alacritty/issues/673\">filed this ticket</a> on how to reduce alacritty latency</li>\n<li>Vehicle headlights: Jennifer Stockburger has noted that, when Consumer Reports started testing headlights, engineers at auto manufacturers thanked CR for giving them the ammunition they needed to force their employers to let them to engineer better headlights; previously, they would often lose the argument to designers who wanted nicer looking but less effective headlights since making cars safer by desiging better headlights is a hard sell because there's no business case, but making cars score higher on Consumer Reports reviews is an easy sell because that impacts sales numbers</li>\n<li>Vehicle <a href=\"https://en.wikipedia.org/wiki/Anti-lock_braking_system\">ABS</a>: after Consumer Reports and Car and Driver found that the Tesla Model 3 had extremely long braking distances (152 ft. from 60mph and 196 ft. from 70mph), Tesla updated the algorithms used to modulate the brakes, which improved braking distances enough that Tesla went from worst in class to better than average</li>\n<li>Vehicle impact safety: Other than Volvo, car manufacturers generally design their cars to get the highest possible score on published crash tests; <a href=\"https://danluu.com/car-safety/\">they'll add safety as necessary to score well on new tests when they're published, but not before</a></li>\n</ul>\n\n<p>This post has made some justifications for why measuring things is valuable but, to be honest, the impetus for my measurements is curiosity. I just want to know the answer to a question; most of the time, I don't write up my results. But even if you have no curiosity about what's actually happening when you interact with the world and you're &quot;just&quot; looking for something useful to do, the lack of measurements of almost everything means that it's easy to find high ROI measurement projects (at least in terms of impact on the world; if you want to make money, building something is probably easier to monetize).</p>\n\n<h3 id=\"appendix-the-motivation-for-my-measurement-posts\">Appendix: the motivation for my measurement posts</h3>\n\n<p><a href=\"https://en.wikipedia.org/wiki/The_Death_of_the_Author\">There's a sense in which it doesn't really matter why I decided to write these posts</a>, but if I were reading someone else's post on this topic, I'd still be curious what got them writing, so here's what prompted me to write my measurement posts (which, for the purposes of this list, include posts where I collate data and don't do any direct measurement).</p>\n\n<ul>\n<li><a href=\"https://danluu.com/car-safety/\">danluu.com/car-safety</a>: I was thinking about buying a car and wanted to know if I should expect significant differences in safety between manufacturers given that cars mostly get top marks on tests done in the U.S.\n\n<ul>\n<li>This wasn't included in the post because I thought it was too trivial to include (because the order of magnitude is obvious even without carrying out the computation), but I also computed the probability of dying in a car accident as well as the expected change in life expectancy between an old used car and a new-ish used car</li>\n</ul></li>\n<li><a href=\"https://danluu.com/cli-complexity/\">danluu.com/cli-complexity</a>: I had this idea when I saw something by Gary Berhardt where he showed off how to count the number of single-letter command line options that <code>ls</code>, which made me wonder if that was a recent change or not</li>\n<li><a href=\"https://danluu.com/overwatch-gender/\">danluu.com/overwatch-gender</a>: I had just seen two gigantic reddit threads debating whether or not there's a gender bias in how women are treated in online games and figured that I could get data on the matter in less time than was spent by people writing comments in those threads</li>\n<li><a href=\"https://danluu.com/input-lag/\">danluu.com/input-lag</a>: I wanted to know if I could trust my feeling that modern computers that I use are much higher latency than older devices that I'd used</li>\n<li><a href=\"https://danluu.com/keyboard-latency/\">danluu.com/keyboard-latency</a>: I wanted to know how much latency came from keyboards (display latency is already well tested by <a href=\"https://blurbusters.com\">https://blurbusters.com</a>)</li>\n<li><a href=\"https://danluu.com/bad-decisions/\">danluu.com/bad-decisions</a>: I saw a comment by someone in the rationality community defending bad baseball coaching decisions, saying that they're not a big deal because they only cost you maybe four games a year, which isn't a big deal and wanted to know how big a deal bad coaching decisions were</li>\n<li><a href=\"https://danluu.com/android-updates/\">danluu.com/android-updates</a>: I was curious how many insecure Android devices are out there due to most Android phones not being updatable</li>\n<li><a href=\"https://danluu.com/filesystem-errors/\">danluu.com/filesystem-errors</a>: I was curious how much filesystems had improved with respect to data corruption errors found by a 2005 paper</li>\n<li><a href=\"https://danluu.com/term-latency/\">danluu.com/term-latency</a>: I felt like terminal benchmarks were all benchmarking something that's basically irrelevant to user experience (throughput) and wanted to know what it would look like if someone benchmarked something that might matter more; I also wanted to know if my feeling that iTerm2 was slow was real or my imagination</li>\n<li><a href=\"https://danluu.com/keyboard-v-mouse/\">danluu.com/keyboard-v-mouse</a>: the most widely cited sources for keyboard vs. mousing productivity were pretty obviously bogus as well as being stated with extremely high confidence; I wanted to see if non-bogus tests would turn up the same results or different results</li>\n<li><a href=\"https://danluu.com/web-bloat/\">danluu.com/web-bloat</a>: I took a road trip across the U.S., where the web was basically unusable, and wanted to quantify the unusability of the web without access to very fast internet</li>\n<li><a href=\"https://danluu.com/bimodal-compensation/\">danluu.com/bimodal-compensation</a>: I was curious if we were seeing a hollowing out of mid-tier jobs in programming like we saw with law jobs</li>\n<li><a href=\"https://danluu.com/yegge-predictions/\">danluu.com/yegge-predictions</a>: I had the impression that Steve Yegge made unusually good predictions about the future of tech and wanted to see of my impression was correct</li>\n<li><a href=\"https://danluu.com/postmortem-lessons/\">danluu.com/postmortem-lessons</a>: I wanted to see what data was out there on postmortem causes to see if I could change how I operate and become more effective</li>\n<li><a href=\"https://danluu.com/boring-languages/\">danluu.com/boring-languages</a>: I was curious how much of the software I use was written in boring, old, languages</li>\n<li><a href=\"https://danluu.com/blog-ads/\">danluu.com/blog-ads</a>: I was curious how much money I could make if I wanted to monetize the blog</li>\n<li><a href=\"https://danluu.com/everything-is-broken/\">danluu.com/everything-is-broken</a>: I wanted to see if my impression of how many bugs I run into was correct</li>\n<li><a href=\"https://danluu.com/integer-overflow/\">danluu.com/integer-overflow</a>: I had a discussion with a language designer who was convinced that integer overflow checking was too expensive to do for an obviously bogus reason (because it's expensive if you do a benchmark that's 100% integer operations) and I wanted to see if my quick mental-math estimate of overhead was the right order of magnitude</li>\n<li><a href=\"https://danluu.com/octopress-speedup/\">danluu.com/octopress-speedup</a>: after watching a talk by Dan Espeset, I wanted to know if there were easy optimizations I could do to my then-Octopress site</li>\n<li><a href=\"https://danluu.com/broken-builds/\">danluu.com/broken-builds</a>: I had a series of discussions with someone who claimed that their project had very good build uptime despite it being broken regularly; I wanted to know if their claim was correct with respect to other, similar, projects</li>\n<li><a href=\"https://danluu.com/empirical-pl/\">danluu.com/empirical-pl</a>: I wanted to know what studies backed up claims from people who said that there was solid empirical proof of the superiority of &quot;fancy&quot; type systems</li>\n<li><a href=\"https://danluu.com/2choices-eviction/\">danluu.com/2choices-eviction</a>: I was curious what would happen if &quot;two random choices&quot; was applied to cache eviction</li>\n<li><a href=\"https://danluu.com/gender-gap/\">danluu.com/gender-gap</a>: I wanted to verify the claims in an article that claimed that there is no gender gap in tech salaries</li>\n<li><a href=\"https://danluu.com/3c-conflict/\">danluu.com/3c-conflict</a>: I wanted to create a simple example illustrating the impact of alignment on memory latency</li>\n</ul>\n\n<p>BTW, writing up this list made me realize that a narrative I had in my head about how and when I started really looking at data seriously must be wrong. I thought that this was something that came out of my current job, but that clearly cannot be the case since a decent fraction of my posts from before my current job are about looking at data and/or measuring things (and I didn't even list some of the data-driven posts where I just read some papers and look at what data they present). Blogger, measure thyself.</p>\n\n<h3 id=\"appendix-why-you-can-t-trust-some-reviews\">Appendix: why you can't trust some reviews</h3>\n\n<p>One thing that both increases and decreases the impact of doing good measurements is that most measurements that are published aren't very good. This increases the personal value of understanding how to do good measurements and of doing good measurements, but it blunts the impact on other people, since people generally don't understand what makes measurements invalid and don't have a good algorithm for deciding which measurements to trust.</p>\n\n<p>There are a variety of reasons that published measurements/reviews are often problematic. A major issue with reviews is that, in some industries, reviewers are highly dependent on manufacturers for review copies.</p>\n\n<p>Car reviews are one of the most extreme examples of this. Consumer Reports is the only major reviewer that independently sources their cars, which often causes them to disagree with other reviewers since they'll try to buy the trim level of the car that most people buy, which is often quite different from the trim level reviewers are given by manufacturers and Consumer Reports generally manages to avoid reviewing cars that are unrepresentatively picked or tuned. There have been a couple where Consumer Reports reviewers (who also buy the cars) have said that they thought someone realized they worked for Consumer Reports and then said that they needed to keep the car overnight before giving them the car they'd just bought; when that's happened, the reviewer has walked away from the purchase.</p>\n\n<p>There's pretty significant copy-to-copy variation between cars and the cars reviewers get tend to be ones that were picked to avoid cosmetic issues (paint problems, panel gaps, etc.) as well as checked for more serious issues. Additionally, cars can have their software and firmware tweaked (e.g., it's common knowledge that review copies of BMWs have an engine &quot;tune&quot; that would void your warranty if you modified your car similarly).</p>\n\n<p>Also, because Consumer Reports isn't getting review copies from manufacturers, they don't have to pull their punches and can write reviews that are highly negative, something you rarely see from car magazines and don't often see from car youtubers, where you generally have to read between the lines to get an honest review since a review that explicitly mentions negative things about a car can mean losing access (the youtuber who goes by &quot;savagegeese&quot; has mentioned having trouble getting access to cars from some companies after giving honest reviews).</p>\n\n<p>Camera lenses are another area where it's been documented that reviewers get unusually good copies of the item. There's tremendous copy-to-copy variation between lenses so vendors pick out good copies and let reviewers borrow those. In many cases (e.g., any of the FE mount ZA Zeiss lenses or the Zeiss lens on the RX-1), based on how many copies of a lens people need to try and return to get a good copy, it appears that the median copy of the lens has noticeable manufacturing defects and that, in expectation, perhaps one in ten lenses has no obvious defect (this could also occur if only a few copies were bad and those were serially returned, but very few photographers really check to see if their lens has issues due to manufacturing variation). Because it's so expensive to obtain a large number of lenses, the amount of copy-to-copy variation was unquantified until <a href=\"https://www.lensrentals.com/\">lensrentals</a> started measuring it; they've found that different manufacturers can have very different levels of copy-to-copy variation, which I hope will apply pressure to lens makers that are currently selling a lot of bad lenses while selecting good ones to hand to reviewers.</p>\n\n<p>Hard drives are yet another area where it's been documented that reviewers get copies of the item that aren't represnetative. Extreme Tech has reported, multiple times, that Adata, Crucial, and Western Digital have handed out review copies of SSDs that are not what you get as a consumer. One thing I find interesting about that case is that Extreme Tech says</p>\n\n<blockquote>\n<p>Agreeing to review a manufacturers product is an extension of trust on all sides. The manufacturer providing the sample is trusting that the review will be of good quality, thorough, and objective. The reviewer is trusting the manufacturer to provide a sample that accurately reflects the performance, power consumption, and overall design of the final product. When readers arrive to read a review, they are trusting that the reviewer in question has actually tested the hardware and that any benchmarks published were fairly run.</p>\n</blockquote>\n\n<p>This makes it sound like the reviewer's job is to take a trusted handed to them by the vendor and then run good benchmarks, absolving the reviewer of the responsibility of obtaining representative devices and ensuring that they're representative. I'm reminded of the SRE motto, &quot;hope is not a strategy&quot;. Trusting vendors is not a strategy. We know that vendors will lie and cheat to look better at benchmarks. Saying that it's a vendor's fault for lying or cheating can shift the blame, but it won't result in reviews being accurate or useful to consumers.</p>\n\n<p>While we've only discussed a few specific areas where there's published evidence that reviews cannot be trusted because they're compromised by companies, but this isn't anything specific to those industries. As consumers, we should expect that any review that isn't performed by a trusted, independent, agency, that purchases its own review copies has been compromised and is not representative of the median consumer experience.</p>\n\n<p>Another issue with reviews is that most online reviews that are highly ranked in search are really just SEO affiliate farms.</p>\n\n<p>A more general issue is that reviews are also affected by the exact same problem as items that are not reviewed: people generally can't tell which reviews are actually good and which are not, so review sites are selected on things other than the quality of the review. A prime example of this is Wirecutter, which is so popular among tech folks that noting that so many tech apartments in SF have identical Wirecutter recommended items is a tired joke. For people who haven't lived in SF, you can get a peek into the mindset by reading the comments <a href=\"https://www.reddit.com/r/fatFIRE/comments/iioq01/impossible_to_avoid_lifestyle_inflation_with/\">on this post about how it's &quot;impossible&quot; to not buy the wirecutter recommendation for anything</a> which is full of comments from people who re-assure that poster that, due to the high value of the poster's time, it would be irresponsible to do anything else.</p>\n\n<p>The thing I find funny about this is that if you take benchmarking seriously (in any field) and just read the methodology for the median Wirecutter review, without even trying out the items reviewed you can see that the methodology is poor and that they'll generally select items that are mediocre and sometimes even worst in class. A thorough exploration of this really deserves its own post, but I'll cite one example of poorly reviewed items here: in <a href=\"https://benkuhn.net/vc\">https://benkuhn.net/vc</a>, Ben Kuhn looked into how to create a nice video call experience, which included trying out a variety of microphones and webcams. Naturally, Ben tried Wirecutter's recommended microphone and webcam. The webcam was quite poor, no better than using the camera from an ancient 2014 iMac or his 2020 Macbook (and, to my eye, actually much worse; more on this later). And the microphone was roughly comparable to using the built-in microphone on his laptop.</p>\n\n<p>I have a lot of experience with Wirecutter's recommended webcam because so many people have it and it is shockingly bad in a distinctive way. Ben noted that, if you look at a still image, the white balance is terrible when used in the house he was in, and if you talk to other people who've used the camera, that is a common problem. But the issue I find to be worse is that, if you look at the video, under many conditions (and I think most, given how often I see this), the webcam will refocus regularly, making the entire video flash out of and then back into focus (another issue is that it often focuses on the wrong thing, but that's less common and I don't see that one with everybody who I talk to who uses Wirecutter's recommended webcam). I actually just had a call yesterday with a friend of mine who was using a different setup than I'd normally seen him with, the mediocre but perfectly acceptable macbook webcam. His video was going in and out of focus every 10-30 seconds, so I asked him if he was using Wirecutter's recommended webcam and of course he was, because what other webcam would someone in tech buy that has the same problem?</p>\n\n<p>This level of review quality is pretty typical for Wirecutter reviews and they appear to generally be the most respected and widely used review site among people in tech.</p>\n\n<h3 id=\"appendix-capitalism\">Appendix: capitalism</h3>\n\n<p>When I was in high school, there was a clique of proto-edgelords who did things like read The Bell Curve and argue its talking points to anyone who would listen.</p>\n\n<p>One of their favorite topics was how the free market would naturally cause companies that make good products rise to the top and companies that make poor products to disappear, resulting in things generally being safe, a good value, and so on and so forth. I still commonly see this opinion espoused by people working in tech, including people who fill their condos with Wirecutter recommended items. I find the juxtaposition of people arguing that the market will generally result in products being good while they themselves buy overpriced garbage to be deliciously ironic. To be fair, it's not all overpriced garbage. Some of it is overpriced mediocrity and some of it is actually good; it's just that it's not too different from what you'd get if you just naively bought random stuff off of Amazon without reading third-party reviews.</p>\n\n<p>For a related discussion, <a href=\"https://danluu.com/tech-discrimination/\">see this post on people who argue that markets eliminate discrimination even as they discriminate</a>.</p>\n\n<h3 id=\"appendix-other-examples-of-the-impact-of-measurement-or-lack-thereof\">Appendix: other examples of the impact of measurement (or lack thereof)</h3>\n\n<ul>\n<li>Electronic stability control\n\n<ul>\n<li>Toyota RAV4: <a href=\"https://www.youtube.com/watch?v=j3qrCNR4U9A\">before</a> <a href=\"https://www.youtube.com/watch?v=VtQ24W_lamY\">reviews</a> <a href=\"https://www.youtube.com/watch?v=xSRCJFCmvTk\">and after reviews</a></li>\n<li>Toyota Hilux <a href=\"https://www.youtube.com/watch?v=xoHbn8-ROiQ\">before reviews</a> <a href=\"https://www.youtube.com/watch?v=y2QSogJj3ec\"> and after reviews</a></li>\n<li>Nissan Rogue: major improvements after Consumer Reports found issues with stability control.</li>\n<li>Jeep Grand Cherokee: <a href=\"https://www.youtube.com/watch?v=zaYFLb8WMGM\">before reviews</a> <a href=\"https://www.youtube.com/watch?v=_xFPdfcNmVc\">and after reviews</a></li>\n</ul></li>\n<li>Some boring stuff at work: a year ago, I wrote <a href=\"https://danluu.com/metrics-analytics/\">this</a> <a href=\"https://danluu.com/tracing-analytics/\">pair</a> of posts on observability infrastructure at work. At the time, that work had driven 8 figures of cost savings and that's now well into the 9 figure range. This probably deserves its own post at some point, but the majority of the work was straightforward once someone could actually observe what's going on.\n\n<ul>\n<li>Relatedly: after seeing a few issues impact production services, I wrote a little (5k LOC) parser to parse every line seen in various host-level logs as a check to see what issues were logged that we weren't catching in our metrics. This found major issues in clusters that weren't using an automated solution to catch and remediate host-level issues; for some clusters, over 90% of hosts were actively corrupting data or had a severe performance problem. This led to the creation of a new team to deal with issues like this</li>\n</ul></li>\n<li>Tires\n\n<ul>\n<li>Almost all manufacturers other than Michelin see severely reduced wet, snow, and ice, performance as the tire wears\n\n<ul>\n<li>Jason Fenske says that a technical reason for this (among others) is that the <a href=\"https://en.wikipedia.org/wiki/Siping_(rubber)\">sipes</a> that improve grip are generally not cut to the full depth because doing so significantly increases manufacturing cost because the device that cuts the sipes will need to be stronger as well as wear out faster</li>\n<li>A non-technical reason for this is that a lot of published tire tests are done on new tires, so tire manufacturers can get nearly the same marketing benchmark value by creating only partial-depth sipes</li>\n</ul></li>\n<li>As Tire Rack has increased in prominence, some tire manufacturers have made their siping more multi-directional to improve handling while cornering instead of having siping mostly or only perpendicular to the direction of travel, which mostly only helps with acceleration and braking (Consumer Reports snow and ice scores are based on accelerating in a straight line on snow and braking in a straight line on ice, respectively, whereas Tire Rack's winter test scores emphasize all-around snow handling)</li>\n<li>An example of how measurement impact is bounded: Farrell Scott, the Project Category Manager for Michelin winter tires said that, when designing the successor to the Michelin X-ICE Xi3, one of the primary design criteria was to change how the tire looked because Michelin found that customers thought that the X-ICE Xi3, despite being up there with the Bridge Blizzak WS80 for being the best all-around winter tire (slightly better at some things, slightly worse at others), potential customers often chose other tires because they looked more like the popular conception of a winter tire, with &quot;aggressive&quot; looking tread blocks (this is one thing the famous Nokian Hakkapeliitta tire line was much better at). They also changed the name; instead of incrementing the number, the new tire was called Michelin X-ICE SNOW, to emphasize that the tire is suitable for snow as well as ice.</li>\n<li>Although some consumers do read reviews, many (and probably most) don't!</li>\n</ul></li>\n<li>HDMI to USB converters for live video\n\n<ul>\n<li>If you read the docs for the <a href=\"https://amzn.to/3gBdiRE\">Camlink 4k</a>, they note that the device should use bulk transfers on Windows and Isochronous transfers on Mac (if you use their software, it will automatically make this adjustment)\n\n<ul>\n<li>Fabian Giesen informed me that this may be for the same reason that, when some colleagues of his tested a particular USB3 device on Windows, only 1 out of 5 chipsets tested supported isochronous properly (the rest would do things like bluescreen or hang the machine)</li>\n</ul></li>\n<li>I've tried miscellaneous cheap HDMI to USB converters as alternatives to the Camlink 4k, and I have yet to find a cheap one that generally works across a wide variety of computers. They will generally work with at least one computer I have access to with at least one piece of software I want to use, but will simply not work or provide very distorted video in some cases. Perhaps someone should publish benchmarks on HDMI to USB converter quality!</li>\n</ul></li>\n<li>HDMI to VGA converters\n\n<ul>\n<li>Many of these get very hot and then overheat and stop working in 15 minutes to 2 hours. Some aren't even warm to the touch. Good luck figuring out which ones work!\n\n<br /></li>\n</ul></li>\n</ul>\n\n<p><small>Thanks to Fabian Giesen, Ben Kuhn, Yuri Vishnevsky, @chordowl, Seth Newman, Justin Blank, Per Vognsen, John Hergenroeder, Pam Wolf, Ivan Echevarria, and Jamie Brandon for comments/corrections/discussion.</small></p>\n"]}]} {:tag :item, :attrs nil, :content [{:tag :title, :attrs nil, :content ["Against essential and accidental complexity"]} {:tag :link, :attrs nil, :content ["https://danluu.com/essential-complexity/"]} {:tag :pubDate, :attrs nil, :content ["Tue, 29 Dec 2020 00:00:00 +0000"]} {:tag :guid, :attrs nil, :content ["https://danluu.com/essential-complexity/"]} {:tag :description, :attrs nil, :content ["\n\n<p>In the classic 1986 essay, <a href=\"http://worrydream.com/refs/Brooks-NoSilverBullet.pdf\">No Silver Bullet</a>, Fred Brooks argued that there is, in some sense, not that much that can be done to improve programmer productivity. His line of reasoning is that programming tasks contain a core of essential/conceptual<sup class=\"footnote-ref\" id=\"fnref:C\"><a rel=\"footnote\" href=\"#fn:C\">1</a></sup> complexity that's fundamentally not amenable to attack by any potential advances in technology (such as languages or tooling). He then uses an <a href=\"https://en.wikipedia.org/wiki/Amdahl%27s_law\">Ahmdahl's law</a> argument, saying that because 1/X of complexity is essential, it's impossible to ever get more than a factor of X improvement via technological improvements.</p>\n\n<p>Towards the end of the essay, Brooks claims that at least 1/2 (most) of complexity in programming is essential, bounding the potential improvement remaining for all technological programming innovations combined to, at most, a factor of 2<sup class=\"footnote-ref\" id=\"fnref:T\"><a rel=\"footnote\" href=\"#fn:T\">2</a></sup>:</p>\n\n<blockquote>\n<p>All of the technological attacks on the accidents of the software process are fundamentally limited by the productivity equation:</p>\n\n<p>Time of task = Sum over i { Frequency_i  Time_i }</p>\n\n<p>If, as I believe, the conceptual components of the task are now taking most of the time, then no amount of activity on the task components that are merely the expression of the concepts can give large productivity gains.</p>\n</blockquote>\n\n<p>Let's see how this essential complexity claim holds for a couple of things I did recently at work:</p>\n\n<ul>\n<li>scp from a bunch of hosts to read and download logs, and then parse the logs to understand the scope of a problem</li>\n<li>Query two years of metrics data from every instance of every piece of software my employer has, for some classes of software and then generate a variety of plots that let me understand some questions I have about what our software is doing and how it's using computer resources</li>\n</ul>\n\n<h4 id=\"logs\">Logs</h4>\n\n<p>If we break this task down, we have</p>\n\n<ul>\n<li>scp logs from a few hundred thousand machines to a local box\n\n<ul>\n<li>used a Python script for this to get parallelism with more robust error handling than you'd get out of pssh/parallel-scp</li>\n<li>~1 minute to write the script</li>\n</ul></li>\n<li>do other work while logs download</li>\n<li>parse downloaded logs (a few TB)\n\n<ul>\n<li>used a Rust script for this, a few minutes to write (used Rust instead of Python for performance reasons here  just opening the logs and scanning each line with idiomatic Python was already slower than I'd want if I didn't want to farm the task out to multiple machines)</li>\n</ul></li>\n</ul>\n\n<p>In 1986, perhaps I would have used telnet or ftp instead of scp. Modern scripting languages didn't exist yet (perl was created in 1987 and perl5, the first version that some argue is modern, was released in 1994), so writing code that would do this with parallelism and &quot;good enough&quot; error handling would have taken more than an order of magnitude more time than it takes today. In fact, I think just getting semi-decent error handling while managing a connection pool could have easily taken an order of magnitude longer than this entire task took me (not including time spent downloading logs in the background).</p>\n\n<p>Next up would be parsing the logs. It's not fair to compare an absolute number like &quot;1 TB&quot;, so let's just call this &quot;enough that we care about performance&quot; (we'll talk about scale in more detail in the metrics example). Today, we have our choice of high-performance languages where it's easy to write, fast, safe code and harness the power of libraries (e.g., a regexp library<sup class=\"footnote-ref\" id=\"fnref:L\"><a rel=\"footnote\" href=\"#fn:L\">3</a></sup>) that make it easy to write a quick and dirty script to parse and classify logs, farming out the work to all of the cores on my computer (I think Zig would've also made this easy, but I used Rust because my team has a critical mass of Rust programmers).</p>\n\n<p>In 1986, there would have been no comparable language, but more importantly, I wouldn't have been able to trivially find, download, and compile the appropriate libraries and would've had to write all of the parsing code by hand, turning a task that took a few minutes into a task that I'd be lucky to get done in an hour. Also, if I didn't know how to use the library or that I could use a library, I could easily find out how I should solve the problem on StackOverflow, which would massively reduce accidental complexity. Needless to say, there was no real equivalent to Googling for StackOverflow solutions in 1986.</p>\n\n<p>Moreover, even today, this task, a pretty standard programmer devops/SRE task, after at least an order of magnitude speedup over the analogous task in 1986, is still nearly entirely accidental complexity.</p>\n\n<p>If the data were exported into our metrics stack or if our centralized logging worked a bit differently, the entire task would be trivial. And if neither of those were true, but the log format were more uniform, I wouldn't have had to write any code after getting the logs; <a href=\"https://github.com/BurntSushi/ripgrep\">rg</a> or <a href=\"https://github.com/ggreer/the_silver_searcher\">ag</a> would have been sufficient. If I look for how much time I spent on the essential conceptual core of the task, it's so small that it's hard to estimate.</p>\n\n<h4 id=\"query-metrics\">Query metrics</h4>\n\n<p>We really only need one counter-example, but I think it's illustrative to look at a more complex task to see how Brooks' argument scales for a more involved task. If you'd like to skip this lengthy example, <a href=\"#summary\">click here to skip to the next section</a>.</p>\n\n<p>We can view my metrics querying task as being made up of the following sub-tasks:</p>\n\n<ul>\n<li>Write a set of <a href=\"https://en.wikipedia.org/wiki/Presto_(SQL_query_engine)\">Presto SQL</a> queries that effectively scan on the order of 100 TB of data each, from a data set that would be on the order of 100 PB of data if I didn't <a href=\"https://danluu.com/metrics-analytics/\">maintain tables that only contain a subset of data that's relevant</a>\n\n<ul>\n<li>Maybe 30 seconds to write the first query and a few minutes for queries to finish, using on the order of 1 CPU-year of CPU time</li>\n</ul></li>\n<li>Write some ggplot code to plot the various properties that I'm curious about\n\n<ul>\n<li>Not sure how long this took; less time than the queries took to complete, so this didn't add to the total time of this task</li>\n</ul></li>\n</ul>\n\n<p>The first of these tasks is so many orders of magnitude quicker to accomplish today that I'm not even able to hazard a guess to as to how much quicker it is today within one or two orders of magnitude, but let's break down the first task into component parts to get some idea about the ways in which the task has gotten easier.</p>\n\n<p>It's not fair to port absolute numbers like 100 PB into 1986, but just the idea of having a pipeline that collects and persists comprehensive data analogous to the data I was looking at for a consumer software company (various data on the resource usage and efficiency of our software) would have been considered absurd in 1986. Here we see one fatal flaw in the concept of accidental essential complexity providing an upper bound on productivity improvements: tasks with too much accidental complexity wouldn't have even been considered possible. The limit on how much accidental complexity Brooks sees is really a limit of his imagination, not something fundamental.</p>\n\n<p>Brooks explicitly dismisses increased computational power as something that will not improve productivity (&quot;Well, how many MIPS can one use fruitfully?&quot;, more on this later), but both storage and CPU power (not to mention network speed and RAM) were sources of accidental complexity so large that they bounded the space of problems Brooks was able to conceive of.</p>\n\n<p>In this example, let's say that we somehow had enough storage to keep the data we want to query in 1986. The next part would be to marshall on the order of 1 CPU-year worth of resources and have the query complete in minutes. As with the storage problem, this would have also been absurd in 1986<sup class=\"footnote-ref\" id=\"fnref:F\"><a rel=\"footnote\" href=\"#fn:F\">4</a></sup>, so we've run into a second piece of non-essential complexity so large that it would stop a person from 1986 from thinking of this problem at all.</p>\n\n<p>Next up would be writing the query. If I were writing for the Cray-2 and wanted to be productive, I probably would have written the queries in Cray's dialect of Fortran 77. Could I do that in less than 300 seconds per query? Not a chance; I couldn't even come close with Scala/Scalding and I think it would be a near thing even with Python/PySpark. This is the aspect where I think we see the smallest gain and we're still well above one order of magnitude here.</p>\n\n<p>After we have the data processed, we have to generate the plots. Even with today's technology, I think not using ggplot would cost me at least 2x in terms of productivity. I've tried every major plotting library that's supposedly equivalent (in any language) and every library I've tried either has multiple show-stopping bugs rendering plots that I consider to be basic in ggplot or is so low-level that I lose more than 2x productivity by being forced to do stuff manually that would be trivial in ggplot. In 2020, the existence of a single library already saves me 2x on this one step. If we go back to 1986, before the concept of <a href=\"https://amzn.to/3r9Mvzw\">the grammar of graphics</a> and any reasonable implementation, there's no way that I wouldn't lose at least two orders of magnitude of time on plotting even assuming some magical workstation hardware that was capable of doing the plotting operations I do in a reasonable amount of time (my machine is painfully slow at rendering the plots; a Cray-2 would not be able to do the rendering in anything resembling a reasonable timeframe).</p>\n\n<p>The number of orders of magnitude of accidental complexity reduction for this problem from 1986 to today is so large I can't even estimate it and yet this problem still contains such a large fraction of accidental complexity that it's once again difficult to even guess at what fraction of complexity is essential. To write it all down all of the accidental complexity I can think of would require at least 20k words, but just to provide a bit of the flavor of the complexity, let me write down a few things.</p>\n\n<ul>\n<li>SQL; this is one of those things that's superficially simple <a href=\"https://scattered-thoughts.net/writing/select-wat-from-sql/\">but actually extremely complex</a>\n\n<ul>\n<li>Also, Presto SQL</li>\n</ul></li>\n<li>Arbitrary Presto limits, some of which are from Presto and some of which are from the specific ways we operate Presto and the version we're using\n\n<ul>\n<li>There's an internal Presto data structure assert fail that gets triggered when I use both <code>numeric_histogram</code> and <code>cross join unnest</code> in a particular way. Because it's a waste of time to write the bug-exposing query, wait for it to fail, and then re-write it, I have a mental heuristic I use to guess, for any query that uses both constructs, whether or not I'll hit the bug and I apply it to avoid having to write two queries. If the heuristic applies, I'll instead write a more verbose query that's slower to execute instead of the more straightforward query</li>\n<li>We partition data by date, but Presto throws this away when I join tables, resulting in very large and therefore expensive joins when I join data across a long period of time even though, in principle, this could be a series of cheap joins; if the join is large enough to cause my query to blow up, I'll write what's essentially a little query compiler to execute day-by-day queries and then post-process the data as necessary instead of writing the naive query\n\n<ul>\n<li>There are a bunch of cases where some kind of optimization in the query will make the query feasible without having to break the query across days (e.g., if I want to join host-level metrics data with the table that contains what cluster a host is in, that's a very slow join across years of data, but I also know what kinds of hosts are in which clusters, which, in some cases, lets me filter hosts out of the host-level metrics data that's in there, like core count and total memory, which can make the larger input to this join small enough that the query can succeed without manually partitioning the query)</li>\n</ul></li>\n<li>We have a Presto cluster that's &quot;fast&quot; but has &quot;low&quot; memory limits a cluster that's &quot;slow&quot; but has &quot;high&quot; memory limits, so I mentally estimate how much per-node memory a query will need so that I can schedule it to the right cluster</li>\n<li>etc.</li>\n</ul></li>\n<li>When, for performance reasons, I should compute the CDF or histogram in Presto vs. leaving it to the end for ggplot to compute</li>\n<li>How much I need to downsample the data, if at all, for ggplot to be able to handle it, and how that may impact analyses</li>\n<li>Arbitrary ggplot stuff\n\n<ul>\n<li>roughly how many points I need to put in a scatterplot before I should stop using <code>size = [number]</code> and should switch to single-pixel plotting because plotting points as circles is too slow</li>\n<li>what the minimum allowable opacity for points is</li>\n<li>If I exceed the maximum density where you can see a gradient in a scatterplot due to this limit, how large I need to make the image to reduce the density appropriately (when I would do this instead of using a heatmap deserves its own post)</li>\n<li>etc.</li>\n</ul></li>\n<li>All of the above is about tools that I use to write and examine queries, but there's also the mental model of all of the data issues that must be taken into account when writing the query in order to generate a valid result, which includes things like clock skew, Linux accounting bugs, issues with our metrics pipeline, issues with data due to problems in the underlying data sources, etc.</li>\n<li>etc.</li>\n</ul>\n\n<p>For each of Presto and ggplot I implicitly hold over a hundred things in my head to be able to get my queries and plots to work and I choose to use these because these are the lowest overhead tools that I know of that are available to me. If someone asked me to name the percentage of complexity I had to deal with that was essential, I'd say that it was so low that there's no way to even estimate it. For some queries, it's arguably zero  my work was necessary only because of some arbitrary quirk and there would be no work to do without the quirk. But even in cases where some kind of query seems necessary, I think it's unbelievable that essential complexity could have been more than 1% of the complexity I had to deal with.</p>\n\n<p>Revisiting Brooks on computer performance, even though I deal with complexity due to the limitations of hardware performance in 2020 and would love to have faster computers today, Brooks wrote off faster hardware as pretty much not improving developer productivity in 1986:</p>\n\n<blockquote>\n<p>What gains are to be expected for the software art from the certain and rapid increase in the power and memory capacity of the individual workstation? Well, how many MIPS can one use fruitfully? The composition and editing of programs and documents is fully supported by todays speeds. Compiling could stand a boost, but a factor of 10 in machine speed would surely . . .</p>\n</blockquote>\n\n<p>But this is wrong on at least two levels. First, if I had access to faster computers, a huge amount of my accidental complexity would go away (if computers were powerful enough, I wouldn't need complex tools like Presto; I could just run a query on my local computer). We have much faster computers now, but it's still true that having faster computers would make many involved engineering tasks trivial. As James Hague notes, in the mid-80s, <a href=\"https://prog21.dadgum.com/29.html\">writing a spellchecker was a serious engineering problem due to performance constraints</a>.</p>\n\n<p>Second, (just for example) ggplot only exists because computers are so fast. A common complaint from people who work on performance is that tool X has somewhere between two and ten orders of magnitude of inefficiency when you look at the fundamental operations it does vs. the speed of hardware today<sup class=\"footnote-ref\" id=\"fnref:O\"><a rel=\"footnote\" href=\"#fn:O\">5</a></sup>. But what fraction of programmers can realize even one half of the potential performance of a modern multi-socket machine? I would guess fewer than one in a thousand and I would say certainly fewer than one in a hundred. And performance knowledge isn't independent of other knowledge  controlling for age and experience, it's negatively correlated with knowledge of non-&quot;systems&quot; domains since time spent learning about the esoteric accidental complexity necessary to realize half of the potential of a computer is time spent not learning about &quot;directly&quot; applicable domain knowledge. When we look software that requires a significant amount of domain knowledge (e.g., ggplot) or that'slarge enough that it requires a large team to implement (e.g., IntelliJ<sup class=\"footnote-ref\" id=\"fnref:V\"><a rel=\"footnote\" href=\"#fn:V\">6</a></sup>), the vast majority of it wouldn't exist if machines were orders of magnitude slower and writing usable software required wringing most of the performance out of the machine. Luckily for us, hardware has gotten much faster, allowing the vast majority of developers to ignore performance-related accidental complexity and instead focus on all of the other accidental complexity necessary to be productive today.</p>\n\n<p>Faster computers both reduce the amount of accidental complexity tool users run into as well as the amount of accidental complexity that tool creators need to deal with, allowing more productive tools to come into existence.</p>\n\n<h3 id=\"summary\">Summary</h3>\n\n<p>To summarize, Brooks states a bound on how much programmer productivity can improve. But, in practice, to state this bound correctly, one would have to be able to conceive of problems that no one would reasonably attempt to solve due to the amount of friction involved in solving the problem with current technologies.</p>\n\n<p>Without being able to predict the future, this is impossible to estimate. If we knew the future, it might turn out that there's some practical limit on how much computational power or storage programmers can productively use, bounding the resources available to a programmer, but getting a bound on the amount of accidental complexity would still require one to correctly reason about how programmers are going to be able to use zillions times more resources than are available today, which is so difficult we might as well call it impossible.</p>\n\n<p>Moreover, for each class of tool that could exist, one would have to effectively anticipate all possible innovations. Brooks' strategy for this was to look at existing categories of tools and state, for each, that they would be ineffective or that they were effective but played out. This was wrong not only because it underestimated gains from classes of tools that didn't exist yet, <a href=\"https://danluu.com/butler-lampson-1999/\">weren't yet effective</a>, or he wasn't familiar with (e.g., he writes off formal methods, but it doesn't even occur to him to mention fuzzers, static analysis tools that don't fully formally verify code, tools like valgrind, etc.) but also because Brooks thought that every class of tool where there was major improvement was played out and it turns out that none of them were (e.g., programming languages, which Brooks wrote just before the rise of &quot;scripting languages&quot; as well as just before GC langauges took over the vast majority of programming).</p>\n\n<p>In some sense, this isn't too different from when <a href=\"https://danluu.com/cli-complexity/#maven\">we looked at Unix and found the Unix mavens saying that we should write software like they did in the 70s</a> and that <a href=\"https://twitter.com/danluu/status/885214004649615360\">the languages they invented are as safe as any language can be</a>. Long before computers were invented, elders have been telling the next generation that they've done everything that there is to be done and that the next generation won't be able to achieve more. Even without knowing any specifics about programming, we can look at how well these kinds of arguments have held up historically and have decent confidence that the elders are not, in fact, correct this time.</p>\n\n<p>Looking at the specifics with the benefit of hindsight, we can see that Brooks' 1986 claim that we've basically captured all the productivity gains high-level languages can provide isn't too different from an assembly language programmer saying the same thing in 1955, thinking that assembly is as good as any language can be<sup class=\"footnote-ref\" id=\"fnref:A\"><a rel=\"footnote\" href=\"#fn:A\">7</a></sup> and that his claims about other categories are similar. The main thing these claims demonstrate are a lack of imagination. When Brooks referred to conceptual complexity, he was referring to complexity of using the conceptual building blocks that Brooks was familiar with in 1986 (on problems that Brooks would've thought of as programming problems). There's no reason anyone should think that Brooks' 1986 conception of programming is fundamental any more than they should think that how an assembly programmer from 1955 thought was fundamental. People often make fun of the apocryphal &quot;640k should be enough for anybody&quot; quote, but Brooks saying that, across all categories of potential productivity improvement, we've done most of what's possible to do, is analogous and not apocryphal!</p>\n\n<p>We've seen that, if we look at the future, the fraction of complexity that might be accidental is effectively unbounded. One might argue that, if we look at the present, these terms wouldn't be meaningless. But, while this will vary by domain, I've personally never worked on a non-trivial problem that isn't completely dominated by accidental complexity, making the concept of essential complexity meaningless on any problem I've worked on that's worth discussing.</p>\n\n<p>Thanks to Peter Bhat Harkins, Ben Kuhn, Yuri Vishnevsky, Chris Granger, Wesley Aptekar-Cassels, Lifan Zeng, Scott Wolchok, Martin Horenovsky, @realcmb, Kevin Burke, Aaron Brown, and Saul Pwanson for comments/corrections/discussion.</p>\n\n<p><link rel=\"prefetch\" href=\"https://danluu.com/cli-complexity/\">\n<link rel=\"prefetch\" href=\"https://danluu.com/metrics-analytics/\">\n<link rel=\"prefetch\" href=\"https://danluu.com/\">\n<link rel=\"prefetch\" href=\"https://danluu.com/about/\"></p>\n<div class=\"footnotes\">\n\n<hr />\n\n<ol>\n<li id=\"fn:C\"><blockquote>\n<p>The accidents I discuss in the next section. First let us consider the essence</p>\n\n<p>The essence of a software entity is a construct of interlocking concepts: data sets, relationships among data items, algorithms, and invocations of functions. This essence is abstract, in that the conceptual construct is the same under many different representations. It is nonetheless highly precise and richly detailed.</p>\n\n<p>I believe the hard part of building software to be the specification, design, and testing of this conceptual construct, not the labor of representing it and testing the fidelity of the representation. We still make syntax errors, to be sure; but they are fuzz compared to the conceptual errors in most systems.</p>\n</blockquote>\n <a class=\"footnote-return\" href=\"#fnref:C\"><sup>[return]</sup></a></li>\n\n<li id=\"fn:T\"><p>Curiously, he also claims, in the same essay, that no individual improvement can yield a 10x improvement within one decade. While this technically doesn't contradict his Ahmdal's law argument plus the claim that &quot;most&quot; (i.e., at least half) of complexity is essential/conceptual, it's unclear why he would include this claim as well.</p>\n\n<p>When Brooks revisited his essay in 1995 in No Silver Bullet Refired, he claimed that he was correct by using the weakest form of the three claims he made in 1986, that within one decade, no single improvement would result in an order of magnitude improvement. However, he did then re-state the strongest form of the claim he made in 1986 and made it again in 1995, saying that this time, no set of technological improvements could improve productivity more than 2x, for real:</p>\n\n<blockquote>\n<p>It is my opinion, and that is all, that the accidental or representational part of the work is now down to about half or less of the total. Since this fraction is a question of fact, its value could in principle be settled by measurement. Failing that, my estimate of it can be corrected by better informed and more current estimates. Significantly, no one who has written publicly or privately has asserted that the accidental part is as large as 9/10.</p>\n</blockquote>\n\n<p>By the way, I find it interesting that he says that no one disputed this 9/10ths figure. Per the body of this post, I would put it at far above 9/10th for my day-to-day work and, if I were to try to solve the same problems in 1986, the fraction would have been so high that people wouldn't have even conceived of the problem. As a side effect of having worked in hardware for a decade, I've also done work that's not too different from what some people faced in 1986 (microcode, assembly &amp; C written for DOS) and I would put that work as easily above 9/10th as well.</p>\n\n<p>Another part of his follow-up that I find interesting is that he quotes Harel's &quot;Biting the Silver Bullet&quot; from 1992, which, among other things, argues that that decade deadline for an order of magnitude improvement is arbitrary. Brooks' response to this is</p>\n\n<blockquote>\n<p>There are other reasons for the decade limit: the claims made for candidate bullets all have had a certain immediacy about them . . . We will surely make substantial progress over the next 40 years; an order of magnitude over 40 years is hardly magical.</p>\n</blockquote>\n\n<p>But by Brooks' own words when he revisits the argument in 1995, if 9/10th of complexity is essential, it would be impossible to get more than an order of magnitude improvement from reducing it, with no caveat on the timespan:</p>\n\n<blockquote>\n<p>&quot;NSB&quot; argues, indisputably, that if the accidental part of the work is less than 9/10 of the total, shrinking it to zero (which would take magic) will not give an order of magnitude productivity improvement.</p>\n</blockquote>\n\n<p>Both his original essay and the 1995 follow-up are charismatically written and contain a sort of local logic, where each piece of the essay sounds somewhat reasonable if you don't think about it too hard and you forget everything else the essay says. As with the original, a pedant could argue that this is technically not incoherent  after all, Brooks could be saying:</p>\n\n<ul>\n<li>at most 9/10th of complexity is accidental (if we ignore the later 1/2 claim, which is the kind of suspension of memory/disbelief one must do to read the essay)</li>\n<li>it would not be surprising for us to eliminate 100% of accidental complexity after 40 years</li>\n</ul>\n\n<p>While this is technically consistent (again, if we ignore the part that's inconsistent) and is a set of claims one could make, this would imply that 40 years from 1986, i.e., in 2026, it wouldn't be implausible for there to be literally zero room for any sort of productivity improvement from tooling, languages, or any other potential source of improvement. But this is absurd. If we look at other sections of Brooks' essay and combine their reasoning, we see other inconsistencies and absurdities.</p>\n <a class=\"footnote-return\" href=\"#fnref:T\"><sup>[return]</sup></a></li>\n\n<li id=\"fn:L\"><p>Another issue that we see here is Brooks' insistence on bright-line distinctions between categories. Essential vs. accidental complexity. &quot;Types&quot; of solutions, such as languages vs. &quot;build vs. buy&quot;, etc.</p>\n\n<p>Brooks admits that &quot;build vs. buy&quot; is one avenue of attack on essential complexity. Perhaps he would agree that buying a regexp package would reduce the essential complexity since that would allow me to avoid keeping all of the concepts associated with writing a parser in my head for simple tasks. But what if, instead of buying regexes, I used a language where they're bundled into the standard library or is otherwise distributed with the language? Or what if, instead of having to write my own concurrency primitives, those are bundled into the language? Or for that matter, what about <a href=\"https://golang.org/pkg/net/http/\">an entire HTTP server</a>? There is no bright-line distinction between what's in a library one can &quot;buy&quot; (for free in many cases nowadays) and one that's bundled into the language, so there cannot be a bright-line distinction between what gains a language provides and what gains can be &quot;bought&quot;. But if there's no bright-line distinction here, then it's not possible to say that one of these can reduce essential complexity and the other can't and maintain a bright-line distinction between essential and accidental complexity (in a response to Brooks, Harel argued against there being a clear distinction in a response, and Brooks' response was to say that there there is, in fact, a bright-line distinction, although he provided no new argument).</p>\n\n<p>Brooks' repeated insistence on these false distinctions means that the reasoning in the essay isn't composable. As we've already seen in another footnote, if you take reasoning from one part of the essay and apply it alongside reasoning from another part of the essay, it's easy to create absurd outcomes and sometimes outright contradictions.</p>\n\n<p>I suspect this is one reason discussions about essential vs. accidental complexity are so muddled. It's not just that <a href=\"https://twitter.com/hillelogram/status/1211433465956196352\">Brooks is being vague and handwave-y</a>, he's actually not self-consistent, so there isn't and cannot be a coherent takeaway. Michael Feathers has noted <a href=\"https://twitter.com/mfeathers/status/1259295515532865543\">that people are generally not able to correct identify essential complexity</a>; as he says, <a href=\"https://twitter.com/mfeathers/status/1256995176959971329\">One persons essential complexity is another persons accidental complexity.</a>. This is exactly what we should expect from the essay, since people who have different parts of it in mind will end up with incompatible views.</p>\n\n<p>This is also a problem when critisizing Brooks. Inevitably, someone will say that what Brooks really meant was something completely different. And that will be true. But Brooks will have meant something completely different while also having meant the things he said that I mention. In defense of the view I'm presenting in the body of the text here, it's a coherent view that one could have had in 1986. Many of Brooks' statements don't make sense even when considered as standalone statements, let alone when cross-referenced with the rest of his essay. For example, the statement that no single development will result in an order of magnitude improvement in the next decade. This statement is meaningless as Brooks does not define and no one can definitively say what a &quot;single improvement&quot; is. And, as mentioned above, Brooks' essay reads quite oddly and basically does not make sense if that's what he's trying to claim. Another issue with most other readings of Brooks is that those are positions that are also meaningless even if Brooks had done the work to make them well defined. Why does it matter if one single improvement or two result in an order of magnitude improvement. If it's two improvements, we'll use them both.</p>\n <a class=\"footnote-return\" href=\"#fnref:L\"><sup>[return]</sup></a></li>\n\n<li id=\"fn:F\"><p>Let's arbitrarily use a Motorola 68k processor with an FP co-processor that could do 200 kFLOPS as a reference for how much power we might have in a consumer CPU (FLOPS is a bad metric for multiple reasons, but this is just to get an idea of what it would take to get 1 CPU-year of computational resources, and Brooks himself uses MIPS as a term as if it's meaningful). By comparison, the Cray-2 could achieve 1.9 GFLOPS, or roughly 10000x the performance (I think actually less if we were to do a comparable comparison instead of using non-comparable GFLOPS numbers, but let's be generous here). There are 525600 / 5 = 105120 five minute periods in a year, so to get 1 CPU year's worth of computation in five minutes we'd need 105120 / 10000 = 10 Cray-2s per query, not including the overhead of aggregating results across Cray-2s.</p>\n\n<p>It's unreasonable to think that a consumer software company in 1986 would have enough Cray-2s lying around to allow for any random programmer to quickly run CPU years worth of queries whenever they wanted to do some data analysis. One sources claims that 27 Cray-2s were ever made over the production lifetime of the machine (1985 to 1990). Even if my employer owned all of them and they were all created by 1986, that still wouldn't be sufficient to allow the kind of ad hoc querying capacity that I have access to in 2020.</p>\n\n<p>Today, someone at a startup can even make an analogous argument when comparing to a decade ago. You used to have to operate a cluster that would be prohibitively annoying for a startup to operate unless the startup is very specialized, but you can now just use Snowflake and basically get Presto but only pay for the computational power you use (plus a healthy markup) instead of paying to own a cluster and for all of the employees necessary to make sure the cluster is operable.</p>\n <a class=\"footnote-return\" href=\"#fnref:F\"><sup>[return]</sup></a></li>\n<li id=\"fn:O\">I actually run into one of these every time I publish a new post. I write my posts in Google docs and then copy them into emacs running inside tmux running inside Alacritty. My posts are small enough to fit inside L2 cache, so I could have 64B/3.5 cycle write bandwidth. And yet, the copy+paste operation can take ~1 minute and is so slow I can watch the text get pasted in. Since my chip is working super hard to make sure the copy+paste happens, it's running at its full non-turbo frequency of 4.2Ghz, giving it 76.8GB/s of write bandwidth. For a 40kB post, 1 minute = 666B/s. 76.8G/666 =~ 8 orders of magnitude left on the table.\n <a class=\"footnote-return\" href=\"#fnref:O\"><sup>[return]</sup></a></li>\n<li id=\"fn:V\">In this specific case, I'm sure somebody will argue that Visual Studio was quite nice in 2000 and ran on much slower computers (and the debugger was arguably better than it is in the current version). But there was no comparable tool on Linux, nor was there anything comparable to today's options in the VSCode-like space of easy-to-learn programming editor that provides programming-specific facilities (as opposed to being a souped up version of notepad) without being a full-fledged IDE.\n <a class=\"footnote-return\" href=\"#fnref:V\"><sup>[return]</sup></a></li>\n\n<li id=\"fn:A\"><p>And by the way, this didn't only happen in 1955. I've worked with people who, this century, told me that assembly is basically as productive as any high level language. This probably sounds ridiculous to almost every reader of this blog, but if you talk to people who spend all day writing microcode or assembly, you'll occasionally meet somebody who believes this.</p>\n\n<p>Thinking that the tools you personally use are as good as it gets is an easy trap to fall into.</p>\n <a class=\"footnote-return\" href=\"#fnref:A\"><sup>[return]</sup></a></li>\n</ol>\n</div>\n"]}]} {:tag :item, :attrs nil, :content [{:tag :title, :attrs nil, :content ["How do cars fare in crash tests they're not specifically optimized for?"]} {:tag :link, :attrs nil, :content ["https://danluu.com/car-safety/"]} {:tag :pubDate, :attrs nil, :content ["Tue, 30 Jun 2020 00:06:34 -0700"]} {:tag :guid, :attrs nil, :content ["https://danluu.com/car-safety/"]} {:tag :description, :attrs nil, :content ["\n\n<p>Any time you have a benchmark that gets taken seriously, some people will start gaming the benchmark. Some famous examples in computing are the CPU benchmark <a href=\"https://spec.org/benchmarks.html\">specfp</a> and video game benchmarks. With specfp, Sun managed to increase its score on <a href=\"https://www.spec.org/osg/cpu2000/CFP2000/179.art/docs/179.art.html\">179.art</a> (a sub-benchmark of specfp) by 12x with a compiler tweak that essentially re-wrote the benchmark kernel, which increased the Sun <a href=\"https://en.wikipedia.org/wiki/UltraSPARC_III\">UltraSPARC</a>s overall specfp score by 20%. At times, GPU vendors have added specialized benchmark-detecting code to their drivers that lowers image quality during benchmarking to produce higher benchmark scores. Of course, gaming the benchmark isn't unique to computing and we see people do this <a href=\"https://danluu.com/discontinuities/\">in other fields</a>. Its not surprising that we see this kind of behavior since improving benchmark scores by cheating on benchmarks is much cheaper (and therefore higher ROI) than improving benchmark scores by actually improving the product.</p>\n\n<p>As a result, I'm generally suspicious when people take highly specific and well-known benchmarks too seriously. Without other data, you don't know what happens when conditions aren't identical to the conditions in the benchmark. With GPU and CPU benchmarks, its possible for most people to run the standard benchmarks with slightly tweaked conditions. If the results change dramatically for small changes to the conditions, thats evidence that the vendor is, if not cheating, at least shading the truth.</p>\n\n<p>Benchmarks of physical devices can be more difficult to reproduce. Vehicle crash tests are a prime example of this -- they're highly specific and well-known benchmarks that use up a car for some test runs.</p>\n\n<p>While there are multiple organizations that do crash tests, they each have particular protocols that they follow. Car manufacturers, if so inclined, could optimize their cars for crash test scores instead of actual safety. Checking to see if crash tests are being gamed with hyper-specific optimizations isn't really feasible for someone who isn't a billionaire. The easiest way we can check is by looking at what happens when new tests are added since that lets us see a crash test result that manufacturers weren't optimizing for just to get a good score.</p>\n\n<p>While having car crash test results is obviously better than not having them, the results themselves don't tell us what happens when we get into an accident that doesn't exactly match a benchmark. Unfortunately, if we get into a car accident, we don't get to ask the driver of the vehicle we're colliding with to change their location, angle of impact, and speed, in order for the collision to comply with an <a href=\"https://en.wikipedia.org/wiki/Insurance_Institute_for_Highway_Safety\">IIHS</a>, <a href=\"https://en.wikipedia.org/wiki/National_Highway_Traffic_Safety_Administration\">NHTSA</a>, or <a href=\"https://en.wikipedia.org/wiki/New_Car_Assessment_Program\">*NCAP</a>, test protocol.</p>\n\n<p>For this post, we're going to look at <a href=\"https://en.wikipedia.org/wiki/Insurance_Institute_for_Highway_Safety\">IIHS</a> test scores when they added the (driver side) small overlap and passenger side small overlap tests, which were added in 2012, and 2018, respectively. We'll start with a summary of the results and then discuss what those results mean and other factors to consider when evaluating car safety, followed by details of the methodology.</p>\n\n<h3 id=\"results\">Results</h3>\n\n<p>The ranking below is mainly based on how well vehicles scored when the driver-side small overlap test was added in 2012 and how well models scored when they were modified to improve test results.</p>\n\n<ul>\n<li><strong>Tier 1</strong>: good without modifications\n\n<ul>\n<li>Volvo</li>\n</ul></li>\n<li><strong>Tier 2</strong>: mediocre without modifications; good with modifications\n\n<ul>\n<li>None</li>\n</ul></li>\n<li><strong>Tier 3</strong>: poor without modifications; good with modifications\n\n<ul>\n<li>Mercedes</li>\n<li>BMW</li>\n</ul></li>\n<li><strong>Tier 4</strong>: poor without modifications; mediocre with modifications\n\n<ul>\n<li>Honda</li>\n<li>Toyota</li>\n<li>Subaru</li>\n<li>Chevrolet</li>\n<li>Tesla</li>\n<li>Ford</li>\n</ul></li>\n<li><strong>Tier 5</strong>: poor with modifications or modifications not made\n\n<ul>\n<li>Hyundai</li>\n<li>Dodge</li>\n<li>Nissan</li>\n<li>Jeep</li>\n<li>Volkswagen</li>\n</ul></li>\n</ul>\n\n<p>These descriptions are approximations. Honda, Ford, and Tesla are the poorest fits for these descriptions, with Ford arguably being halfway in between Tier 4 and Tier 5 but also arguably being better than Tier 4 and not fitting into the classification and Honda and Tesla not really properly fitting into any category (with their category being the closest fit), but some others are also imperfect. Details below.</p>\n\n<h3 id=\"general-commentary\">General commentary</h3>\n\n<p>If we look at overall mortality in the U.S., there's a pretty large age range for which car accidents are the leading cause of death. Although the numbers will vary depending on what data set we look at, when the driver-side small overlap test was added, the IIHS estimated that 25% of vehicle fatalities came from small overlap crashes. It's also worth noting that small overlap crashes were thought to be implicated in a significant fraction of vehicle fatalities at least since the 90s; this was not a novel concept in 2012.</p>\n\n<p>Despite the importance of small overlap crashes, from looking at the results when the IIHS added the driver-side and passenger-side small overlap tests in 2012 and 2018, it looks like almost all car manufacturers were optimizing for benchmark and not overall safety. Except for Volvo, all carmakers examined produced cars that fared poorly on driver-side small overlap crashes until the driver-side small overlap test was added.</p>\n\n<p>When the driver-side small overlap test was added in 2012, most manufacturers modified their vehicles to improve driver-side small overlap test scores. However, until the IIHS added a passenger-side small overlap test in 2018, most manufacturers skimped on the passenger side. When the new test was added, they beefed up passenger safety as well. To be fair to car manufacturers, some of them got the hint about small overlap crashes when the driver-side test was added in 2012 and did not need to make further modifications to score well on the passenger-side test, including Mercedes, BMW, and Tesla (and arguably a couple of others, but the data is thinner in the other cases; Volvo didn't need a hint).</p>\n\n<h3 id=\"other-benchmark-limitations\">Other benchmark limitations</h3>\n\n<p>There are a number of other areas where we can observe that most car makers are optimizing for benchmarks at the expensive of safety.</p>\n\n<h4 id=\"gender-weight-and-height\">Gender, weight, and height</h4>\n\n<p>Another issue is crash test dummy overfitting. For a long time, adult NHSTA and IIHS tests used a 1970s 50%-ile male dummy, which is 5'9&quot; and 171lbs. Regulators called for a female dummy in 1980 but due to budget cutbacks during the Reagan era, initial plans were shelved and the NHSTA didn't put one in a car until 2003. The female dummy is a scaled down version of the male dummy, scaled down to 5%-ile 1970s height and weight (4'11&quot;, 108lbs; another model is 4'11&quot;, 97lbs). In frontal crash tests, when a female dummy is used, it's always a passenger (a 5%-ile woman is in the driver's seat in one NHSTA side crash test and the IIHS side crash test). For reference, in 2019, the average weight of a U.S. adult male was 198 lbs and the average weight of a U.S. adult female was 171 lbs.</p>\n\n<p>Using a 1970s U.S. adult male crash test dummy causes a degree of overfitting for 1970s 50%-ile men. For example, starting in the 90s, manufacturers started adding systems to protect against whiplash. Volvo and Toyota use a kind of system that reduces whiplash in men and women and appears to have slightly more benefit for women. Most car makers use a kind of system that reduces whiplash in men but, on average, has little impact on whiplash injuries in women.</p>\n\n<p>It appears that we also see a similar kind of optimization for crashes in general and not just whiplash. We don't have crash test data on this, and looking at real-world safety data is beyond the scope of this post, but I'll note that, until around the time the NHSTA put the 5%-ile female dummy into some crash tests, most car manufacturers not named Volvo had a significant fatality rate differential in side crashes based on gender (with men dying at a lower rate and women dying at a higher rate).</p>\n\n<p>Volvo claims to have been using computer models to simulate what would happen if women (including pregnant women) are involved in a car accident for decades.</p>\n\n<h4 id=\"other-crashes\">Other crashes</h4>\n\n<p>Volvo is said to have a crash test facility where they do a number of other crash tests that aren't done by testing agencies. A reason that they scored well on the small overlap tests when they were added is that they were already doing small overlap crash tests before the IIHS started doing small overlap crash tests.</p>\n\n<p>Volvo also says that they test rollovers (the IIHS tests roof strength and the NHSTA computes how difficult a car is to roll based on properties of the car, but neither tests what happens in a real rollover accident), rear collisions (Volvo claims these are especially important to test if there are children in the 3rd row of a 3-row SUV), and driving off the road (Volvo has a &quot;standard&quot; ditch they use; they claim this test is important because running off the road is implicated in a large fraction of vehicle fatalities).</p>\n\n<p>If other car makers do similar tests, I couldn't find much out about the details. Based on crash test scores, it seems like they weren't doing or even considering small overlap crash tests before 2012. Based on how many car makers had poor scores when the passenger side small overlap test was added in 2018, I think it would be surprising if other car makers had a large suite of crash tests they ran that aren't being run by testing agencies, but it's theoretically possible that they do and just didn't include a passenger side small overlap test.</p>\n\n<h3 id=\"caveats\">Caveats</h3>\n\n<p>We shouldn't overgeneralize from these test results. As we noted above, crash test results test very specific conditions. As a result, what we can conclude when a couple new crash tests are added is also very specific. Additionally, there are a number of other things we should keep in mind when interpreting these results.</p>\n\n<h5 id=\"limited-sample-size\">Limited sample size</h5>\n\n<p>One limitation of this data is that we don't have results for a large number of copies of the same model, so we're unable to observe intra-model variation, which could occur due to minor, effectively random, differences in test conditions as well as manufacturing variations between different copies of same model. We can observe that these do matter since some cars will see different results when two copies of the same model are tested. For example, here's a quote from the IIHS report on the Dodge Dart:</p>\n\n<blockquote>\n<p>The Dodge Dart was introduced in the 2013 model year. Two tests of the Dart were conducted because electrical power to the onboard (car interior) cameras was interrupted during the first test. In the second Dart test, the driver door opened when the hinges tore away from the door frame. In the first test, the hinges were severely damaged and the lower one tore away, but the door stayed shut. In each test, the Darts safety belt and front and side curtain airbags appeared to adequately protect the dummys head and upper body, and measures from the dummy showed little risk of head and chest injuries.</p>\n</blockquote>\n\n<p>It looks like, had electrical power to the interior car cameras not been disconnected, there would have been only one test and it wouldn't have become known that there's a risk of the door coming off due to the hinges tearing away. In general, we have no direct information on what would happen if another copy of the same model were tested.</p>\n\n<p>Using IIHS data alone, one thing we might do here is to also consider results from different models made by the same manufacturer (or built on the same platform). Although this isn't as good as having multiple tests for the same model, test results between different models from the same manufacturer are correlated and knowing that, for example, a 2nd test of a model that happened by chance showed significantly worse results should probably reduce our confidence in other test scores from the same manufacturer. There are some things that complicate this, e.g., if looking at Toyota, the Yaris is actually a re-branded Mazda2, so perhaps that shouldn't be considered as part of a pooled test result, and doing this kind of statistical analysis is beyond the scope of this post.</p>\n\n<h4 id=\"actual-vehicle-tested-may-be-different\">Actual vehicle tested may be different</h4>\n\n<p>Although I don't think this should impact the results in this post, another issue to consider when looking at crash test results is how results are shared between models. As we just saw, different copies of the same model can have different results. Vehicles that are somewhat similar are often considered the same for crash test purposes and will share the same score (only one of the models will be tested).</p>\n\n<p>For example, this is true of the Kia Stinger and the Genesis G70. The Kia Stinger is 6&quot; longer than the G70 and a fully loaded AWD Stinger is about 500 lbs heavier than a base-model G70. The G70 is the model that IIHS tested -- if you look up a Kia Stinger, you'll get scores for a Stinger with a note that a base model G70 was tested. That's a pretty big difference considering that cars that are nominally identical (such as the Dodge Darts mentioned above) can get different scores.</p>\n\n<h4 id=\"quality-may-change-over-time\">Quality may change over time</h4>\n\n<p>We should also be careful not to overgeneralize temporally. If we look at crash test scores of recent Volvos (vehicles on the Volvo P3 and Volvo SPA platforms), crash test scores are outstanding. However, if we look at Volvo models based on the older Ford C1 platform<sup class=\"footnote-ref\" id=\"fnref:F\"><a rel=\"footnote\" href=\"#fn:F\">1</a></sup>, crash test scores for some of these aren't as good (in particular, while the S40 doesn't score poorly, it scores Acceptable in some categories instead of Good across the board). Although Volvo has had stellar crash test scores recently, this doesn't mean that they have always had or will always have stellar crash test scores.</p>\n\n<h4 id=\"models-may-vary-across-markets\">Models may vary across markets</h4>\n\n<p>We also can't generalize across cars sold in different markets, even for vehicles that sound like they might be identical. For example, see <a href=\"https://www.youtube.com/watch?v=UL_2MdSTM7g\">this crash test of a Nissan NP300 manufactured for sale in Europe vs. a Nissan NP300 manufactured for sale in Africa</a>. Since European cars undergo EuroNCAP testing (similar to how U.S. cars undergo NHSTA and IIHS testing), vehicles sold in Europe are optimized to score well on EuroNCAP tests. Crash testing cars sold in Africa has only been done relatively recently, so car manufacturers haven't had PR pressure to optimize their cars for benchmarks and they'll produce cheaper models or cheaper variants of what superficially appear to be the same model. This appears to be no different from what most car manufacturers do in the U.S. or Europe -- they're optimizing for cost as long as they can do that without scoring poorly on benchmarks. It's just that, since there wasn't an African crash test benchmark, that meant they could go all-in on the cost side of the cost-safety tradeoff<sup class=\"footnote-ref\" id=\"fnref:A\"><a rel=\"footnote\" href=\"#fn:A\">2</a></sup>.</p>\n\n<p><a href=\"https://deepblue.lib.umich.edu/bitstream/handle/2027.42/112977/103199.pdf\">This report</a> compared U.S. and European car models and found differences in safety due to differences in regulations. They found that European models had lower injury risk in frontal/side crashes and that driver-side mirrors were designed in a way that reduced the risk of lane-change crashes relative to U.S. designs and that U.S. vehicles were safer in rollovers and had headlamps that made pedestrians more visible.</p>\n\n<h4 id=\"non-crash-tests\">Non-crash tests</h4>\n\n<p>Over time, more and more of the &quot;low hanging fruit&quot; from crash safety has been picked, making crash avoidance relatively more important. Tests of crash mitigation are relatively primitive compared to crash tests and we've seen that crash tests had and have major holes. One might expect, based on what we've seen with crash tests, that Volvo has a particularly good set of tests they use for their crash avoidance technology (traction control, stability control, automatic braking, etc.), but I don't know of any direct evidence for that.</p>\n\n<p>Crash avoidance becoming more important might also favor companies that have safer driver assistance systems, e.g., in multiple generations of tests, Consumer Reports has given GM's Super Cruise system the highest rating while they've repeatedly noted that Tesla's Autopilot system facilitates unsafe behavior.</p>\n\n<h4 id=\"scores-of-vehicles-of-different-weights-aren-t-comparable\">Scores of vehicles of different weights aren't comparable</h4>\n\n<p>A 2700lb subcompact vehicle that scores Good may fare worse than a 5000lb SUV that scores Acceptable. This is because the small overlap tests involve driving the vehicle into a fixed obstacle, as opposed to a reference vehicle or vehicle-like obstacle of a specific weight. This is, in some sense, equivalent to crashing the vehicle into a vehicle of the same weight, so it's as if the 2700lb subcompact was tested by running it into a 2700lb subcompact and the 5000lb SUV was tested by running it into another 5000 lb SUV.</p>\n\n<h4 id=\"how-to-increase-confidence\">How to increase confidence</h4>\n\n<p>We've discussed some reasons we should reduce our confidence in crash test scores. If we wanted to increase our confidence in results, we could look at test results from other test agencies and aggregate them and also look at public crash fatality data (more on this later). I haven't looked at the terms and conditions of scores from other agencies, but one complication is that the IIHS does not allow you to display the result of any kind of aggregation if you use their API or data dumps (I, time consumingly, did not use their API for this post because of that).</p>\n\n<h4 id=\"using-real-life-crash-data\">Using real life crash data</h4>\n\n<p>Public crash fatality data is complex and deserves its own post. In this post, I'll note that, if you look at the easiest relevant data for people in the U.S., this data does not show that Volvos are particularly safe (or unsafe). For example, if we look at <a href=\"https://www.iihs.org/api/datastoredocument/status-report/pdf/52/3\">this report from 2017, which covers models from 2014</a>, two Volvo models made it into the report and both score roughly middle of the pack for their class. In the previous report, one Volvo model is included and it's among the best in its class, in the next, one Volvo model is included and it's among the worst in its class. We can observe this kind of variance for other models, as well. For example, among 2014 models, the Volkswagen Golf had one of the highest fatality rates for all vehicles (not just in its class). But among 2017 vehicles, it had among the lowest fatality rates for all vehicles. It's unclear how much of that change is from random variation and how much is because of differences between a 2014 and 2017 Volkswagen Golf.</p>\n\n<p>Overall, it seems like noise is a pretty important factor in results. And if we look at the information that's provided, we can see a few things that are odd. First, there are a number of vehicles where the 95% confidence interval for the fatality rate runs from 0 to N. We should have pretty strong priors that there was no 2014 model vehicle that was so safe that the probability of being killed in a car accident was zero. If we were taking a Bayesian approach (though I believe the authors of the report are not), and someone told us that the uncertainty interval for the true fatality rate of a vehicle had a &gt;= 5% of including zero, we would say that either we should use a more informative prior or we should use a model that can incorporate more data (in this case, perhaps we could try to understand the variance between fatality rates of different models in the same class and then use the base rate of fatalities for the class as a prior, or we could incorporate information from other models under the same make if those are believed to be correlated).</p>\n\n<p>Some people object to using informative priors as a form of bias laundering, but we should note that the prior that's used for the IIHS analysis is not completely uninformative. All of the intervals reported stop at zero because they're using the fact that a vehicle cannot create life to bound the interval at zero. But we have information that's nearly as strong that no 2014 vehicle is so safe that the expected fatality rate is zero, using that information is not fundamentally different from capping the interval at zero and not reporting negative numbers for the uncertainty interval of the fatality rate.</p>\n\n<p>Also, the IIHS data only includes driver fatalities. This is understandable since that's the easiest way to normalize for the number of passengers in the car, but it means that we can't possibly see the impact of car makers not improving passenger small-overlap safety until the passenger-side small overlap test was added in 2018, the result of lack of rear crash testing for the case Volvo considers important (kids in the back row of a 3rd row SUV). This also means that we cannot observe the impact of a number of things Volvo has done, e.g., being very early on pedestrian and then cyclist detection in their automatic braking system, adding a crumple zone to reduce back injuries in run-off-road accidients, which they observed often cause life-changing spinal injuries due to the impact from vehicles drop, etc.</p>\n\n<p>We can also observe that, in the IIHS analysis, many factors that one might want to control for aren't (e.g., miles driven isn't controlled for, which will make trucks look relatively worse and luxury vehicles look relatively better, rural vs. urban miles driven also isn't controlled for, which will also have the same directional impact). One way to see that the numbers are heavily influenced by confounding factors is by looking at AWD or 4WD vs. 2WD versions of cars. They often have wildly different fatalty rates even though the safety differences are not very large (and the difference is often in favor of the 2WD vehicle). Some plausible causes of that are random noise, differences in who buys different versions of the same vehicle, and differences in how the vehicle are used.</p>\n\n<p>If we'd like to answer the question &quot;which car makes or models are more or less safe&quot;, I don't find any of the aggregations that are publicly available to be satisfying and I think we need to look at the source data and do our own analysis to see if the data are consistent with what we see in crash test results.</p>\n\n<h3 id=\"conclusion\">Conclusion</h3>\n\n<p>We looked at 12 different car makes and how they fared when the IIHS added small overlap tests. We saw that only Volvo was taking this kind of accident seriously before companies were publicly shamed for having poor small overlap safety by the IIHS even though small overlap crashes were known to be a significant source of fatalities at least since the 90s.</p>\n\n<p>Although I don't have the budget to do other tests, such as a rear crash test in a fully occupied vehicle, it appears plausible and perhaps even likely that most car makers that aren't Volvo would have mediocre or poor test scores if a testing agency decided to add another kind of crash test.</p>\n\n<h3 id=\"bonus-real-engineering-vs-programming\">Bonus: &quot;real engineering&quot; vs. programming</h3>\n\n<p>As Hillel Wayne has noted, although <a href=\"https://twitter.com/danluu/status/1162469763374673920\">programmers often have an idealized view of what &quot;real engineers&quot; do</a>, when you <a href=\"https://youtu.be/3018ABlET1Y\">compare what &quot;real engineers&quot; do with what programmers do, it's frequently not all that different</a>. In particular, a common lament of programmers is that we're not held liable for our mistakes or poor designs, even in cases where that costs lives.</p>\n\n<p>Although automotive companies can, in some cases, be held liable for unsafe designs, just optimizing for a small set of benchmarks, which must've resulted in extra deaths over optimizing for safety instead of benchmark scores, isn't something that engineers or corporations were, in general, held liable for.</p>\n\n<h3 id=\"bonus-reputation\">Bonus: reputation</h3>\n\n<p>If I look at what people in my extended social circles think about vehicle safety, Tesla has the best reputation by far. If you look at broad-based consumer polls, that's a different story, and Volvo usually wins there, with other manufacturers fighting for a distant second.</p>\n\n<p>I find the Tesla thing interesting since their responses are basically the opposite of what you'd expect from a company that was serious about safety. When serious problems have occurred (with respect to safety or otherwise), they often have a very quick response that's basically &quot;everything is fine&quot;. I would expect an organization that's serious about safety or improvement to respond with &quot;we're investigating&quot;, followed by a detailed postmortem explaining what went wrong, but that doesn't appear to be Tesla's style.</p>\n\n<p>For example, on the driver-side small overlap test, Tesla had one model with a relevant score and it scored Acceptable (below Good, but above Poor and Marginal) even after modifications were made to improve the score. <a href=\"https://www.businessinsider.com/tesla-responds-to-model-s-crash-test-findings-iihs-2017-7\">Tesla disputed the results, saying they make &quot;the safest cars in history&quot;</a> and implying that IIHS should be ignored in favor of <a href=\"https://en.wikipedia.org/wiki/National_Highway_Traffic_Safety_Administration\">NHSTA</a> test scores:</p>\n\n<blockquote>\n<p>While IIHS and dozens of other private industry groups around the world have methods and motivations that suit their own subjective purposes, the most objective and accurate independent testing of vehicle safety is currently done by the U.S. Government which found Model S and Model X to be the two cars with the lowest probability of injury of any cars that it has ever tested, making them the safest cars in history.</p>\n</blockquote>\n\n<p>As we've seen, Tesla isn't unusual for optimizing for a specific set of crash tests and achieving a mediocre score when an unexpected type of crash occurs, but their response is unusual. However, it makes sense from a cynical PR perspective. As we've seen over the past few years, loudly proclaiming something, regardless of whether or not it's true, even when there's incontrovertible evidence that it's untrue, seems to not only work, that kind of bombastic rhetoric appears to attract superfans who will aggressively defend the brand. If you watch car reviewers on youtube, they'll sometimes mention that they get hate mail for reviewing Teslas just like they review any other car and that they don't see anything like it for any other make.</p>\n\n<p>Apple also used this playbook to good effect in the 90s and early '00s, when they were rapidly falling behind in performance and responded not by improving performance, but by running a series of ad campaigns saying that had the best performance in the world and that they were shipping &quot;supercomputers&quot; on the desktop.</p>\n\n<p>Another reputational quirk is that I know a decent number of people who believe that the safest cars they can buy are &quot;American Cars from the 60's and 70's that aren't made of plastic&quot;. We don't have directly relevant small overlap crash test scores for old cars, but the test data we do have on old cars indicates that they fare extremely poorly in overall safety compared to modern cars. For a visually dramatic example, <a href=\"https://www.youtube.com/watch?v=fPF4fBGNK0U\">see this crash test of a 1959 Chevrolet Bel Air vs. a 2009 Chevrolet Malibu</a>.</p>\n\n<h3 id=\"appendix-methodology-summary\">Appendix: methodology summary</h3>\n\n<p>The top-line results section uses scores for the small overlap test both because it's the one where I think it's the most difficult to justify skimping on safety as measured by the test and it's also been around for long enough that we can see the impact of modifications to existing models and changes to subsequent models, which isn't true of the passenger side small overlap test (where many models are still untested).</p>\n\n<p>For the passenger side small overlap test, someone might argue that the driver side is more important because you virtually always have a driver in a car accident and may or may not have a front passenger. Also, for small overlap collisions (which simulates a head-to-head collision where the vehicles only overlap by 25%), driver's side collisions are more likely than passenger side collisions.</p>\n\n<p>Except to check Volvo's scores, I didn't look at roof crash test scores (which were added in 2009). I'm not going to describe the roof test in detail, but for the roof test, someone might argue that the roof test score should be used in conjunction with scoring the car for rollover probability since the roof test just tests roof strength, which is only relevant when a car has rolled over. I think, given what the data show, this objection doesn't hold in many cases (the vehicles with the worst roof test scores are often vehicles that have relatively high rollover rates), but it does in some cases, which would complicate the analysis.</p>\n\n<p>In most cases, we only get one reported test result for a model. However, there can be multiple versions of a model -- including before and after making safety changes intended to improve the test score. If changes were made to the model to improve safety, the test score is usually from after the changes were made and we usually don't get to see the score from before the model was changed. However, there are many exceptions to this, which are noted in the detailed results section.</p>\n\n<p>For this post, scores only count if the model was introduced before or near when the new test was introduced, since models introduced later could have design changes that optimize for the test.</p>\n\n<h3 id=\"appendix-detailed-results\">Appendix: detailed results</h3>\n\n<p>On each test, IIHS gives an overall rating (from worst to best) of Poor, Marginal, Acceptable, or Good. The tests have sub-scores, but we're not going to use those for this analysis. In each sub-section, we'll look at how many models got each score when the small overlap tests were added.</p>\n\n<h4 id=\"volvo\">Volvo</h4>\n\n<p>All Volvo models examined scored Good (the highest possible score) on the new tests when they were added (roof, driver-side small overlap, and passenger-side small overlap). One model, the 2008-2017 XC60, had a change made to trigger its side curtain airbag during a small overlap collision in 2013. Other models were tested without modifications.</p>\n\n<h4 id=\"mercedes\">Mercedes</h4>\n\n<p>Of three pre-existing models with test results for driver-side small overlap, one scored Marginal without modifications and two scored Good after structural modifications. The model where we only have unmodified test scores (Mercedes C-Class) was fully re-designed after 2014, shortly after the driver-side small overlap test was introduced.</p>\n\n<p>As mentioned above, we often only get to see public results for models without modifications to improve results xor with modifications to improve results, so, for the models that scored Good, we don't actually know how they would've scored if you bought a vehicle before Mercedes updated the design, but the Marginal score from the one unmodified model we have is a negative signal.</p>\n\n<p>Also, when the passenger side small overlap test was added, the Mercedes vehicles also generally scored Good. This is, indicating that Mercedes didn't only increase protection on the driver's side in order to improve test scores.</p>\n\n<h4 id=\"bmw\">BMW</h4>\n\n<p>Of the two models where we have relevant test scores, both scored Marginal before modifications. In one of the cases, there's also a score after structural changes were made in the 2017 model (recall that the driver-side small overlap test was introduced in 2012) and the model scored Good afterwards. The other model was fully-redesigned after 2016.</p>\n\n<p>For the five models where we have relevant passenger-side small overlap scores, all scored Good, indicating that the changes made to improve driver-side small overlap test scores weren't only made on the driver's side.</p>\n\n<h4 id=\"honda\">Honda</h4>\n\n<p>Of the five Honda models where we have relevant driver-side small overlap test scores, two scored Good, one scored Marginal, and two scored Poor. The model that scored Marginal had structural changes plus a seatbelt change in 2015 that changed its score to Good, other models weren't updated or don't have updated IIHS scores.</p>\n\n<p>Of the six Honda models where we have passenger driver-side small overlap test scores, two scored Good without modifications, two scored Acceptable without modifications, and one scored Good with modifications to the bumper.</p>\n\n<p>All of those models scored Good on the driver side small overlap test, indicating that when Honda increased the safety on the driver's side to score Good on the driver's side test, they didn't apply the same changes to the passenger side.</p>\n\n<h4 id=\"toyota\">Toyota</h4>\n\n<p>Of the six Toyota models where we have relevant driver-side small overlap test scores for unmodified models, one score Acceptable, four scored Marginal, and one scored Poor.</p>\n\n<p>The model that scored Acceptable had structural changes made to improve its score to Good, but on the driver's side only. The model was later tested in the passenger-side small overlap test and scored Acceptable. Of the four models that scored Marginal, one had structural modifications made in 2017 that improved its score to Good and another had airbag and seatbelt changes that improved its score to to Acceptable. The vehicle that scored Poor had structural changes made that improved its score to acceptable in 2014, followed by later changes that improved its score to Good.</p>\n\n<p>There are four additional models where we only have scores from after modifications were made. Of those, one scored Good, one score Acceptable, one scored Marginal, and one scored Poor.</p>\n\n<p>In general, changes appear to have been made to the driver's side only and, on introduction of the passenger side small overlap test, vehicles had passenger side small overlap scores that were the same as the driver's side score before modifications.</p>\n\n<h4 id=\"ford\">Ford</h4>\n\n<p>Of the two models with relevant driver-side small overlap test scores for unmodified models, one scored Marginal and one scored Poor. Both of those models were produced into 2019 and neither has an updated test result. Of the three models where we have relevant results for modified vehicles, two scored Acceptable and one score Marginal. Also, one model was released the year the small overlap test was introduced and one the year after; both of those scored Acceptable. It's unclear if those should be considered modified or not since the design may have had last-minute changes before release.</p>\n\n<p>We only have three relevant passenger-side small overlap tests. One is Good (for a model released in 2015) and the other two are Poor; these are the two models mentioned above as having scored Marginal and Poor, respectively, on the driver-side small overlap test. It appears that the models continued to be produced into 2019 without safety changes. Both of these unmodified models were trucks and this isn't very unusual for a truck and is one of a number of reasons that fatality rates are generally higher in trucks -- until recently, many of them are based on old platforms that hadn't been updated for a long time.</p>\n\n<h4 id=\"chevrolet\">Chevrolet</h4>\n\n<p>Of the three Chevrolet models where we have relevant driver-side small overlap test scores before modifications, one scored Acceptable and two scored Marginal. One of the Marginal models had structural changes plus a change that caused side curtain airbags to deploy sooner in 2015, which improved its score to Good.</p>\n\n<p>Of the four Chevrolet models where we only have relevant driver-side small overlap test scores after the model was modified (all had structural modifications), two scored Good and two scored Acceptable.</p>\n\n<p>We only have one relevant score for the passenger-side small overlap test, that score is Marginal. That's on the model that was modified to improve its driver-side small overlap test score from Marginal to Good, indicating that the changes were made to improve the driver-side test score and not to improve passenger safety.</p>\n\n<h4 id=\"subaru\">Subaru</h4>\n\n<p>We don't have any models where we have relevant passenger-side small overlap test scores for models before they were modified.</p>\n\n<p>One model had a change to cause its airbag to deploy during small overlap tests; it scored Acceptable. Two models had some kind of structural changes, one of which scored Good and one of which score Acceptable.</p>\n\n<p>The model that had airbag changes had structural changes made in 2015 that improved its score from Acceptable to Good.</p>\n\n<p>For the one model where we have relevant passenger-side small overlap test scores, the score was Marginal. Also, for one of the models with structural changes, it was indicated that, among the changes, were changes to the left part of the firewall, indicating that changes were made to improve the driver's side test score without improving safety for a passenger on a passenger-side small overlap crash.</p>\n\n<h4 id=\"tesla\">Tesla</h4>\n\n<p>There's only one model with relevant results for the driver-side small overlap test. That model scored Acceptable before and after modifications were made to improve test scores.</p>\n\n<h4 id=\"hyundai\">Hyundai</h4>\n\n<p>Of the five vehicles where we have relevant driver-side small overlap test scores, one scored Acceptable, three scored Marginal, and one scored Poor. We don't have any indication that models were modified to improve their test scores.</p>\n\n<p>Of the two vehicles where we have relevant passenger-side small overlap test scores for unmodified models, one scored Good and one scored Acceptable.</p>\n\n<p>We also have one score for a model that had structural modifications to score Acceptable, which later had further modifications that allowed it to score Good. That model was introduced in 2017 and had a Good score on the driver-side small overlap test without modifications, indicating that it was designed to achieve a good test score on the driver's side test without similar consideration for a passenger-side impact.</p>\n\n<h4 id=\"dodge\">Dodge</h4>\n\n<p>Of the five models where we have relevant driver-side small overlap test scores for unmodified models, two scored Acceptable, one scored Marginal, and two scored Poor. There are also two models where we have test scores after structural changes were made for safety in 2015; both of those models scored Marginal.</p>\n\n<p>We don't have relevant passenger-side small overlap test scores for any model, but even if we did, the dismal scores on the modified models means that we might not be able to tell if similar changes were made to the passenger side.</p>\n\n<h4 id=\"nissan\">Nissan</h4>\n\n<p>Of the seven models where we have relevant driver-side small overlap test scores for unmodified models, two scored Acceptable and five scored Poor.</p>\n\n<p>We have one model that only has test scores for a modified model; the frontal airbags and seatbelts were modified in 2013 and the side curtain airbags were modified in 2017. The score afterward modifications was Marginal.</p>\n\n<p>One of the models that scored Poor had structural changes made in 2015 that improved its score to Good.</p>\n\n<p>Of the four models where we have relevant passenger-side small overlap test scores, two scored Good, one scored Acceptable (that model scored good on the driver-side test), and one score Marginal (that model also scored Marginal on the driver-side test).</p>\n\n<h4 id=\"jeep\">Jeep</h4>\n\n<p>Of the two models where we have relevant driver-side small overlap test scores for unmodified models, one scored Marginal and one scored Poor.</p>\n\n<p>There's one model where we only have test score after modifications; that model has changes to its airbags and seatbelts and it scored Marginal after the changes. This model was also later tested on the passenger-side small overlap test and scored Poor.</p>\n\n<p>One other model has a relevant passenger-side small overlap test score; it scored Good.</p>\n\n<h4 id=\"volkswagen\">Volkswagen</h4>\n\n<p>The two models where we have relevant driver-side small overlap test scores for unmodified models both scored Marginal.</p>\n\n<p>Of the two models where we only have scores after modifications, one was modified 2013 and scored Marginal after modifications. It was then modified again in 2015 and scored Good after modifications. That model was later tested on the passenger side small-overlap test, where it scored Acceptable, indicating that the modifications differentially favored the driver's side. The other scored Acceptable after changes made in 2015 and then scored Good after further changes made in 2016. The 2016 model was later tested on the passenger-side small overlap test and scored Marginal, once again indicating that changes differentially favored the driver's side.</p>\n\n<p>We have passenger-side small overlap test for two other models, both of which scored Acceptable. These were models introduced in 2015 (well after the introduction of the driver-side small overlap test) and scored Good on the driver-side small overlap test.</p>\n\n<h3 id=\"2021-update\">2021 update</h3>\n\n<p><a href=\"https://www.iihs.org/news/detail/small-suvs-struggle-in-new-tougher-side-test\">The IIHS has released the first set of results for their new &quot;upgraded&quot; side-impact tests</a>. They've been making noises about doing this for quite and have mentioned that in real-world data on (some) bad crashes, they've observed intrusion into the cabin that's significantly greater than is seen on their tests. They've mentioned that some vehicles do relatively well on on the new tests and some less well but haven't released official scores until now.</p>\n\n<p>The results in the new side-impact tests are different from the results described in the posts above. So far, only small SUVs have had their results released and only the Mazda CX-5 has a result of &quot;Good&quot;. Of the three manufacturers that did well on the tests describe in this post, only Volvo has public results and they scored &quot;Acceptable&quot;. Some questions I have are:</p>\n\n<ul>\n<li>Will Volvo score better for their other vehicles (most of their vehicles are built on a different platform from the vehicle that has public results)?</li>\n<li>Will Volvo quickly update their vehicles to achieve the highest score on the test? Unlike a lot of other manufacturers, we don't have recent data from Volvo on how they responded to something like this because they didn't need to update their vehicles to achieve the highest score on the last two new tests</li>\n<li>Will BMW and Mercedes either score well and the new test or quickly update their vehicles to score well once again?</li>\n<li>Will other Mazda vehicles also score well without updates?</li>\n</ul>\n\n<h3 id=\"appendix-miscellania\">Appendix: miscellania</h3>\n\n<p>A number of name brand car makes weren't included. Some because they have relatively low sales in the U.S. are low and/or declining rapidly (Mitsubishi, Fiat, Alfa Romeo, etc.), some because there's very high overlap in what vehicles are tested (Kia, Mazda, Audi), and some because there aren't relevant models with driver-side small overlap test scores (Lexus). When a corporation owns an umbrella of makes, like FCA with Jeep, Dodge, Chrysler, Ram, etc., these weren't pooled since most people who aren't car nerds aren't going to recognize FCA, but may recognize Jeep, Dodge, and Chrysler.</p>\n\n<p>If the terms of service of the API allowed you to use IIHS data however you wanted, I would've included smaller makes, but since the API comes with very restrictive terms on how you can display or discuss the data which aren't compatible with exploratory data analysis and I couldn't know how I would want to display or discuss the data before looking at the data, I pulled all of these results by hand (and didn't click through any EULAs, etc.), which was fairly time consuming, so there was a trade-off between more comprehensive coverage and the rest of my life.</p>\n\n<h3 id=\"appendix-what-car-should-i-buy\">Appendix: what car should I buy?</h3>\n\n<p>That depends on what you're looking for, there's no way to make a blanket recommendation. For practical information about particular vehicles, <a href=\"https://www.youtube.com/user/TTACVideo\">Alex on Autos</a> is the best source that I know of. I don't generally like videos as a source of practical information, but car magazines tend to be much less informative than youtube car reviewers. There are car reviewers that are much more popular, but their popularity appears to come from having witty banter between charismatic co-hosts or other things that not only aren't directly related to providing information, they actually detract from providing information. If you just want to know about how cars work, <a href=\"https://www.youtube.com/user/EngineeringExplained\">Engineering Explained</a> is also quite good, but the information there is generally practical.</p>\n\n<p>For reliability information, Consumer Reports is probably your best bet (you can also look at J.D. Power, but the way they aggregate information makes it much less useful to consumers).</p>\n\n<p><small>Thanks to Leah Hanson, Travis Downs, Prabin Paudel, Jeshua Smith, and Justin Blank for comments/corrections/discussion</small></p>\n\n<p><link rel=\"prefetch\" href=\"https://danluu.com/\">\n<link rel=\"prefetch\" href=\"https://danluu.com/about/\"></p>\n<div class=\"footnotes\">\n\n<hr />\n\n<ol>\n<li id=\"fn:F\">this includes the 2004-2012 Volvo S40/V50, 2006-2013 Volvo C70, and 2007-2013 Volvo C30, which were designed during the period when Ford owned Volvo. Although the C1 platform was a joint venture between Ford, Volvo, and Mazda engineers, the work was done under a Ford VP at a Ford facility.\n <a class=\"footnote-return\" href=\"#fnref:F\"><sup>[return]</sup></a></li>\n\n<li id=\"fn:A\"><p>to be fair, as we saw with the IIHS small overlap tests, not every manufacturer did terribly. In 2017 and 2018, 8 vehicles sold in Africa were crash tested. One got what we would consider a mediocre to bad score in the U.S. or Europe, five got what we would consider to be a bad score, and &quot;only&quot; three got what we would consider to be an atrocious score. The Nissan NP300, Datsun Go, and Cherry QQ3 were the three vehicles that scored the worst. Datsun is a sub-brand of Nissan and Cherry is a Chinese brand, also known as Qirui.</p>\n\n<p>We see the same thing if we look at cars sold in India. Recently, some tests have been run on cars sent to the Indian market and a number of vehicles from Datsun, Renault, Chevrolet, Tata, Honda, Hyundai, Suzuki, Mahindra, and Volkswagen came in with atrocious scores that would be considered impossibly bad in the U.S. or Europe.</p>\n <a class=\"footnote-return\" href=\"#fnref:A\"><sup>[return]</sup></a></li>\n</ol>\n</div>\n"]}]} {:tag :item, :attrs nil, :content [{:tag :title, :attrs nil, :content ["Finding the Story"]} {:tag :link, :attrs nil, :content ["https://danluu.com/voyager-story/"]} {:tag :pubDate, :attrs nil, :content ["Tue, 02 Jun 2020 00:05:34 -0700"]} {:tag :guid, :attrs nil, :content ["https://danluu.com/voyager-story/"]} {:tag :description, :attrs nil, :content ["<p><em>This is an archive of an old pseudonymously written post from the 90s from someone whose former pseudonym seems to have disappeared from the internet.</em></p>\n\n<p>I see that <cite>Star Trek: Voyager</cite> has added a new character, a Borg.\n(From the photos, I also see that they're still breeding women for breast size\nin the 24th century.) What ticked me off was the producer's comment (I'm\nparaphrasing), &quot;The addition of Seven of Nine will give us limitless\nstory possibilities.&quot;</p>\n\n<p>Uh-huh. Riiiiiight.</p>\n\n<p>Look, they did't recognize the stories they <i>had</i>.\nI watched the first few episodes of <cite>Voyager</cite>\nand quit when my bullshit meter when off the scale.\n(Maybe that's not fair, to judge them by only a few episodes.\nBut it's not fair to subject me to crap like the holographic lungs, either.)</p>\n\n<p>For those of you who don't watch <cite>Star Trek: Voyager</cite>,\nthe premise is that the <i>Voyager</i>, sort of a space corvette,\ngets transported umpteen zillions of light years from where it should be.\nIt will take over seventy years at top speed for them to get home to their\nloved ones.\nFor reasons we needn't go into here,\nthe crew consists of a mix of loyal Federation members and rebels.</p>\n\n<p>On paper, this looks good. There's an uneasy alliance in the crew,\nthere's exploration as they try to get home, there's the whole &quot;island\nin space&quot; routine. And the <cite>Voyager</cite> is nowhere near as big as the\nEnterprise -- it's not mentally healthy for people to stay aboard for\nthat long.</p>\n\n<p>But can this idea actually sustain a whole series?  Would it be\ninteresting to watch five years of &quot;the crew bickers&quot; or &quot;they find a\nnew clue to faster interstellar travel but it falls through&quot;?  I don't\nthink so.</p>\n\n<p>(And, in fact, the crew settled down <i>awfully</i> quickly.)</p>\n\n<p>The demands of series television subvert the premise.  The basic demand\nof series television is that our regular characters are people we come\nto know and to care about -- we want them to come into our living rooms\nevery week.  We must care about their changes, their needs, their\ndesires.  We must worry when they're put in jeopardy.  But we know it's a\nseries, so it's hard to make us worry.  We know that the characters will\nbe back next week.</p>\n\n<p>The demands of a <i>story</i> require someone to <i>change</i> of their own\naccord, to recognize some difference.  The need to change can be imposed\nfrom without, but the actual change must be self-motivated. (This is the\nfundamental paradox of series television: the only character allowed to\nchange is a guest, but the instrument of that change has to be a series\nregular, therefore depriving both characters of the chance to do\nsomething interesting.)</p>\n\n<p>Series with strict continuity of episodes (episode 2 must follow episode\n1) allow change -- but they're harder to sell in syndication after the\nshow goes off the air. Economics favour unchanging regular characters.</p>\n\n<p>Some series -- such as Hill Street Blues -- get around the jeopardy\nproblem by actually making characters disposable.  Some characters show\nup for a few episodes and then die, reminding us that it could happen to\nthe regulars, too.  Sometimes it does happen to the regulars.</p>\n\n<p>(When the characters change in the pilot, there may be a problem.  A\nwriter who was approached to work on Mary Tyler Moore's last series saw\nfrom the premise that it would be brilliant for six episodes and then\nhad noplace to go.  The first Fox series starring Tea Leoni,\n<cite>Flying Blind</cite>, had a very funny pilot and set up an untenable\nsituation.)</p>\n\n<p>I'm told the only interesting character on <cite>Voyager</cite>\nhas been the doctor, who can change.\nHe's the only character allowed to grow.</p>\n\n<p>The first problem with Voyager, then, is that characters aren't allowed\nto change -- or the change is imposed from outside.\n(By the way, an imposed change is a great way to <i>start</i> a story.\nThe character then\nfights it, and that's interesting. It's a terrible way to <i>end</i> a story.)</p>\n\n<p>The second problem is that they don't make use of the elements they\nhave. Let's go back to the first season. There was an episode in which\nthere's a traitor on board who is as smart as Janeway herself. (How\npsychiatric testing missed this, I don't know, but the Trek universe has\nnever had really good luck with psychiatry.) After leading Janeway by\nthe nose for fifty minutes, she figures out who it is, and\nconfronts him. He says yes -- <i>and beams off the ship</i>, having\nconveniently made a deal with the locals.</p>\n\n<p>Perfect for series television. We've got a supposedly intelligent villain\nout there who could come back and Janeway's been given a run for her money\n-- except that I felt cheated. Where's the story? Where's the resolution?</p>\n\n<p>Here's what I think they should have done. It's not traditional series\ntelevision, but I think it would have been better stories.</p>\n\n<p>First of all, the episode ends when Janeway confronts the bad guy and\narrests him. He's put in the brig -- <i>and stays there</i>. The viewer gets\nsome sense of victory here.</p>\n\n<p>But now there's someone as smart as Janeway in the brig. Suddenly we've\nset up <cite>Silence of the Lambs</cite>.\n(I don't mind stealing if I steal from good sources.)\nWhenever a problem is <i>big enough</i>,\nJaneway has this option: she can go to the brig and try and make a deal\nwith the bad guy. &quot;The ship dies, you die.&quot; Not only that, here's someone\non board ship with whom she has a unique relationship -- one not formally\nbounded by rank. What does the bad guy really want?</p>\n\n<p>And whenever Janeway's feeling low, he can taunt her.  &quot;By the way, I\nthought of a way to get everyone home in one-tenth the time.  Have you,\nCaptain?&quot;</p>\n\n<p>You wouldn't put him in every episode.  But any time you need that extra\npush, he's there.  Remember, we can have him escape any time we want,\nthrough the same sleight used in the original episode.</p>\n\n<p>Furthermore, it's one thing to catch him;\nit's another thing to keep him there.\nYou can generate another entire episode\nout of an escape attempt by the prisoner. But that would be an\nintermediate thing. Let's talk about the finish I would have liked to\nhave seen.</p>\n\n<p>Let's invent a crisis.  The balonium generator explodes; we're deep\nin warp space; our crack engineering crew has jury-rigged a repair to\nthe sensors and found a Class M planet that might do for the repairs.\nExcept it's just too far away.  The margin is tight -- but can't be\ndone.  There are two too many people on board ship.  Each requires a\ncertain amount of food, air, water, etc.  Under pressure, Neelix admits\nthat his people can go into suspended animation, so he does.  The doctor\ntries heroically but the engineer who was tending the balonium generator\ndies.  (Hmmm.  Power's low.  The doctor can only be revived at certain\ncritical moments.) Looks good -- but they were using air until they\ndied; one more crew member <i>must</i> die for the rest to live.</p>\n\n<p>And somebody remembers the guy in the brig. &quot;The question of his guilt,&quot;\nsays Tuvok, &quot;is resolved. The authority of the Captain is absolute. You\nare within your rights to hold a summary court martial and sentence him\nto death.&quot;</p>\n\n<p>And Janeway says no. &quot;The Federation doesn't do that.&quot;</p>\n\n<p>Except that everyone will die if she doesn't.  The pressure is on\nJaneway, now.  Janeway being Janeway, she's looking for a technological\nfix.  &quot;Find an answer, dammit!&quot; And the deadline is coming up.  After a\ncertain point, the prisoner has to die, along with someone else.</p>\n\n<p>A crewmember volunteers to die (a regular).  Before Janeway can accept,\nyet another (regular) crewmember volunteers, and Janeway is forced to\ndecide.  -- And Tuvok points out that while morally it's defensible if\nthat member volunteered to die, the ship cannot continue without either\nof those crewmembers.\nIt <i>can</i> continue without the prisoner.  Clearly the\nprisoner is not worth as much as those crewmembers, but she is the\ncaptain.  She <i>must</i> make this decision.</p>\n\n<p>Our fearless engineering crew thinks they might have a solution, but it\nwill use nearly everything they've got, and they need another six hours\nto work on the feasibility.  Someone in the crew tries to resolve the\nproblem for her by offing the prisoner -- the failure uses up more\nvaluable power.  Now the deadline moves up closer, past the six hours\ndeadline.  The engineering crew's idea is no longer feasible.</p>\n\n<p>For his part, the prisoner is now bargaining. He says he's got\nideas to help. Does he? He's tried to destroy the ship before. And he\nwon't reveal them until he gets a full pardon.</p>\n\n<p>(This is all basic plotting: keep piling on difficulties. Put a carrot in\nfront of the characters, keep jerking it away.)</p>\n\n<p>The tricky part is the ending.\nIt's a requirement that the ending derive logically\nfrom what has gone before.  If you're going to invoke a technological\nfix, you have to set the groundwork for it in the first half of the\nshow.  Otherwise it's technobabble.  It's deus ex machina.  (Any time\nsomeone says just after the last commercial break, &quot;Of course!  If we\nvorpalize the antibogon flow, we're okay!&quot; I want to smack a writer in\nthe head.)</p>\n\n<p>Given the situation set up here, we have three possible endings:</p>\n\n<ul>\n<li>Some member of the crew tries to solve the problem by sacrificing\nthemselves. (Remember, McCoy and Spock did this.) This is a weak\nsolution (unless Janeway does it) because it takes the focus off\nJaneway's decision.</li>\n<li>Janeway strikes a deal with the prisoner, and together they come up with\na solution (which doesn't involve the antibogon flow). This has the\ninteresting repercussions of granting the prisoner his freedom --\nwhile everyone else on ship hates his guts. Grist for another episode,\nanyway.</li>\n<li>Janeway kills the prisoner but refuses to hold the court martial.\nShe may luck out -- the prisoner might survive; that million-to-one-shot\nthey've been praying for but couldn't rely on comes through -- but she\nhas decided to kill the prisoner rather than her crew.</li>\n</ul>\n\n<p>My preferred ending is the third one,\neven though the prisoner need not die.\nThe decision we've set up is a difficult one,\nand it is meaningful. It is a command decision.\nWhether she ends up killing the prisoner is not relevant; what is\nrelevant is that she <i>decides</i> to do it.</p>\n\n<p>John Gallishaw once categorized all stories as either stories of\n<i>achievement</i> or of <i>decision</i>.\nA decision story is much harder to\nwrite, because both choices have to matter.</p>\n"]}]} {:tag :item, :attrs nil, :content [{:tag :title, :attrs nil, :content ["A simple way to get more value from tracing"]} {:tag :link, :attrs nil, :content ["https://danluu.com/tracing-analytics/"]} {:tag :pubDate, :attrs nil, :content ["Sun, 31 May 2020 00:06:34 -0700"]} {:tag :guid, :attrs nil, :content ["https://danluu.com/tracing-analytics/"]} {:tag :description, :attrs nil, :content ["\n\n<p>A lot of people seem to think that distributed tracing isn't useful, or at least not without extreme effort that isn't worth it for companies smaller than FB. For example, <a href=\"https://twitter.com/theatrus/status/1220205677672452097\">here are</a> <a href=\"https://twitter.com/mattklein123/status/1049813546077323264\">a couple of</a> public conversations that sound like a number of private conversations I've had. Sure, there's value somewhere, <a href=\"https://twitter.com/mattklein123/status/1220150248741326855\">but it costs too much to unlock</a>.</p>\n\n<p>I think this overestimates how much work it is to get a lot of value from tracing. At Twitter, Rebecca Isaacs was able to lay out a vision for how to get value from tracing and executed on it (with help from a number other folks, including Jonathan Simms, Yuri Vishnevsky, Ruben Oanta, Dave Rusek, Hamdi Allam, and many others<sup class=\"footnote-ref\" id=\"fnref:O\"><a rel=\"footnote\" href=\"#fn:O\">1</a></sup>) such that the work easily paid for itself. This post is going to describe the tracing &quot;infrastructure&quot; we've built and describe some use cases where we've found it to be valuable. Before we get to that, let's start with some background about the situation before Rebecca's vision came to fruition.</p>\n\n<p>At a high level, we could say that we had a trace-view oriented system and ran into all of the issues that one might expect from that. Those issues are discussed in more detail in <a href=\"https://medium.com/@copyconstruct/distributed-tracing-weve-been-doing-it-wrong-39fc92a857df\">this article by Cindy Sridharan</a>. However, I'd like to discuss the particular issues we had in more detail since I think it's useful to look at what specific things were causing problems.</p>\n\n<p>Taken together, the issues were problematic enough that tracing was underowned and arguably unowned for years. Some individuals did work in their spare time to keep the lights on or improve things, but the lack of obvious value from tracing led to a vicious cycle where the high barrier to getting value out of tracing made it hard to fund organizationally, which made it hard to make tracing more usable.</p>\n\n<p>Some of the issues that made tracing low ROI included:</p>\n\n<ul>\n<li>Schema made it impossible to run simple queries &quot;in place&quot;</li>\n<li>No real way to aggregate info\n\n<ul>\n<li>No way to find interesting or representative traces</li>\n</ul></li>\n<li>Impossible to know actual sampling rate, sampling highly non-representative</li>\n<li>Time</li>\n</ul>\n\n<h3 id=\"schema\">Schema</h3>\n\n<p>The schema was effectively a set of traces, where each trace was a set of spans and each span was a set of annotations. Each span that wasn't a root span had a pointer to its parent, so that the graph structure of a trace could be determined.</p>\n\n<p>For the purposes of this post, we can think of each trace as either an external request including all sub-RPCs or a subset of a request, rooted downstream instead of at the top of the request. We also trace some things that aren't requests, like builds and git operations, but for simplicity we're going to ignore those for this post even though the techniques we'll discuss also apply to those.</p>\n\n<p>Each span corresponds to an RPC and each annotation is data that a developer chose to record on a span (e.g., the size of the RPC payload, queue depth of various queues in the system at the time of the span, or GC pause time for GC pauses that interrupted the RPC).</p>\n\n<p>Some issues that came out of having a schema that was a set of sets (of bags) included:</p>\n\n<ul>\n<li>Executing any query that used information about the graph structure inherent in a trace required reading every span in the trace and reconstructing the graph</li>\n<li>Because there was no index or summary information of per-trace information, any query on a trace required reading every span in a trace</li>\n<li>Practically speaking, because the two items above are too expensive to do at query time in an ad hoc fashion, the only query people ran was some variant of &quot;give me a few spans matching a simple filter&quot;</li>\n</ul>\n\n<h3 id=\"aggregation\">Aggregation</h3>\n\n<p>Until about a year and a half ago, the only supported way to look at traces was to go to the UI, filter by a service name from a combination search box + dropdown, and then look at a list of recent traces, where you could click on any trace to get a &quot;trace view&quot;. Each search returned the N most recent results, which wouldn't necessarily be representative of all recent results (for reasons mentioned below in the Sampling section), let alone representative of all results over any other time span.</p>\n\n<p>Per the problems discussed above in the schema section, since it was too expensive to run queries across a non-trivial number of traces, it was impossible to ask questions like &quot;are any of the traces I'm looking at representative of common traces or am I looking at weird edge cases?&quot; or &quot;show me traces of specific tail events, e.g., when a request from service A to service B times out or when write amplification from service A to some backing database is &gt; 3x&quot;, or even &quot;only show me complete traces, i.e., traces where we haven't dropped spans from the trace&quot;.</p>\n\n<p>Also, if you clicked on a trace that was &quot;too large&quot;, the query would time out and you wouldn't be able to view the trace -- this was another common side effect of the lack of any kind of rate limiting logic plus the schema.</p>\n\n<h3 id=\"sampling\">Sampling</h3>\n\n<p>There were multiple places where a decision was made to sample or not. There was no document that listed all of these places, making it impossible to even guess at the sampling rate without auditing all code to figure out where sampling decisions were being made.</p>\n\n<p>Moreover, there were multiple places where an unintentional sampling decision would be made due to the implementation. Spans were sent from services that had tracing enabled to a local agent, then to a &quot;collector&quot; service, and then from the collector service to our backing DB. Spans could be dropped at of these points: in the local agent; in the collector, which would have nodes fall over and lose all of their data regularly; and at the backing DB, which would reject writes due to hot keys or high load in general.</p>\n\n<p>This design where the trace id is the database key, with no intervening logic to pace out writes, meant that a 1M span trace (which we have) would cause 1M writes to the same key over a period of a few seconds. Another problem would be requests with a fanout of thousands (which exists at every tech company I've worked for), which could cause thousands writes with the same key over a period of a few milliseconds.</p>\n\n<p>Another sampling quirk was that, in order to avoid missing traces that didn't start at our internal front end, there was logic that caused an independent sampling decision in every RPC. If you do the math on this, if you have a service-oriented architecture like ours and you sample at what naively might sound like a moderately low rate, like, you'll end up with the vast majority of your spans starting at a leaf RPC, resulting in a single span trace. Of the non-leaf RPCs, the vast majority will start at the 2nd level from the leaf, and so on. The vast majority of our load and our storage costs were from these virtually useless traces that started at or near a leaf, and if you wanted to do any kind of analysis across spans to understand the behavior of the entire system, you'd have to account for this sampling bias on top of accounting for all of the other independent sampling decisions.</p>\n\n<h3 id=\"time\">Time</h3>\n\n<p>There wasn't really any kind of adjustment for clock skew (there was something, but it attempted to do a local pairwise adjustment, which didn't really improve things and actually made it more difficult to reasonably account for clock skew).</p>\n\n<p>If you just naively computed how long a span took, even using timestamps from a single host, which removes many sources of possible clock skew, you'd get a lot of negative duration spans, which is of course impossible because a result can't get returned before the request for the result is created. And if you compared times across different hosts, the results were even worse.</p>\n\n<h3 id=\"solutions\">Solutions</h3>\n\n<p>The solutions to these problems fall into what I think of as two buckets. For problems like dropped spans due to collector nodes falling over or the backing DB dropping requests, there's some straightforward engineering solution using well understood and widely used techniques. For that particular pair of problems, the short term bandaid was to do some GC tuning that reduced the rate of collector nodes falling over by about a factor of 100. That took all of two minutes, and then we replaced the collector nodes with a real queue that could absorb larger bursts in traffic and pace out writes to the DB. For the issue where we oversampled leaf-level spans due to rolling the sampling dice on every RPC, that's <a href=\"https://danluu.com/algorithms-interviews/\">one of these little questions that most people would get right in an interview that can sometimes get lost as part of a larger system</a> that has a number of solutions, e.g., since each span has a parent pointer, we must be able to know if an RPC has a parent or not in a relevant place and we can make a sampling decision and create a traceid iff a span has no parent pointer, which results in a uniform probability of each span being sampled, with each sampled trace being a complete trace.</p>\n\n<p>The other bucket is building up datasets and tools (and adding annotations) that allow users to answer questions they might have. This isn't a new idea, <a href=\"https://storage.googleapis.com/pub-tools-public-publication-data/pdf/36356.pdf\">section 5 of the Dapper paper</a> discussed this and it was published in 2010.</p>\n\n<p>Of course, one major difference is that Google has probably put at least two orders of magnitude more effort into building tools on top of Dapper than we've put into building tools on top of our tracing infra, so a lot of our tooling is much rougher, e.g., figure 6 from the Dapper paper shows a trace view that displays a set of relevant histograms, which makes it easy to understand the context of a trace. We haven't done the UI work for that yet, so the analogous view requires running a simple SQL query. While that's not hard, presenting the user with the data would be a better user experience than making the user query for the data.</p>\n\n<p>Of the work that's been done, the simplest obviously high ROI thing we've done is build a set of tables that contain information people might want to query, structured such that common queries that don't inherently have to do a lot of work don't have to do a lot of work.</p>\n\n<p>We have, partitioned by day, the following tables:</p>\n\n<ul>\n<li>trace_index\n\n<ul>\n<li>high-level trace-level information, e.g., does the trace have a root; what is the root; if relevant, what request endpoint was hit, etc.</li>\n</ul></li>\n<li>span_index\n\n<ul>\n<li>information on the client and server</li>\n</ul></li>\n<li>anno_index\n\n<ul>\n<li>&quot;standard&quot; annotations that people often want to query, e.g., request and response payload sizes, client/server send/recv timestamps, etc.</li>\n</ul></li>\n<li>span_metrics\n\n<ul>\n<li>computed metrics, e.g., span durations</li>\n</ul></li>\n<li>flat_annotation\n\n<ul>\n<li>All annotations, in case you want to query something not in anno_index</li>\n</ul></li>\n<li>trace_graph\n\n<ul>\n<li>For each trace, contains a graph representation of the trace, for use with queries that need the graph structure</li>\n</ul></li>\n</ul>\n\n<p>Just having this set of tables, queryable with SQL queries (or a Scalding or Spark job in cases where Presto SQL isn't ideal, like when doing some graph queries) is enough for tracing to pay for itself, to go from being difficult to justify to being something that's obviously high value.</p>\n\n<p>Some of the questions we've been to answer with this set of tables includes:</p>\n\n<ul>\n<li>For this service that's having problems, give me a representative set of traces</li>\n<li>For this service that has elevated load, show me which upstream service is causing the load</li>\n<li>Give me the list of all services that have unusual write amplification to downstream service X\n\n<ul>\n<li>Is traffic from a particular service or for a particular endpoint causing unusual write amplification? For example, in some cases, we see nothing unusual about the total write amplification from B -&gt; C, but we see very high amplification from B -&gt; C when B is called by A.</li>\n</ul></li>\n<li>Show me how much time we spend on <a href=\"https://en.wikipedia.org/wiki/Serialization\">serdes</a> vs. &quot;actual work&quot; for various requests</li>\n<li>Show me how much different kinds of requests cost in terms of backend work</li>\n<li>For requests that have high latency, as determined by mobile client instrumentation, show me what happened on the backend</li>\n<li>Show me the set of latency critical paths for this request endpoint (with the annotations we currently have, this has a number issues that probably deserve their own post)</li>\n<li>Show me the CDF of services that this service depends on\n\n<ul>\n<li>This is a distribution because whether or not a particular service calls another service is data dependent; it's not uncommon to have a service that will only call another one every 1000 calls (on average)</li>\n</ul></li>\n</ul>\n\n<p>We have built and are building other tooling, but just being able to run queries and aggregations against trace data, both recent and historical, easily pays for all of the other work we'd like to do. <a href=\"https://danluu.com/metrics-analytics/\">This analogous to what we saw when we looked at metrics data, taking data we already had and exposing it in a way that lets people run arbitrary queries immediately paid dividends</a>. Doing that for tracing is less straightforward than doing that for metrics because the data is richer, but it's a not fundamentally different idea.</p>\n\n<p>I think that having something to look at other than the raw data is also more important for tracing than it is for metrics since the metrics equivalent of a raw &quot;trace view&quot; of traces, a &quot;dashboard view&quot; of metrics where you just look at graphs, is obviously and intuitively useful. If that's all you have for metrics, people aren't going to say that it's not worth funding your metrics infra because dashboards are really useful! However, it's a lot harder to see how to get value out of a raw view of traces, which is where a lot of the comments about tracing not being valuable come from. This difference between the complexity of metrics data and tracing data makes the value add for higher-level views of tracing larger than it is for metrics.</p>\n\n<p>Having our data in a format that's not just blobs in a NoSQL DB has also allowed us to more easily build tooling on top of trace data that lets users who don't want to run SQL queries get value out of our trace data. An example of this is the Service Dependency Explorer (SDE), which was primarily built by Yuri Vishnevsky, Rebecca Isaacs, and Jonathan Simms, with help from Yihong Chen. If we try to look at the RPC call graph for a single request, we get something that's pretty large. In some cases, the depth of the call tree can be hundreds of levels deep and it's also not uncommon to see a fanout of 20 or more at some levels, which makes a naive visualization difficult to interpret.</p>\n\n<p>In order to see how SDE works, let's look at a smaller example where it's relatively easy to understand what's going on. Imagine we have 8 services, <code>A</code> through <code>H</code> and they call each other as shown in the tree below, we we have service <code>A</code> called 10 times, which calls service <code>B</code> a total of 10 times, which calls <code>D</code>, <code>D</code>, and <code>E</code> 50, 20, and 10 times respectively, where the two <code>D</code>s are distinguished by being different RPC endpoints (calls) even though they're the same service, and so on, shown below:</p>\n\n<p><img src=\"https://danluu.com/images/tracing-analytics/rpc-tree.png\" alt=\"Diagram of RPC call graph; this will implicitly described in the relevant sections, although the entire SDE section in showing off a visual tool and will probably be unsatisfying if you're just reading the alt text; the tables described in the previous section are more likely to be what you want if you want a non-visual interpretation of the data, the SDE is a kind of visualization\" width=\"610\" height=\"488\"></p>\n\n<p>If we look at SDE from the standpoint of node E, we'll see the following:\n<img src=\"https://danluu.com/images/tracing-analytics/e.png\" alt=\"SDE centered on service E, showing callers and callees, direct and indirect\" width=\"1266\" height=\"576\"></p>\n\n<p>We can see the direct callers and callees, 100% of calls of E are from C, and 100% of calls of E also call C and that we have 20x load amplification when calling C (200/10 = 20), the same as we see if we look at the RPC tree above. If we look at indirect callees, we can see that D has a 4x load amplification (40 / 10 = 4).</p>\n\n<p>If we want to see what's directly called by C downstream of E, we can select it and we'll get arrows to the direct descendants of C, which in this case is every indirect callee of E.</p>\n\n<p><img src=\"https://danluu.com/images/tracing-analytics/e.png\" alt=\"SDE centered on service E, with callee C highlighted\" width=\"1263\" height=\"239\"></p>\n\n<p>For a more complicated example, we can look at service D, which shows up in orange in our original tree, above.</p>\n\n<p>In this case, our summary box reads:</p>\n\n<ul>\n<li>On May 28, 2020 there were...\n\n<ul>\n<li>10 total <a href=\"https://courses.cs.washington.edu/courses/cse551/15sp/notes/talk-tsa.pdf\">TFE</a>-rooted traces</li>\n<li>110 total traced RPCs to D</li>\n<li>2.1 thousand total traced RPCs caused by D</li>\n<li>3 unique call paths from TFE endpoints to D endpoints</li>\n</ul></li>\n</ul>\n\n<p>The fact that we see D three times in the tree is indicated in the summary box, where it says we have 3 unique call paths from our front end, <a href=\"https://courses.cs.washington.edu/courses/cse551/15sp/notes/talk-tsa.pdf\">TFE</a> to D.</p>\n\n<p>We can expand out the calls to D and, in this case, see both of the calls and what fraction of traffic is to each call.</p>\n\n<p><img src=\"https://danluu.com/images/tracing-analytics/d.png\" alt=\"SDE centered on service D, with different calls to D expanded by having clicked on D\" width=\"1262\" height=\"364\"></p>\n\n<p>If we click on one of the calls, we can see which nodes are upstream and downstream dependencies of a particular call, <code>call4</code> is shown below and we can see that it never hits services <code>C</code>, <code>H</code>, and <code>G</code> downstream even though service <code>D</code> does for <code>call3</code>. Similarly, we can see that its upstream dependencies consist of being called directly by C, and indirectly by B and E but not A and C:</p>\n\n<p><img src=\"https://danluu.com/images/tracing-analytics/d-4.png\" alt=\"SDE centered on service D, with call4 of D highlighted by clicking on call 4; shows only upstream and downstream load that are relevant to call4\" width=\"1263\" height=\"343\"></p>\n\n<p>Some things we can easily see from SDE are:</p>\n\n<ul>\n<li>What load a service or RPC call causes\n\n<ul>\n<li>Where we have unusual load amplification, whether that's generally true for a service or if it only occurs on some call paths</li>\n</ul></li>\n<li>What causes load to a service or RPC call</li>\n<li>Where and why we get cycles (very common for <a href=\"https://www.thestrangeloop.com/2018/leverage-vs-autonomy-in-a-large-software-system.html\">Strato</a>, among other things</li>\n<li>What's causing weird super deep traces</li>\n</ul>\n\n<p>These are all things a user could get out of queries to the data we store, but having a tool with a UI that lets you click around in real time to explore things lowers the barrier to finding these things out.</p>\n\n<p>In the example shown above, there are a small number of services, so you could get similar information out of the more commonly used sea of nodes view, where each node is a service, with some annotations on the visualization, but when we've looked at real traces, showing thousands of services and a global makes it very difficult to see what's going on. Some of Rebecca's early analyses used a view like that, but we've found that you need to have a lot of implicit knowledge to make good use of a view like that, a view that discards a lot more information and highlights a few things makes it easier to users who don't happen to have the right implicit knowledge to get value out of looking at traces.</p>\n\n<p>Although we've demo'd a view of RPC count / load here, we could also display other things, like latency, errors, payload sizes, etc.</p>\n\n<h3 id=\"conclusion\">Conclusion</h3>\n\n<p>More generally, this is just a brief description of a few of the things we've built on top of the data you get if you have basic distributed tracing set up. You probably don't want to do exactly what we've done since you probably have somewhat different problems and you're very unlikely to encounter the exact set of problems that our tracing infra had. From backchannel chatter with folks at other companies, I don't think the level of problems we had was unique; if anything, our tracing infra was in a better state than at many or most peer companies (which excludes behemoths like FB/Google/Amazon) since it basically worked and people could and did use the trace view we had to debug real production issues. But, as they say, unhappy systems are unhappy in their own way.</p>\n\n<p>Like our previous look at <a href=\"https://danluu.com/metrics-analytics/\">metrics analytics</a>, this work was done incrementally. Since trace data is much richer than metrics data, a lot more time was spent doing ad hoc analyses of the data before writing the Scalding (MapReduce) jobs that produce the tables mentioned in this post, but the individual analyses were valuable enough that there wasn't really a time when this set of projects didn't pay for itself after the first few weeks it took to clean up some of the worst data quality issues and run an (extremely painful) ad hoc analysis with the existing infra.</p>\n\n<p>Looking back at discussions on whether or not it makes sense to work on tracing infra, people often point to the numerous failures at various companies to justify a buy (instead of build) decision. I don't think that's exactly unreasonable, the base rate of failure of similar projects shouldn't be ignored. But, on the other hand, most of the work described wasn't super tricky, beyond getting organizational buy-in and having a clear picture of the value that tracing can bring.</p>\n\n<p>One thing that's a bit beyond the scope of this post that probably deserves its own post is that, tracing and metrics, while not fully orthogonal, are complementary and having only one or the other leaves you blind to a lot of problems. You're going to pay a high cost for that in a variety of ways: unnecessary incidents, extra time spent debugging incidents, generally higher monetary costs due to running infra inefficiently, etc. Also, while metrics and tracing individually gives you much better visibility than having either alone, some problemls require looking at both together; some of the most interesting analyses I've done involve joining (often with a literal SQL join) trace data and <a href=\"https://danluu.com/metrics-analytics/\">metrics data</a>.</p>\n\n<p>To make it concrete, an example of something that's easy to see with tracing but annoying to see with logging unless you add logging to try to find this in particular (which you can do for any individual case, but probably don't want to do for the thousands of things tracing makes visible), is something we looked at above: &quot;show me cases where a specific call path from the load balancer to <code>A</code> causes high load amplification on some service <code>B</code>, which may be multiple hops away from <code>A</code> in the call graph. In some cases, this will be apparent because <code>A</code> generally causes high load amplificaiton on <code>B</code>, but if it only happens in some cases, that's still easy to handle with tracing but it's very annoying if you're just looking at metrics.</p>\n\n<p>An example of something where you want to join tracing and metrics data is when looking at the performance impact of something like a bad host on latency. You will, in general, not be able to annotate the appropriate spans that pass through the host as bad because, if you knew the host was bad at the time of the span, the host wouldn't be in production. But you can sometimes find, with historical data, a set of hosts that are bad, and then look up latency critical paths that pass through the host to determine the end-to-end impact of the bad host.</p>\n\n<p>Everyone has their own biases, with respect to tracing, mine come from generally working on things that try to direct improve cost, reliability, and latency, so the examples are focused on that, but there are also a lot of other uses for tracing. You can check out Distributed Tracing in Practice or Mastering Distributed Tracing for some other perspectives.</p>\n\n<h4 id=\"acknowledgements\">Acknowledgements</h4>\n\n<p><small>Thanks to Rebecca Isaacs, Leah Hanson, Yao Yue, and Yuri Vishnevsky for comments/corrections/discussion.</small></p>\n\n<p><link rel=\"prefetch\" href=\"https://danluu.com/\">\n<link rel=\"prefetch\" href=\"https://danluu.com/metrics-analytics/\">\n<link rel=\"prefetch\" href=\"https://danluu.com/algorithms-interviews/\">\n<link rel=\"prefetch\" href=\"https://danluu.com/about/\"></p>\n<div class=\"footnotes\">\n\n<hr />\n\n<ol>\n<li id=\"fn:O\"><p>this will almost certainly be an incomplete list, but some other people who've pitched in include Moses, Tiina, Rich, Rahul, Ben, Mike, Mary, Arash, Feng, Jenny, Andy, Yao, Yihong, Vinu, and myself.</p>\n\n<p>Note that this relatively long list of contributors doesn't contradict this work being high ROI. I'd estimate that there's been less than 2 person-years worth of work on everything discussed in this post. Just for example, while I spend a fair amount of time doing analyses that use the tracing infra, I think I've only spent on the order of one week on the infra itself.</p>\n\n<p>In case it's not obvious from the above, even though I'm writing this up, I was a pretty minor contributor to this. I'm just writing it up because I sat next to Rebecca as this work was being done and was super impressed by both her process and the outcome.</p>\n <a class=\"footnote-return\" href=\"#fnref:O\"><sup>[return]</sup></a></li>\n</ol>\n</div>\n"]}]} {:tag :item, :attrs nil, :content [{:tag :title, :attrs nil, :content ["A simple way to get more value from metrics"]} {:tag :link, :attrs nil, :content ["https://danluu.com/metrics-analytics/"]} {:tag :pubDate, :attrs nil, :content ["Sat, 30 May 2020 00:06:34 -0700"]} {:tag :guid, :attrs nil, :content ["https://danluu.com/metrics-analytics/"]} {:tag :description, :attrs nil, :content ["\n\n<p>We spent one day<sup class=\"footnote-ref\" id=\"fnref:D\"><a rel=\"footnote\" href=\"#fn:D\">1</a></sup> building a system that immediately found a mid 7 figure optimization (which ended up shipping). In the first year, we shipped mid 8 figures per year worth of cost savings as a result. The key feature this system introduces is the ability to query metrics data across all hosts and all services and over any period of time (since inception), so we've called it LongTermMetrics (LTM) internally since I like boring, descriptive, names.</p>\n\n<p>This got started when I was looking for a starter project that would both help me understand the Twitter infra stack and also have some easily quantifiable value. Andy Wilcox suggested looking at <a href=\"https://stackoverflow.com/a/37102630/334816\">JVM survivor space</a> utilization for some large services. If you're not familiar with what survivor space is, you can think of it as a configurable, fixed-size buffer, in the JVM (at least if you use the GC algorithm that's default at Twitter). At the time, if you looked at a random large services, you'd usually find that either:</p>\n\n<ol>\n<li>The buffer was too small, resulting in poor performance, sometimes catastrophically poor when under high load.</li>\n<li>The buffer was too large, resulting in wasted memory, i.e., wasted money.</li>\n</ol>\n\n<p>But instead of looking at random services, there's no fundamental reason that we shouldn't be able to query all services and get a list of which services have room for improvement in their configuration, sorted by performance degradation or cost savings. And if we write that query for JVM survivor space, this also goes for other configuration parameters (e.g., other JVM parameters, CPU quota, memory quota, etc.). Writing a query that worked for all the services turned out to be a little more difficult than I was hoping due to a combination of data consistency and performance issues. Data consistency issues included things like:</p>\n\n<ul>\n<li>Any given metric can have ~100 names, e.g., I found 94 different names for JVM survivor space\n\n<ul>\n<li>I suspect there are more, these were just the ones I could find via a simple search</li>\n</ul></li>\n<li>The same metric name might have a different meaning for different services\n\n<ul>\n<li>Could be a counter or a gauge</li>\n<li>Could have different units, e.g., bytes vs. MB or microseconds vs. milliseconds</li>\n</ul></li>\n<li>Metrics are sometimes tagged with an incorrect service name</li>\n<li>Zombie shards can continue to operate and report metrics even though the cluster manager has started up a new instance of the shard, resulting in duplicate and inconsistent metrics for a particular shard name</li>\n</ul>\n\n<p>Our metrics database, <a href=\"https://blog.twitter.com/engineering/en_us/topics/infrastructure/2019/metricsdb.html\" rel=\"noreferrer\">MetricsDB</a>, was specialized to handle monitoring, dashboards, alerts, etc. and didn't support general queries. That's totally reasonable, since monitoring and dashboards are lower on Maslow's hierarchy of observability needs than general metrics analytics. In backchannel discussions from folks at other companies, the entire set of systems around MetricsDB seems to have solved a lot of the problems that plauge people at other companies with similar scale, but the specialization meant that we couldn't run arbitrary SQL queries against metrics in MetricsDB.</p>\n\n<p>Another way to query the data is to use the copy that gets written to <a href=\"https://en.wikipedia.org/wiki/Apache_Hadoop#HDFS\">HDFS</a> in <a href=\"https://en.wikipedia.org/wiki/Apache_Parquet\">Parquet</a> format, which allows people to run arbitrary SQL queries (as well as write <a href=\"https://en.wikipedia.org/wiki/Cascading_(software)#cite_ref-27\">Scalding</a> (MapReduce) jobs that consume the data).</p>\n\n<p>Unfortunately, due to the number of metric names, the data on HDFS can't be stored in a columnar format with one column per name -- <a href=\"https://en.wikipedia.org/wiki/Presto_(SQL_query_engine)\">Presto</a> gets unhappy if you feed it too many columns and we have enough different metrics that we're well beyond that limit. If you don't use a columnar format (and don't apply any other tricks), you end up reading a lot of data for any non-trivial query. The result was that you couldn't run any non-trivial query (or even many trivial queries) across all services or all hosts without having it time out. We don't have similar timeouts for Scalding, but Scalding performance is much worse and a simple Scalding query against a day's worth of metrics will usually take between three and twenty hours, depending on cluster load, making it unreasonable to use Scalding for any kind of exploratory data analysis.</p>\n\n<p>Given the data infrastructure that already existed, an easy way to solve both of these problems was to write a Scalding job to store the 0.1% to 0.01% of metrics data that we care about for performance or capacity related queries and re-write it into a columnar format. I would guess that at least 90% of metrics are things that almost no one will want to look at in almost any circumstance, and of the metrics anyone really cares about, the vast majority aren't performance related. A happy side effect of this is that since such a small fraction of the data is relevant, it's cheap to store it indefinitely. The standard metrics data dump is deleted after a few weeks because it's large enough that it would be prohibitively expensive to store it indefinitely; a longer metrics memory will be useful for capacity planning or other analyses that prefer to have historical data.</p>\n\n<p>The data we're saving includes (but isn't limited to) the following things for each shard of each service:</p>\n\n<ul>\n<li>utilizations and sizes of various buffers</li>\n<li>CPU, memory, and other utilization</li>\n<li>number of threads, context switches, core migrations</li>\n<li>various queue depths and network stats</li>\n<li>JVM version, feature flags, etc.</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Garbage_collection_(computer_science)\">GC</a> stats</li>\n<li><a href=\"https://github.com/twitter/finagle\">Finagle</a> metrics</li>\n</ul>\n\n<p>And for each host:</p>\n\n<ul>\n<li>various things from <a href=\"https://en.wikipedia.org/wiki/Procfs\">procfs</a>, like <code>iowait</code> time, <code>idle</code>, etc.</li>\n<li>what cluster the machine is a part of</li>\n<li>host-level info like NIC speed, number of cores on the host, memory,</li>\n<li>host-level stats for &quot;health&quot; issues like thermal throttling, machine checks, etc.</li>\n<li>OS version, host-level software versions, host-level feature flags, etc.</li>\n<li><a href=\"https://github.com/twitter/rezolus\">Rezolus</a> metrics</li>\n</ul>\n\n<p>For things that we know change very infrequently (like host NIC speed), we store these daily, but most of these are stored at the same frequency and granularity that our other metrics is stored for. In some cases, this is obviously wasteful (e.g., for JVM tenuring threshold, which is typically identical across every shard of a service and rarely changes), but this was the easiest way to handle this given the infra we have around metrics.</p>\n\n<p>Although the impetus for this project was figuring out which services were under or over configured for JVM survivor space, it started with GC and container metrics since those were very obvious things to look at and we've been incrementally adding other metrics since then. To get an idea of the kinds of things we can query for and how simple queries are if you know a bit of SQL, here are some examples:</p>\n\n<h4 id=\"very-high-p90-jvm-survivor-space\">Very High p90 JVM Survivor Space</h4>\n\n<p>This is part of the original goal of finding under/over-provisioned services. Any service with a very high p90 JVM survivor space utilization is probably under-provisioned on survivor space.  Similarly, anything with a very low p99 or p999 JVM survivor space utilization when under peak load is probably overprovisioned (query not displayed here, but we can scope the query to times of high load).</p>\n\n<p>A Presto query for very high p90 survivor space across all services is:</p>\n\n<pre><code>with results as (\n  select servicename,\n    approx_distinct(source, 0.1) as approx_sources, -- number of shards for the service\n    -- real query uses [coalesce and nullif](https://prestodb.io/docs/current/functions/conditional.html) to handle edge cases, omitted for brevity\n    approx_percentile(jvmSurvivorUsed / jvmSurvivorMax, 0.90) as p90_used,\n    approx_percentile(jvmSurvivorUsed / jvmSurvivorMax, 0.50) as p50_used,\n  from ltm_service \n  where ds &gt;= '2020-02-01' and ds &lt;= '2020-02-28'\n  group by servicename)\nselect * from results\nwhere approx_sources &gt; 100\norder by p90_used desc\n</code></pre>\n\n<p>Rather than having to look through a bunch of dashboards, we can just get a list and then send diffs with config changes to the appropriate teams or write a script that takes the output of the query and automatically writes the diff. The above query provides a pattern for any basic utilization numbers or rates; you could look at memory usage, new or old gen GC frequency, etc., with similar queries. In one case, we found a service that was wasting enough RAM to pay my salary for a decade.</p>\n\n<p>I've been moving away from using thresholds against simple percentiles to find issues, but I'm presenting this query because this is a thing people commonly want to do that's useful and I can write this without having to spend a lot of space explain why it's a reasonable thing to do; what I prefer to do instead is out of scope of this post and probably deserves its own post.</p>\n\n<h4 id=\"network-utilization\">Network utilization</h4>\n\n<p>The above query was over all services, but we can also query across hosts. In addition, we can do queries that join against properties of the host, feature flags, etc.</p>\n\n<p>Using one set of queries, we were able to determine that we had a significant number of services running up against network limits even though host-level network utilization was low. The compute platform team then did a gradual rollout of a change to network caps, which we monitored with queries like the one below to determine that we weren't see any performance degradation (theoretically possible if increasing network caps caused hosts or switches to hit network limits).</p>\n\n<p>With the network change, we were able to observe, smaller queue depths, smaller queue size (in bytes), fewer packet drops, etc.</p>\n\n<p>The query below only shows queue depths for brevity; adding all of the quantities mentioned is just a matter of typing more names in.</p>\n\n<p>The general thing we can do is, for any particular rollout of a platform or service-level feature, we can see the impact on real services.</p>\n\n<pre><code>with rolled as (\n select\n   -- rollout was fixed for all hosts during the time period, can pick an arbitrary element from the time period\n   arbitrary(element_at(misc, 'egress_rate_limit_increase')) as rollout,\n   hostId\n from ltm_deploys\n where ds = '2019-10-10'\n and zone = 'foo'\n group by ipAddress\n), host_info as(\n select\n   arbitrary(nicSpeed) as nicSpeed,\n   hostId\n from ltm_host\n where ds = '2019-10-10'\n and zone = 'foo'\n group by ipAddress\n), host_rolled as (\n select\n   rollout,\n   nicSpeed,\n   rolled.hostId\n from rolled\n join host_info on rolled.ipAddress = host_info.ipAddress\n), container_metrics as (\n select\n   service,\n   netTxQlen,\n   hostId\n from ltm_container\n where ds &gt;= '2019-10-10' and ds &lt;= '2019-10-14'\n and zone = 'foo'\n)\nselect\n service,\n nicSpeed,\n approx_percentile(netTxQlen, 1, 0.999, 0.0001) as p999_qlen,\n approx_percentile(netTxQlen, 1, 0.99, 0.001) as p99_qlen,\n approx_percentile(netTxQlen, 0.9) as p90_qlen,\n approx_percentile(netTxQlen, 0.68) as p68_qlen,\n rollout,\n count(*) as cnt\nfrom container_metrics\njoin host_rolled on host_rolled.hostId = container_metrics.hostId\ngroup by service, nicSpeed, rollout\n</code></pre>\n\n<h4 id=\"other-questions-that-became-easy-to-answer\">Other questions that became easy to answer</h4>\n\n<ul>\n<li>What's the latency, CPU usage, CPI, or other performance impact of X?\n\n<ul>\n<li>Increasing or decreasing the number of performance counters we monitor per container</li>\n<li>Tweaking kernel parameters</li>\n<li>OS or other releases</li>\n<li>Increasing or decreasing host-level oversubscription</li>\n<li>General host-level load</li>\n<li>Retry budget exhaustion</li>\n</ul></li>\n<li>For relevant items above, what's the distribution of X, in general or under certain circumstances?</li>\n<li>What hosts have unusually poor service-level performance for every service on the host, after controlling for load, etc.?\n\n<ul>\n<li>This has usually turned out to be due to a hardware misconfiguration or fault</li>\n</ul></li>\n<li>Which services don't play nicely with other services aside from the general impact on host-level load?</li>\n<li>What's the latency impact of failover, or other high-load events?\n\n<ul>\n<li>What level of load should we expect in the future given a future high-load event plus current growth?</li>\n<li>Which services see more load during failover, which services see unchanged load, and which fall somewhere in between?</li>\n</ul></li>\n<li>What config changes can we make for any fixed sized buffer or allocation that will improve performance without increasing cost or reduce cost without degrading performance?</li>\n<li>For some particular host-level health problem, what's the probability it recurs if we see it N times?</li>\n<li>etc., there are a lot of questions that become easy to answer if you can write arbitrary queries against historical metrics data</li>\n</ul>\n\n<h4 id=\"design-decisions\">Design decisions</h4>\n\n<p>LTM is about as boring a system as is possible. Every design decision falls out of taking the path of least resistance.</p>\n\n<ul>\n<li>Why using Scalding?\n\n<ul>\n<li>It's standard at Twitter and the integration made everything trivial. I tried Spark, which has some advantages. However, at the time, I would have had to do manual integration work that I got for free with Scalding.</li>\n</ul></li>\n<li>Why use Presto and not something that allows for live slice &amp; dice queries like Druid?\n\n<ul>\n<li><a href=\"https://danluu.com/tracing-analytics/\">Rebecca Isaacs and Jonathan Simms were doing related work on tracing</a> and we knew that we'd want to do joins between LTM and whatever they created. That's trivial with Presto but would have required more planning and work with something like Druid, at least at the time.</li>\n<li>George Sirois imported a subset of the data into Druid so we could play with it and the facilities it offers are very nice; it's probably worth re-visiting at some point</li>\n</ul></li>\n<li>Why not use Postgres or something similar?\n\n<ul>\n<li>The amount of data we want to store makes this infeasible without a massive amount of effort; even though the cost of data storage is quite low, it's still a &quot;big data&quot; problem</li>\n</ul></li>\n<li>Why Parquet instead of a more efficient format?\n\n<ul>\n<li>It was the most suitable of the standard supported formats (the other major suppported format is raw thrift), introducing a new format would be a much larger project than this project</li>\n</ul></li>\n<li>Why is the system not real-time (with delays of at least one hour)?\n\n<ul>\n<li>Twitter's batch job pipeline is easy to build on, all that was necessary was to read some tutorial on how it works and then write something similar, but with different business logic.</li>\n<li>There was a nicely written proposal to build a real-time analytics pipeline for metrics data written a couple years before I joined Twitter, but that never got built because (I estimate) it would have been one to four quarters of work to produce an MVP and it wasn't clear what team had the right mandate to work on that and also had 4 quarters of headcount available. But the add a batch job took one day, you don't need to have roadmap and planning meetings for a day of work, you can just do it and then do follow-on work incrementally.</li>\n<li>If we're looking for misconfigurations or optimization opportunities, these rarely go away within an hour (and if they did, they must've had small total impact) and, in fact, they often persist for months to years, so we don't lose much by givng up on real-time (we do lose the ability to use the output of this for some monitoring use cases)</li>\n<li>The real-time version would've been a system that significant operational cost can't be operated by one person without undue burden. This system has more operational/maintenance burden than I'd like, probably 1-2 days of mine time per month a month on average, which at this point makes that a pretty large fraction of the total cost of the system, but it never pages, and the amount of work can easily be handeled by one person.</li>\n</ul></li>\n</ul>\n\n<h4 id=\"boring-technology\">Boring technology</h4>\n\n<p>I think writing about systems like this, that are just boring work is really underrated. A disproportionate number of posts and talks I read are about systems using hot technologies. I don't have anything against hot new technologies, but a lot of useful work comes from plugging boring technologies together and doing the obvious thing. Since posts and talks about boring work are relatively rare, I think writing up something like this is more useful than it has any right to be.</p>\n\n<p>For example, a couple years ago, at a local meetup that Matt Singer organizes for companies in our size class to discuss infrastructure (basically, companies that are smaller than FB/Amazon/Google) I asked if anyone was doing something similar to what we'd just done. No one who was there was (or not who'd admit to it, anyway), and engineers from two different companies expressed shock that we could store so much data, and not just the average per time period, but some histogram information as well. This work is too straightforward and obvious to be novel, I'm sure people have built analogous systems in many places. It's literally just storing metrics data on HDFS (or, if you prefer a more general term, a data lake) indefinitely in a format that allows interactive queries.</p>\n\n<p>If you do the math on the cost of metrics data storage for a project like this in a company in our size class, the storage cost is basically a rounding error. We've shipped individual diffs that easily pay for the storage cost for decades. I don't think there's any reason storing a few years or even a decade worth of metrics should be shocking when people deploy analytics and observability tools that cost much more all the time. But it turns out this was surprising, in part because people don't write up work this boring.</p>\n\n<p>An unrelated example is that, a while back, I ran into someone at a similarly sized company who wanted to get similar insights out of their metrics data. Instead of starting with something that would take a day, like this project, they started with deep learning. While I think there's value in applying ML and/or stats to infra metrics, they turned a project that could return significant value to the company after a couple of person-days into a project that took person-years. And if you're only going to <em>either</em> apply simple heuristics guided by someone with infra experience and simple statistical models <em>or</em> naively apply deep learning, I think the former has much higher ROI. Applying both sophisticated stats/ML <em>and</em> practitioner guided heuristics together can get you better results than either alone, but I think it makes a lot more sense to start with the simple project that takes a day to build out and maybe another day or two to start to apply than to start with a project that takes months or years to build out and start to apply. But there are a lot of biases towards doing the larger project: it makes a better resume item (deep learning!), in many places, it makes a better promo case, and people are more likely to give a talk or write up a blog post on the cool system that uses deep learning.</p>\n\n<p>The above discusses why writing up work is valuable for the industry in general. <a href=\"https://danluu.com/corp-eng-blogs/\">We covered why writing up work is valuable to the company doing the write-up in a previous post</a>, so I'm not going to re-hash that here.</p>\n\n<h4 id=\"appendix-stuff-i-screwed-up\">Appendix: stuff I screwed up</h4>\n\n<p><a href=\"https://twitter.com/danluu/status/1220228489522974721\">I think it's unfortunate that you don't get to hear about the downsides of systems without backchannel chatter</a>, so here are things I did that are pretty obvious mistakes in retrospect. I'll add to this when something else becomes obvious in retrospect.</p>\n\n<ul>\n<li>Not using a double for almost everything\n\n<ul>\n<li>In an ideal world, some things aren't doubles, but everything in our metrics stack goes through a stage where basically every metric is converted to a double</li>\n<li>I stored most things that &quot;should&quot; be an integral type as an integral type, but doing the conversion from <code>long -&gt; double -&gt; long</code> is never going to be more precise than just doing the<code>long -&gt; double</code> conversion and it opens the door to other problems</li>\n<li>I stored some things that shouldn't be an integral type as an integral type, which causes small values to unnecessarily lose precision\n\n<ul>\n<li>Luckily this hasn't caused serious errors for any actionable analysis I've done, but there are analyses where it could cause problems</li>\n</ul></li>\n</ul></li>\n<li>Using asserts instead of writing bad entries out to some kind of &quot;bad entries&quot; table\n\n<ul>\n<li>For reasons that are out of scope of this post, there isn't really a reasonable way to log errors or warnings in Scalding jobs, so I used asserts to catch things that shoudn't happen, which causes the entire job to die every time something unexpected happens; a better solution would be to write bad input entries out into a table and then have that table emailed out as a soft alert if the table isn't empty\n\n<ul>\n<li>An example of a case where this would've saved some operational overhead is where we had an unusual amount of clock skew (3600 years), which caused a timestamp overflow. If I had a table that was a log of bad entries, the bad entry would've been omitted from the output, which is the correct behavior, and it would've saved an interruption plus having to push a fix and re-deploy the job.</li>\n</ul></li>\n</ul></li>\n<li>Longterm vs. LongTerm in the code\n\n<ul>\n<li>I wasn't sure which way this should be capitalized when I was first writing this and, when I made a decision, I failed to grep for and squash everything that was written the wrong way, so now this pointless inconsistency exists in various places</li>\n</ul></li>\n</ul>\n\n<p>These are the kind of thing you expect when you crank out something quickly and don't think it through enough. The last item is trivial to fix and not much of a problem since the ubiquitous use of IDEs at Twitter means that basically anyone who would be impacted will have their IDE supply the correct capitalization for them.</p>\n\n<p>The first item is more problematic, both in that it could actually cause incorrect analyses and in that fixing it will require doing a migration of all the data we have. My guess is that, at this point, this will be half a week to a week of work, which I could've easily avoided by spending thirty more seconds thinking through what I was doing.</p>\n\n<p>The second item is somewhere in between. Between the first and second items, I think I've probably signed up for roughly double the amount of direct work on this system (so, not including time spent on data analysis on data in the system, just the time spent to build the system) for essentially no benefit.</p>\n\n<p><small>Thanks to Leah Hanson, Andy Wilcox, Lifan Zeng, and Matej Stuchlik for comments/corrections/discussion</small></p>\n\n<p><link rel=\"prefetch\" href=\"https://danluu.com/\">\n<link rel=\"prefetch\" href=\"https://danluu.com/corp-eng-blogs/\">\n<link rel=\"prefetch\" href=\"https://danluu.com/tracing-analytics/\">\n<link rel=\"prefetch\" href=\"https://danluu.com/about/\"></p>\n<div class=\"footnotes\">\n\n<hr />\n\n<ol>\n<li id=\"fn:D\"><p>The actual work involved was about a day's work, but it was done over a week since I had to learn Scala as well as Scalding and the general Twitter stack, the metrics stack, etc.</p>\n\n<p>One day is also just an estimate for the work for the initial data sets. Since then, I've done probably a couple more weeks of work and Wesley Aptekar-Cassels and Kunal Trivedi have probably put in another week or two of time. The opertional cost is probably something like 1-2 days of my time per month (on average), bringing the total cost to on the order a month or two.</p>\n\n<p>I'm also not counting time spent using the dataset, or time spent debugging issues, which will include a lot of time that I can only roughly guess at, e.g., when the compute platform team changed the network egress limits as a result of some data analysis that took about an hour, that exposed a latent mesos bug that probably cost a day of Ilya Pronin's time, David Mackey has spent a fair amount of time tracking down weird issues where the data shows something odd is going on, but we don't know what is, etc. If you wanted to fully account for time spent on work that came out of some data analysis on the data sets discussed in the post, I suspect, between service-level teams, plus platform-level teams like our JVM, OS, and HW teams, we're probably at roughly 1 person-year of time.</p>\n\n<p>But, because the initial work it took to create a working and useful system was a day plus time spent working on orientation material and the system returned seven figures, it's been very easy to justify all of this additional time spent, which probably wouldn't have been the case if a year of up-front work was required. Most of the rest of the time isn't the kind of thing that's usually &quot;charged&quot; on roadmap reviews on creating a system (time spent by users, operational overhead), but perhaps the ongoing operational cost shlould be &quot;charged&quot; when creating the system (I don't think it makes sense to &quot;charge&quot; time spent by users to the system since, the more useful a system is, the more time users will spend using it, that doesn't really seem like a cost).</p>\n\n<p>There'a also been work to build tools on top of this, Kunal Trivedi has spent a fair amount of time building a layer on top of this to make the presentation more user friendly than SQL queries, which could arguably be charged to this project.</p>\n <a class=\"footnote-return\" href=\"#fnref:D\"><sup>[return]</sup></a></li>\n</ol>\n</div>\n"]}]} {:tag :item, :attrs nil, :content [{:tag :title, :attrs nil, :content ["How (some) good corporate engineering blogs are written"]} {:tag :link, :attrs nil, :content ["https://danluu.com/corp-eng-blogs/"]} {:tag :pubDate, :attrs nil, :content ["Wed, 11 Mar 2020 00:00:00 +0000"]} {:tag :guid, :attrs nil, :content ["https://danluu.com/corp-eng-blogs/"]} {:tag :description, :attrs nil, :content ["\n\n<p>I've been comparing notes with people who run corporate engineering blogs and one thing that I think is curious is that it's pretty common for my personal blog to get more traffic than the entire corp eng blog for a company with a nine to ten figure valuation and it's not uncommon for my blog to get an order of magnitude more traffic.</p>\n\n<p>I think this is odd because tech companies in that class often have hundreds to thousands of employees. They're overwhelmingly likely to be better equipped to write a compelling blog than I am and companies get a lot more value from having a compelling blog than I do.</p>\n\n<p>With respect to the former, employees of the company will have done more interesting engineering work, have more fun stories, and have more in-depth knowledge than any one person who has a personal blog. On the latter, my blog helps me with job searching and it helps companies hire. But I only need one job, so more exposure, at best, gets me a slightly better job, whereas all but one tech company I've worked for is desperate to hire and loses candidates to other companies all the time. Moreover, I'm not really competing against other candidates when I interview (even if we interview for the same job, if the company likes more than one of us, it will usually just make more jobs). The high-order bit on this blog with respect to job searching is whether or not the process can take significant non-interview feedback or if <a href=\"https://danluu.com/algorithms-interviews/\">I'll fail the interview because they do a conventional interview</a> and the marginal value of an additional post is probably very low with respect to that. On the other hand, companies compete relatively directly when recruiting, so being more compelling relative to another company has value to them; replicating the playbook Cloudflare or Segment has used with their engineering &quot;brands&quot; would be a significant recruiting advantage. The playbook isn't secret: these companies broadcast their output to the world and are generally happy to talk about their blogging process.</p>\n\n<p>Despite the seemingly obvious benefits of having a &quot;good&quot; corp eng blog, most corp eng blogs are full of stuff engineers don't want to read. Vague, high-level fluff about how amazing everything is, content marketing, handwave-y posts about the new hotness (today, that might be using deep learning for inappropriate applications; ten years ago, that might have been using &quot;big data&quot; for inappropriate applications), etc.</p>\n\n<p>To try to understand what companies with good corporate engineering blog have in common, I interviewed folks at three different companies that have compelling corporate engineering blogs (Cloudflare, Heap, and Segment) as well as folks at three different companies that have lame corporate engineering blogs (which I'm not going to name).</p>\n\n<p>At a high level, the compelling engineering blogs had processes that shared the following properties:</p>\n\n<ul>\n<li>Easy approval process, not many approvals necessary</li>\n<li>Few or no non-engineering approvals required</li>\n<li>Implicit or explicit fast <a href=\"https://en.wikipedia.org/wiki/Service-level_objective\">SLO</a> on approvals</li>\n<li>Approval/editing process mainly makes posts more compelling to engineers</li>\n<li>Direct, high-level (co-founder, C-level, or VP-level) support for keeping blog process lightweight</li>\n</ul>\n\n<p>The less compelling engineering blogs had processes that shared the following properties:</p>\n\n<ul>\n<li>Slow approval process</li>\n<li>Many approvals necessary</li>\n<li>Significant non-engineering approvals necessary\n\n<ul>\n<li>Non-engineering approvals suggest changes authors find frustrating</li>\n<li>Back-and-forth can go on for months</li>\n</ul></li>\n<li>Approval/editing process mainly de-risks posts, removes references to specifics, makes posts vaguer and less interesting to engineers</li>\n<li>Effectively no high-level support for blogging\n\n<ul>\n<li>Leadership may agree that blogging is good in the abstract, but it's not a high enough priority to take concrete action</li>\n<li>Reforming process to make blogging easier very difficult; previous efforts have failed</li>\n<li>Changing process to reduce overhead requires all &quot;stakeholders&quot; to sign off (14 in one case)\n\n<ul>\n<li>Any single stakeholder can block</li>\n<li>No single stakeholder can approve</li>\n</ul></li>\n<li>Stakeholders wary of approving anything that reduces overhead\n\n<ul>\n<li>Approving involves taking on perceived risk (what if something bad happens) with no perceived benefit to them</li>\n</ul></li>\n</ul></li>\n</ul>\n\n<p>One person at a company with a compelling blog noted that a downside of having only one approver and/or one primary approver is that if that person is busy, it can takes weeks to get posts approved. That's fair, that's a downside of having centralized approval. However, when we compare to the alternative processes, at one company, people noted that it's typical for approvals to take three to six months and tail cases can take a year.</p>\n\n<p>While a few weeks can seem like a long time for someone used to a fast moving company, people at slower moving companies would be ecstatic to have an approval process that only takes twice that long.</p>\n\n<p>Here are the processes, as described to me, for the three companies I interviewed (presented in <code>sha512sum</code> order, which is coincidentally ordered by increasing size of company, from a couple hundred employees to nearly one thousand employees):</p>\n\n<h4 id=\"heap\">Heap</h4>\n\n<ul>\n<li>Someone has an idea to write a post</li>\n<li>Writer (who is an engineer) is paired with a &quot;buddy&quot;, who edits and then approves the post\n\n<ul>\n<li>Buddy is an engineer who has a track record of producing reasonable writing</li>\n<li>This may take a few rounds, may change thrust of the post</li>\n</ul></li>\n<li>CTO reads and approves\n\n<ul>\n<li>Usually only minor feedback</li>\n<li>May make suggestions like &quot;a designer could make this graph look better&quot;</li>\n</ul></li>\n<li>Publish post</li>\n</ul>\n\n<p>The first editing phase used to involve posting a draft to a slack channel where &quot;everyone&quot; would comment on the post. This was an unpleasant experience since &quot;everyone&quot; would make comments and a lot of revision would be required. This process was designed to avoid getting &quot;too much&quot; feedback.</p>\n\n<h4 id=\"segment\">Segment</h4>\n\n<ul>\n<li>Someone has an idea to write a post\n\n<ul>\n<li>Often comes from: internal docs, external talk, shipped project, open source tooling (built by Segment)</li>\n</ul></li>\n<li>Writer (who is an engineer) writes a draft\n\n<ul>\n<li>May have a senior eng work with them to write the draft</li>\n</ul></li>\n<li>Until recently, no one really owned the feedback process\n\n<ul>\n<li>Calvin French-Owen (co-founder) and Rick (engineering manager) would usually give most feedback</li>\n<li>Maybe also get feedback from manager and eng leadership</li>\n<li>Typically, 3rd draft is considered finished</li>\n<li>Now, have a full-time editor who owns editing posts</li>\n</ul></li>\n<li>Also socialize among eng team, get get feedback from 15-20 people</li>\n<li>PR and legal will take a look, lightweight approval</li>\n</ul>\n\n<p>Some changes that have been made include</p>\n\n<ul>\n<li>At one point, when trying to establish an &quot;engineering brand&quot;, making in-depth technical posts a top-level priority</li>\n<li>had a &quot;blogging retreat&quot;, one week spent on writing a post</li>\n<li>added writing and speaking as explicit criteria to be rewarded in performance reviews and career ladders</li>\n</ul>\n\n<p>Although there's legal and PR approval, Calvin noted &quot;In general we try to keep it fairly lightweight. I see the bigger problem with blogging being a lack of posts or vague, high level content which isn't interesting rather than revealing too much.&quot;</p>\n\n<h4 id=\"cloudflare\">Cloudflare</h4>\n\n<ul>\n<li>Someone has an idea to write a post\n\n<ul>\n<li>Internal blogging is part of the culture, some posts come from the internal blog</li>\n</ul></li>\n<li>John Graham-Cumming (CTO) reads every post, other folks will read and comment\n\n<ul>\n<li>John is approver for posts</li>\n</ul></li>\n<li>Matthew Prince (CEO) also generally supportive of blogging</li>\n<li>&quot;Very quick&quot; legal approval process, SLO of 1 hour\n\n<ul>\n<li>This process is so lightweight that one person didn't really think of it as an approval, another person didn't mention it at all (a third person did mention this step)</li>\n<li>Comms generally not involved</li>\n</ul></li>\n</ul>\n\n<p>One thing to note is that this only applies to technical blog posts. Product announcements have a heavier process because they're tied to sales material, press releases, etc.</p>\n\n<p>One thing I find interesting is that Marek interviewed at Cloudflare because of their blog (<a href=\"https://blog.cloudflare.com/a-tour-inside-cloudflares-latest-generation-servers/\">this 2013 blog post on their 4th generation servers caught his eye</a>) and he's now both a key engineer for them as well as one of the main sources of compelling Cloudflare blog posts. At this point, the Cloudflare blog has generated at least a few more generations of folks who interviewed because they saw a blog post and now write compelling posts for the blog.</p>\n\n<h3 id=\"general-comments\">General comments</h3>\n\n<p>My opinion is that the natural state of a corp eng blog where people <a href=\"https://danluu.com/p95-skill/\">get a bit of feedback</a> is a pretty interesting blog. There's <a href=\"https://twitter.com/rakyll/status/1043952902157459456\">a dearth of real, in-depth, technical writing</a>, which makes any half decent, honest, public writing about technical work interesting.</p>\n\n<p>In order to have a boring blog, the corporation has to actively stop engineers from putting interesting content out there. Unfortunately, it appears that the natural state of large corporations tends towards risk aversion and blocking people from writing, just in case it causes a legal or PR or other problem. Individual contributors (ICs) might have the opinion that it's ridiculous to block engineers from writing low-risk technical posts while, simultaneously, C-level execs and VPs regularly make public comments that turn into PR disasters, but ICs in large companies don't have the authority or don't feel like they have the authority to do something just because it makes sense. And none of the fourteen stakeholders who'd have to sign off on approving a streamlined process care about streamlining the process since that would be good for the company in a way that doesn't really impact them, not when that would mean seemingly taking responsibility for the risk a streamlined process would add, however small. An exec or a senior VP willing to take a risk can take responsibility for the fallout and, if they're interested in engineering recruiting or morale, they may see a reason to do so.</p>\n\n<p>One comment I've often heard from people at more bureaucratic companies is something like &quot;every company our size is like this&quot;, but that's not true. Cloudflare, a $6B company approaching 1k employees is in the same size class as many other companies with a much more onerous blogging process. The corp eng blog situation seems similar to situation on giving real interview feedback. <a href=\"http://blog.interviewing.io/no-engineer-has-ever-sued-a-company-because-of-constructive-post-interview-feedback-so-why-dont-employers-do-it/\">interviewing.io claims that there's significant upside and very little downside to doing so</a>. Some companies actually do give real feedback and the ones that do generally find that it gives them an easy advantage in recruiting with little downside, but the vast majority of companies don't do this and people at those companies will claim that it's impossible to do give feedback since you'll get sued or the company will be &quot;cancelled&quot; even though this generally doesn't happen to companies that give feedback and there are even entire industries where it's common to give interview feedback. It's easy to handwave that some risk exists and very few people have the authority to dismiss vague handwaving about risk when it's coming from multiple orgs.</p>\n\n<p>Although this is a small sample size and it's dangerous to generalize too much from small samples, the idea that you need high-level support to blast through bureaucracy is consistent with what I've seen in other areas where most large companies have a hard time doing something easy that has obvious but diffuse value. While this post happens to be about blogging, I've heard stories that are the same shape on a wide variety of topics.</p>\n\n<h3 id=\"appendix-examples-of-compelling-blog-posts\">Appendix: examples of compelling blog posts</h3>\n\n<p>Here are some blog posts from the blogs mentioned with a short comment on why I thought the post was compelling. This time, in reverse sha512 hash order.</p>\n\n<h4 id=\"cloudflare-1\">Cloudflare</h4>\n\n<ul>\n<li><a href=\"https://blog.cloudflare.com/how-verizon-and-a-bgp-optimizer-knocked-large-parts-of-the-internet-offline-today/\">https://blog.cloudflare.com/how-verizon-and-a-bgp-optimizer-knocked-large-parts-of-the-internet-offline-today/</a>\n\n<ul>\n<li>Talks about a real technical problem that impacted a lot of people, reasonably in depth</li>\n<li>Timely, released only eight hours after the outage, when people were still really interested in hearing about what happened; most companies can't turn around a compelling blog post this quickly or can only do it on a special-case basis, Cloudflare is able to crank out timely posts semi-regularly</li>\n</ul></li>\n<li><a href=\"https://blog.cloudflare.com/the-relative-cost-of-bandwidth-around-the-world/\">https://blog.cloudflare.com/the-relative-cost-of-bandwidth-around-the-world/</a>\n\n<ul>\n<li>Exploration of some data</li>\n</ul></li>\n<li><a href=\"https://blog.cloudflare.com/the-story-of-one-latency-spike/\">https://blog.cloudflare.com/the-story-of-one-latency-spike/</a>\n\n<ul>\n<li>A debugging story</li>\n</ul></li>\n<li><a href=\"https://blog.cloudflare.com/when-bloom-filters-dont-bloom/\">https://blog.cloudflare.com/when-bloom-filters-dont-bloom/</a>\n\n<ul>\n<li>A debugging story, this time in the context of developing a data structure</li>\n</ul></li>\n</ul>\n\n<h4 id=\"segment-1\">Segment</h4>\n\n<ul>\n<li><a href=\"https://segment.com/blog/when-aws-autoscale-doesn-t/\">https://segment.com/blog/when-aws-autoscale-doesn-t/</a>\n\n<ul>\n<li>Concrete explanation of a gotcha in a widely used service</li>\n</ul></li>\n<li><a href=\"https://segment.com/blog/gotchas-from-two-years-of-node/\">https://segment.com/blog/gotchas-from-two-years-of-node/</a>\n\n<ul>\n<li>Concrete example and explanation of a gotcha in a widely used tool</li>\n</ul></li>\n<li><a href=\"https://segment.com/blog/automating-our-infrastructure/\">https://segment.com/blog/automating-our-infrastructure/</a>\n\n<ul>\n<li>Post with specific details about how a company operates; in theory, any company could write this, but few do</li>\n</ul></li>\n</ul>\n\n<h4 id=\"heap-1\">Heap</h4>\n\n<ul>\n<li><a href=\"https://heap.io/blog/engineering/basic-performance-analysis-saved-us-millions\">https://heap.io/blog/engineering/basic-performance-analysis-saved-us-millions</a>\n\n<ul>\n<li>Talks about a real problem and solution</li>\n</ul></li>\n<li><a href=\"https://heap.io/blog/engineering/clocksource-aws-ec2-vdso\">https://heap.io/blog/engineering/clocksource-aws-ec2-vdso</a>\n\n<ul>\n<li>Talks about a real problem and solution</li>\n<li>In HN comments, engineers (malisper, kalmar) have technical responses with real reasons in them and not just the usual dissembling that you see in most cases</li>\n</ul></li>\n<li><a href=\"https://heap.io/blog/analysis/migrating-to-typescript\">https://heap.io/blog/analysis/migrating-to-typescript</a>\n\n<ul>\n<li>Real talk about how the first attempt at driving a company-wide technical change failed</li>\n</ul></li>\n</ul>\n\n<p>One thing to note is that these blogs all have different styles. Personally, I prefer the style of Cloudflare's blog, which has a higher proportion of &quot;deep dive&quot; technical posts, but different people will prefer different styles. There are a lot of styles that can work.</p>\n\n<p><small>Thanks to Marek Majkowski, Kamal Marhubi, Calvin French-Owen, John Graham-Cunning, Michael Malis, Matthew Prince, Yuri Vishnevsky, Julia Evans, Wesley Aptekar-Cassels, Nathan Reed, Jake Seliger, an anonymous commenter, plus sources from the companies I didn't name for comments/corrections/discussion; none of the people explicitly mentioned in the acknowledgements were sources for information on the less compelling blogs</small></p>\n\n<p><link rel=\"prefetch\" href=\"https://danluu.com/algorithms-interviews/\">\n<link rel=\"prefetch\" href=\"https://danluu.com/p95-skill/\">\n<link rel=\"prefetch\" href=\"https://danluu.com/\">\n<link rel=\"prefetch\" href=\"https://danluu.com/about/\"></p>\n"]}]} {:tag :item, :attrs nil, :content [{:tag :title, :attrs nil, :content ["The growth of command line options, 1979-Present"]} {:tag :link, :attrs nil, :content ["https://danluu.com/cli-complexity/"]} {:tag :pubDate, :attrs nil, :content ["Tue, 03 Mar 2020 00:00:00 +0000"]} {:tag :guid, :attrs nil, :content ["https://danluu.com/cli-complexity/"]} {:tag :description, :attrs nil, :content ["\n\n<p><a href=\"https://www.xkcd.com/1795/\">My hobby</a>: opening up <a href=\"https://danluu.com/mcilroy-unix/\">McIlroys UNIX philosophy</a> on one monitor while reading manpages on the other.</p>\n\n<p>The first of McIlroy's dicta is often paraphrased as &quot;do one thing and do it well&quot;, which is <a href=\"https://danluu.com/mcilroy-unix/\">shortened from</a> &quot;Make each program do one thing well. To do a new job, build afresh rather than complicate old programs by adding new 'features.'&quot;</p>\n\n<p>McIlroy's example of this dictum is:</p>\n\n<blockquote>\n<p>Surprising to outsiders is the fact that UNIX compilers produce no listings: printing can be done better and more flexibly by a separate program.</p>\n</blockquote>\n\n<p>If you open up a manpage for <code>ls</code> on mac, youll see that it starts with</p>\n\n<pre><code>ls [-ABCFGHLOPRSTUW@abcdefghiklmnopqrstuwx1] [file ...]\n</code></pre>\n\n<p>That is, the one-letter flags to <code>ls</code> include every lowercase letter except for <code>{jvyz}</code>, 14 uppercase letters, plus <code>@</code> and <code>1</code>. Thats 22 + 14 + 2 = 38 single-character options alone.</p>\n\n<p>On ubuntu 17, if you read the manpage for coreutils <code>ls</code>, you dont get a nice summary of options, but youll see that <code>ls</code> has 58 options (including <code>--help</code> and <code>--version</code>).</p>\n\n<p>To see if <code>ls</code> is an aberration or if it's normal to have commands that do this much stuff, we can look at some common commands, sorted by frequency of use.</p>\n\n<p><style>table {border-collapse:collapse;margin:0px auto;}table,th,td {border: 1px solid black;}td {text-align:center;}</style>\n<table>\n<tr>\n<th>command</th><th>1979</th><th>1996</th><th>2015</th><th>2017</th></tr>\n<tr>\n<td>ls</td><td bgcolor=#6baed6>11</td><td bgcolor=#2171b5><font color=white>42</font></td><td bgcolor=#08519c><font color=white>58</font></td><td bgcolor=#08519c><font color=white>58</font></td></tr>\n<tr>\n<td>rm</td><td bgcolor=#deebf7>3</td><td bgcolor=#9ecae1>7</td><td bgcolor=#6baed6>11</td><td bgcolor=#6baed6>12</td></tr>\n<tr>\n<td>mkdir</td><td bgcolor=white>0</td><td bgcolor=#c6dbef>4</td><td bgcolor=#9ecae1>6</td><td bgcolor=#9ecae1>7</td></tr>\n<tr>\n<td>mv</td><td bgcolor=white>0</td><td bgcolor=#9ecae1>9</td><td bgcolor=#6baed6>13</td><td bgcolor=#6baed6>14</td></tr>\n<tr>\n<td>cp</td><td bgcolor=white>0</td><td bgcolor=#4292c6>18</td><td bgcolor=#2171b5><font color=white>30</font></td><td bgcolor=#2171b5><font color=white>32</font></td></tr>\n<tr>\n<td>cat</td><td bgcolor=#f7fbff>1</td><td bgcolor=#6baed6>12</td><td bgcolor=#6baed6>12</td><td bgcolor=#6baed6>12</td></tr>\n<tr>\n<td>pwd</td><td bgcolor=white>0</td><td bgcolor=#deebf7>2</td><td bgcolor=#c6dbef>4</td><td bgcolor=#c6dbef>4</td></tr>\n<tr>\n<td>chmod</td><td bgcolor=white>0</td><td bgcolor=#9ecae1>6</td><td bgcolor=#9ecae1>9</td><td bgcolor=#9ecae1>9</td></tr>\n<tr>\n<td>echo</td><td bgcolor=#f7fbff>1</td><td bgcolor=#c6dbef>4</td><td bgcolor=#c6dbef>5</td><td bgcolor=#c6dbef>5</td></tr>\n<tr>\n<td>man</td><td bgcolor=#c6dbef>5</td><td bgcolor=#6baed6>16</td><td bgcolor=#2171b5><font color=white>39</font></td><td bgcolor=#2171b5><font color=white>40</font></td></tr>\n<tr>\n<td>which</td><td bgcolor=silver></td><td bgcolor=white>0</td><td bgcolor=#f7fbff>1</td><td bgcolor=#f7fbff>1</td></tr>\n<tr>\n<td>sudo</td><td bgcolor=silver></td><td bgcolor=white>0</td><td bgcolor=#4292c6>23</td><td bgcolor=#4292c6>25</td></tr>\n<tr>\n<td>tar</td><td bgcolor=#6baed6>12</td><td bgcolor=#08519c><font color=white>53</font></td><td bgcolor=black><font color=white>134</font></td><td bgcolor=black><font color=white>139</font></td></tr>\n<tr>\n<td>touch</td><td bgcolor=#f7fbff>1</td><td bgcolor=#9ecae1>9</td><td bgcolor=#6baed6>11</td><td bgcolor=#6baed6>11</td></tr>\n<tr>\n<td>clear</td><td bgcolor=silver></td><td bgcolor=white>0</td><td bgcolor=white>0</td><td bgcolor=white>0</td></tr>\n<tr>\n<td>find</td><td bgcolor=#6baed6>14</td><td bgcolor=#08519c><font color=white>57</font></td><td bgcolor=#08519c><font color=white>82</font></td><td bgcolor=#08519c><font color=white>82</font></td></tr>\n<tr>\n<td>ln</td><td bgcolor=white>0</td><td bgcolor=#6baed6>11</td><td bgcolor=#6baed6>15</td><td bgcolor=#6baed6>16</td></tr>\n<tr>\n<td>ps</td><td bgcolor=#c6dbef>4</td><td bgcolor=#4292c6>22</td><td bgcolor=#08306b><font color=white>85</font></td><td bgcolor=#08306b><font color=white>85</font></td></tr>\n<tr>\n<td>ping</td><td bgcolor=silver></td><td bgcolor=#6baed6>12</td><td bgcolor=#6baed6>12</td><td bgcolor=#2171b5><font color=white>29</font></td></tr>\n<tr>\n<td>kill</td><td bgcolor=#f7fbff>1</td><td bgcolor=#deebf7>3</td><td bgcolor=#deebf7>3</td><td bgcolor=#deebf7>3</td></tr>\n<tr>\n<td>ifconfig</td><td bgcolor=silver></td><td bgcolor=#6baed6>16</td><td bgcolor=#4292c6>25</td><td bgcolor=#4292c6>25</td></tr>\n<tr>\n<td>chown</td><td bgcolor=white>0</td><td bgcolor=#9ecae1>6</td><td bgcolor=#6baed6>15</td><td bgcolor=#6baed6>15</td></tr>\n<tr>\n<td>grep</td><td bgcolor=#6baed6>11</td><td bgcolor=#4292c6>22</td><td bgcolor=#2171b5><font color=white>45</font></td><td bgcolor=#2171b5><font color=white>45</font></td></tr>\n<tr>\n<td>tail</td><td bgcolor=#f7fbff>1</td><td bgcolor=#9ecae1>7</td><td bgcolor=#6baed6>12</td><td bgcolor=#6baed6>13</td></tr>\n<tr>\n<td>df</td><td bgcolor=white>0</td><td bgcolor=#6baed6>10</td><td bgcolor=#4292c6>17</td><td bgcolor=#4292c6>18</td></tr>\n<tr>\n<td>top</td><td bgcolor=silver></td><td bgcolor=#9ecae1>6</td><td bgcolor=#6baed6>12</td><td bgcolor=#6baed6>14</td></tr>\n</table></p>\n\n<p>This table has the number of command line options for various commands for v7 Unix (1979), slackware 3.1 (1996), ubuntu 12 (2015), and ubuntu 17 (2017). Cells are darker and blue-er when they have more options (log scale) and are greyed out if no command was found.</p>\n\n<p>We can see that the number of command line options has dramatically increased over time; entries tend to get darker going to the right (more options) and there are no cases where entries get lighter (fewer options).</p>\n\n<p><a href=\"https://archive.org/details/DougMcIlroy_AncestryOfLinux_DLSLUG\">McIlroy has long decried the increase in the number of options, size, and general functionality of commands</a><sup class=\"footnote-ref\" id=\"fnref:M\"><a rel=\"footnote\" href=\"#fn:M\">1</a></sup>:</p>\n\n<blockquote>\n<p>Everything was small and my heart sinks for Linux when I see the size [inaudible]. The same utilities that used to fit in eight k[ilobytes] are a meg now. And the manual page, which used to really fit on, which used to really be a manual <em>page</em>, is now a small volume with a thousand options... We used to sit around in the UNIX room saying &quot;what can we throw out? Why is there this option?&quot; It's usually, it's often because there's some deficiency in the basic design  you didn't really hit the right design point. Instead of putting in an option, figure out why, what was forcing you to add that option. This viewpoint, which was imposed partly because there was very small hardware ... has been lost and we're not better off for it.</p>\n</blockquote>\n\n<p>Ironically, one of the reasons for the rise in the number of command line options is another McIlroy dictum, &quot;Write programs to handle text streams, because that is a universal interface&quot; (see <code>ls</code> for one example of this).</p>\n\n<p>If structured data or objects were passed around, formatting could be left to a final formatting pass. But, with plain text, the formatting and the content are intermingled; because formatting can only be done by parsing the content out, it's common for commands to add formatting options for convenience. Alternately, formatting can be done when the user leverages their knowledge of the structure of the data and encodes that knowledge into arguments to <code>cut</code>, <code>awk</code>, <code>sed</code>, etc. (also using their knowledge of how those programs handle formatting; it's different for different programs and the user is expected to, for example, <a href=\"https://unix.stackexchange.com/a/132322/261842\">know how <code>cut -f4</code> is different from <code>awk '{ print $4 }'</code></a><sup class=\"footnote-ref\" id=\"fnref:T\"><a rel=\"footnote\" href=\"#fn:T\">2</a></sup>). That's a lot more hassle than passing in one or two arguments to the last command in a sequence and it pushes the complexity from the tool to the user.</p>\n\n<p>People sometimes say that they don't want to support structured data because they'd have to support multiple formats to make a universal tool, but they already have to support multiple formats to make a universal tool. Some standard commands can't read output from other commands because they use different formats, <code>wc -w</code> doesn't handle Unicode correctly, etc. Saying that &quot;text&quot; is a universal format is like saying that &quot;binary&quot; is a universal format.</p>\n\n<p>I've heard people say that there isn't really any alternative to this kind of complexity for command line tools, but people who say that have never really tried the alternative, something like PowerShell. I have plenty of complaints about PowerShell, but passing structured data around and easily being able to operate on structured data without having to hold metadata information in my head so that I can pass the appropriate metadata to the right command line tools at that right places the pipeline isn't among my complaints<sup class=\"footnote-ref\" id=\"fnref:W\"><a rel=\"footnote\" href=\"#fn:W\">3</a></sup>.</p>\n\n<p>The sleight of hand that's happening when someone says that we can keep software simple and compatible by making everything handle text is the pretense that text data doesn't have a structure that needs to be parsed<sup class=\"footnote-ref\" id=\"fnref:C\"><a rel=\"footnote\" href=\"#fn:C\">4</a></sup>. In some cases, we can just think of everything as a single space separated line, or maybe a table with some row and column separators that we specify (<a href=\"https://unix.stackexchange.com/a/132322/261842\">with some behavior that isn't consistent across tools, of course</a>). That adds some hassle when it works, and then there are the cases where serializing data to a flat text format adds considerable complexity since the structure of data means that simple flattening requires significant parsing work to re-ingest the data in a meaningful way.</p>\n\n<p>Another reason commands now have more options is that people have added convenience flags for functionality that could have been done by cobbling together a series of commands. These go all the way back to v7 unix, where <code>ls</code> has an option to reverse the sort order (which could have been done by passing the output to something like <code>tac</code> had they written <code>tac</code> instead of adding a special-case reverse option).</p>\n\n<p>Over time, more convenience options have been added. For example, to pick a command that originally has zero options, <code>mv</code> can move <em>and</em> create a backup (three options; two are different ways to specify a backup, one of which takes an argument and the other of which takes zero explicit arguments and reads an implicit argument from the <code>VERSION_CONTROL</code> environment variable; one option allows overriding the default backup suffix). <code>mv</code> now also has options to never overwrite and to only overwrite if the file is newer.</p>\n\n<p><code>mkdir</code> is another program that used to have no options where, excluding security things for SELinux or SMACK as well as help and version options, the added options are convenience flags: setting the permissions of the new directory and making parent directories if they don't exist.</p>\n\n<p>If we look at <code>tail</code>, which originally had one option (<code>-number</code>, telling <code>tail</code> where to start), it's added both formatting and convenience options For formatting, it has <code>-z</code>, which makes the line delimiter <code>null</code> instead of a newline. Some examples of convenience options are <code>-f</code> to print when there are new changes, <code>-s</code> to set the sleep interval between checking for <code>-f</code> changes, <code>--retry</code> to retry if the file isn't accessible.</p>\n\n<p>McIlroy says &quot;we're not better off&quot; for having added all of these options but I'm better off. I've never used some of the options we've discussed and only rarely use others, but that's the beauty of command line options  unlike with a GUI, adding these options doesn't clutter up the interface. The manpage can get cluttered, but in the age of google and stackoverflow, I suspect many people just search for a solution to what they're trying to do without reading the manpage anyway.</p>\n\n<p>This isn't to say there's no cost to adding options  more options means more maintenance burden, but that's a cost that maintainers pay to benefit users, which isn't obviously unreasonable considering the ratio of maintainers to users. This is analogous to Gary Bernhardt's comment that it's reasonable to practice a talk fifty times since, if there's a three hundred person audience, the ratio of time spent watching to the talk to time spent practicing will still only be 1:6. In general, this ratio will be even more extreme with commonly used command line tools.</p>\n\n<p>Someone might argue that all these extra options create a burden for users. That's not exactly wrong, but that complexity burden was always going to be there, it's just a question of where the burden was going to lie. If you think of the set of command line tools along with a shell as forming a language, a language where anyone can write a new method and it effectively gets added to the standard library if it becomes popular, where standards are defined by dicta like &quot;write programs to handle text streams, because that is a universal interface&quot;, the language was always going to turn into a write-only incoherent mess when taken as a whole. At least with tools that bundle up more functionality and options than is UNIX-y users can replace a gigantic set of wildly inconsistent tools with a merely large set of tools that, while inconsistent with each other, may have some internal consistency.</p>\n\n<p>McIlroy implies that the problem is that people didn't think hard enough, the old school UNIX mavens would have sat down in the same room and thought longer and harder until they came up with a set of consistent tools that has &quot;unusual simplicity&quot;. But that was never going to scale, the philosophy made the mess we're in inevitable. It's not a matter of not thinking longer or harder; it's a matter of having a philosophy that cannot scale unless you have a relatively small team with a shared cultural understanding, able to to sit down in the same room.</p>\n\n<p>If anyone can write a tool and the main instruction comes from &quot;the unix philosophy&quot;, people will have different opinions about what &quot;<a href=\"https://twitter.com/hillelogram/status/1174714902151421952\">simplicity</a>&quot; or &quot;doing one thing&quot;<sup class=\"footnote-ref\" id=\"fnref:P\"><a rel=\"footnote\" href=\"#fn:P\">5</a></sup> means, what the right way to do things is, and inconsistency will bloom, resulting in the kind of complexity you get when dealing with a wildly inconsistent language, like PHP. People make fun of PHP and javascript for having all sorts of warts and weird inconsistencies, but as a language and a standard library, any commonly used shell plus the collection of widely used *nix tools taken together is much worse and contains much more accidental complexity due to inconsistency even within a single Linux distro and there's no other way it could have turned out. If you compare across Linux distros, BSDs, Solaris, AIX, etc.,  the amount of accidental complexity that users have to hold in their heads when switching systems dwarfs PHP or javascript's incoherence. The most widely mocked programming languages are paragons of great design by comparison.</p>\n\n<p><a id=\"maven\"></a>To be clear, I'm not saying that I or anyone else could have done better with the knowledge available in the 70s in terms of making a system that was practically useful at the time that would be elegant today. It's easy to look back and find issues with the benefit of hindsight. What I disagree with are comments from Unix mavens speaking today; comments like McIlroy's, which imply that we just forgot or don't understand the value of simplicity, or <a href=\"https://twitter.com/danluu/status/885214004649615360\">Ken Thompson saying that C is as safe a language as any and if we don't want bugs we should just write bug-free code</a>. These kinds of comments imply that there's not much to learn from hindsight; in the 70s, we were building systems as effectively as anyone can today; five decades of collective experience, tens of millions of person-years, have taught us nothing; if we just go back to building systems like the original Unix mavens did, all will be well. I respectfully disagree.</p>\n\n<h3 id=\"appendix-memory\">Appendix: memory</h3>\n\n<p>Although addressing McIlroy's complaints about binary size bloat is a bit out of scope for this, I will note that, in 2017, I bought a Chromebook that had 16GB of RAM for $300. A 1 meg binary might have been a serious problem in 1979, when a standard Apple II had 4KB. An Apple II cost $1298 in 1979 dollars, or $4612 in 2020 dollars. You can get a low end Chromebook that costs less than 1/15th as much which has four million times more memory. Complaining that memory usage grew by a factor of one thousand when a (portable!) machine that's more than an order of magnitude cheaper has four million times more memory seems a bit ridiculous.</p>\n\n<p>I prefer slimmer software, which is why I optimized my home page down to two packets (it would be a single packet if my CDN served high-level brotli), but that's purely an aesthetic preference, something I do for fun. The bottleneck for command line tools isn't memory usage and spending time optimizing the memory footprint of a tool that takes one meg is like getting a homepage down to a single packet. Perhaps a fun hobby, but not something that anyone should prescribe.</p>\n\n<h3 id=\"methodology-for-table\">Methodology for table</h3>\n\n<p>Command frequencies were sourced from public command history files on github, not necessarily representative of your personal usage. Only &quot;simple&quot; commands were kept, which ruled out things like curl, git, gcc (which has &gt; 1000 options), and wget. What's considered simple is arbitrary. <a href=\"https://en.wikipedia.org/wiki/Shell_builtin\">Shell builtins</a>, like <code>cd</code> weren't included.</p>\n\n<p>Repeated options aren't counted as separate options. For example, <code>git blame -C</code>, <code>git blame -C -C</code>, and <code>git blame -C -C -C</code> have different behavior, but these would all be counted as a single argument even though <code>-C -C</code> is effectively a different argument from <code>-C</code>.</p>\n\n<p>The table counts sub-options as a single option. For example, <code>ls</code> has the following:</p>\n\n<blockquote>\n<p>--format=WORD\nacross -x, commas -m, horizontal -x, long -l, single-column -1, verbose -l, vertical -C</p>\n</blockquote>\n\n<p>Even though there are seven format options, this is considered to be only one option.</p>\n\n<p>Options that are explicitly listed as not doing anything are still counted as options, e.g., <code>ls -g</code>, which reads <code>Ignored; for Unix compatibility.</code> is counted as an option.</p>\n\n<p>Multiple versions of the same option are also considered to be one option. For example, with <code>ls</code>, <code>-A</code> and <code>--almost-all</code> are counted as a single option.</p>\n\n<p>In cases where the manpage says an option is supposed to exist, but doesn't, the option isn't counted. For example, the v7 <code>mv</code> manpage says</p>\n\n<blockquote>\n<p>BUGS</p>\n\n<p>If file1 and file2 lie on different file systems, mv must copy the file and deletethe original.  In this case the owner name becomes that of the copying process and any linking relationship with other files is lost.</p>\n\n<p>Mv should take <strong>-f</strong> flag, like rm, to suppress the question if the target exists and isnot writable.</p>\n</blockquote>\n\n<p><code>-f</code> isn't counted as a flag in the table because the option doesn't actually exist.</p>\n\n<p>The latest year in the table is 2017 because I wrote the first draft for this post in 2017 and didn't get around to cleaning it up until 2020.</p>\n\n<h3 id=\"related\">Related</h3>\n\n<p><a href=\"https://blog.plover.com/Unix/tools.html\">mjd on the Unix philosophy, with an aside into the mess of /usr/bin/time vs. built-in time</a>.</p>\n\n<p><a href=\"https://groups.google.com/forum/m/#!topic/rec.humor.funny/Q-HG4LpW564\">mjd making a joke about the proliferation of command line options in 1991</a>.</p>\n\n<p>On HN:</p>\n\n<blockquote>\n<blockquote>\n<p>p1mrx:</p>\n\n<p><a href=\"https://unix.stackexchange.com/q/112125/261842\">It's strange that ls has grown to 58 options, but still can't output \\0-terminated filenames</a></p>\n\n<p>As an exercise, try to sort a directory by size or date, and pass the result to xargs, while supporting any valid filename. I eventually just gave up and made my script ignore any filenames containing \\n.</p>\n</blockquote>\n\n<p>whelming_wave:</p>\n\n<p>Here you go: sort all files in the current directory by modification time, whitespace-in-filenames-safe.\nThe <code>printf (od -&gt; sed)' construction converts back out of null-separated characters into newline-separated, though feel free to replace that with anything accepting null-separated input. Granted,</code>sort --zero-terminated' is a GNU extension and kinda cheating, but it's even available on macOS so it's probably fine.</p>\n</blockquote>\n\n<pre><code>      printf '%b' $(\n        find . -maxdepth 1 -exec sh -c '\n          printf '\\''%s %s\\0'\\'' &quot;$(stat -f '\\''%m'\\'' &quot;$1&quot;)&quot; &quot;$1&quot;\n        ' sh {} \\; | \\\n        sort --zero-terminated | \\\n        od -v -b | \\\n        sed 's/^[^ ]*//\n      s/ *$//\n      s/  */ \\\\/g\n      s/\\\\000/\\\\012/g')\n</code></pre>\n\n<blockquote>\n<p>If you're running this under zsh, you'll need to prefix it with `command' to use the system executable: zsh's builtin printf doesn't support printing octal escape codes for normally printable characters, and you may have to assign the output to a variable and explicitly word-split it.</p>\n\n<p>This is all POSIX as far as I know, except for the sort.</p>\n</blockquote>\n\n<p><a href=\"https://en.wikipedia.org/wiki/The_Unix-Haters_Handbook\">The Unix haters handbook</a>.</p>\n\n<p><a href=\"http://www.oilshell.org/blog/2018/01/28.html\">Why create a new shell</a>?</p>\n\n<p><small>\nThanks to Leah Hanson, Hillel Wayne, Wesley Aptekar-Cassels, Mark Jason Dominus, Travis Downs, and Yuri Vishnevsky for comments/corrections/discussion.\n</small></p>\n\n<p><link rel=\"prefetch\" href=\"https://danluu.com/mcilroy-unix/\">\n<link rel=\"prefetch\" href=\"https://danluu.com/\">\n<link rel=\"prefetch\" href=\"https://danluu.com/discontinuities/\">\n<link rel=\"prefetch\" href=\"https://danluu.com/about/\"></p>\n<div class=\"footnotes\">\n\n<hr />\n\n<ol>\n<li id=\"fn:M\">This quote is slightly different than the version I've seen everywhere because I watched <a href=\"https://archive.org/details/DougMcIlroy_AncestryOfLinux_DLSLUG\">the source video</a>. AFAICT, every copy of this quote that's on the internet (indexed by Bing, DuckDuckGo, or Google) is a copy of one person's transcription of the quote. There's some ambiguity because the audio is low quality and I hear something a bit different than whoever transcribed that quote heard.\n <a class=\"footnote-return\" href=\"#fnref:M\"><sup>[return]</sup></a></li>\n<li id=\"fn:T\">Another example of something where the user absorbs the complexity because different commands handle formatting differently is <a href=\"https://blog.plover.com/Unix/tools.html\">time formatting</a>  the shell builtin <code>time</code> is, of course, inconsistent with <code>/usr/bin/time</code> and the user is expected to know this and know how to handle it.\n <a class=\"footnote-return\" href=\"#fnref:T\"><sup>[return]</sup></a></li>\n\n<li id=\"fn:W\"><p>Just for example, you can use <code>ConvertTo-Json</code> or <code>ConvertTo-CSV</code> on any object, you <a href=\"https://docs.microsoft.com/en-us/powershell/scripting/samples/using-format-commands-to-change-output-view\">can use cmdlets to change how properties are displayed for objects</a>, and you can write formatting configuration files that define how you prefer things to be formatted.</p>\n\n<p>Another way to look at this is through the lens of <a href=\"https://en.wikipedia.org/wiki/Conway's_law\">Conway's law</a>. If we have a set of command line tools that are built by different people, often not organizationally connected, the tools are going to be wildly inconsistent unless someone can define a standard and get people to adopt it. This actually works relatively well on Windows, and not just in PowerShell.</p>\n\n<p>A common complaint about Microsoft is that they've created massive API churn, often for non-technical organizational reasons (e.g., a Sinofsky power play, like the one described in the replies to the now-deleted Tweet at <a href=\"https://twitter.com/stevesi/status/733654590034300929\">https://twitter.com/stevesi/status/733654590034300929</a>). It's true. Even so, from the standpoint of a naive user, off-the-shelf Windows software is generally a lot better at passing non-textual data around than *nix. One thing this falls out of is Windows's embracing of non-textual data, which goes back at least to <a href=\"https://en.wikipedia.org/wiki/Component_Object_Model\">COM</a> in 1999 (and arguably OLE and DDE, released in 1990 and 1987, respectively).</p>\n\n<p>For example, if you copy from Foo, which supports binary formats <code>A</code> and <code>B</code>, into Bar, which supports formats <code>B</code> and <code>C</code> and you then copy from Bar into Baz, which supports <code>C</code> and <code>D</code>, this will work even though Foo and Baz have no commonly supported formats.</p>\n\n<p>When you cut/copy something, the application basically &quot;tells&quot; the clipboard what formats it could provide data in. When you paste into the application, the destination application can request the data in any of the formats in which it's available. If the data is already in the clipboard, &quot;Windows&quot; provides it. If it isn't, Windows gets the data from the source application and then gives to the destination application and a copy is saved for some length of time in Windows. If you &quot;cut&quot; from Excel it will tell &quot;you&quot; that it has the data available in many tens of formats. This kind of system is pretty good for compatibility, although it definitely isn't simple or minimal.</p>\n\n<p>In addition to nicely supporting many different formats and doing so for long enough that a lot of software plays nicely with this, Windows also generally has nicer clipboard support out of the box.</p>\n\n<p>Let's say you copy and then paste a small amount of text. Most of the time, this will work like you'd expect on both Windows and Linux. But now let's say you copy some text, close the program you copied from, and then paste it. A mental model that a lot of people have is that when they copy, the data is stored in the clipboard, not in the program being copied from. On Windows, software is typically written to conform to this expectation (although, technically, users of the clipboard API don't have to do this). This is less common on Linux with X, where the correct mental model for most software is that copying stores a pointer to the data, which is still owned by the program the data was copied from, which means that paste won't work if the program is closed. When I've (informally) surveyed programmers, they're usually surprised by this if they haven't actually done copy+paste related work for an application. When I've surveyed non-programmers, they tend to find the behavior to be confusing as well as surprising.</p>\n\n<p>The downside of having the OS effectively own the contents of the clipboard is that it's expensive to copy large amounts of data. Let's say you copy a really large amount of text, many gigabytes, or some complex object and then never paste it. You don't really want to copy that data from your program into the OS so that it can be available. Windows also handles this reasonably: applications can <a href=\"https://docs.microsoft.com/en-us/openspecs/windows_protocols/ms-rdpeclip/fa309d1b-8034-44bf-b927-adfc753e69c1\">provide data only on request</a> when that's deemed advantageous. In the case mentioned above, when someone closes the program, the program can decide whether or not it should push that data into the clipboard or discard it. In that circumstance, a lot of software (e.g., Excel) will prompt to &quot;keep&quot; the data in the clipboard or discard it, which is pretty reasonable.</p>\n\n<p>It's not impossible to support some of this on Linux. For example, <a href=\"https://freedesktop.org/wiki/ClipboardManager/\">the ClipboardManager spec</a> describes a persistence mechanism and GNOME applications generally kind of sort of support it (although <a href=\"https://bugzilla.gnome.org/show_bug.cgi?id=510204#c8\">there are some bugs</a>) but the situation on *nix is really different from the more pervasive support Windows applications tend to have for nice clipboard behavior.</p>\n <a class=\"footnote-return\" href=\"#fnref:W\"><sup>[return]</sup></a></li>\n\n<li id=\"fn:C\"><p>Another example of this are tools that are available on top of modern compilers. If we go back and look at McIlroy's canonical example, how proper UNIX compilers are so specialized that listings are a separate tool, we can see that this has changed even if there's still a separate tool you can use for listings. Some commonly used Linux compilers have literally thousands of options and do many things. For example, one of the many things <code>clang</code> now does is static analysis. As of this writing, <a href=\"https://clang.llvm.org/docs/analyzer/checkers.html#default-checkers\">there are 79 normal static analysis checks and 44 experimental checks</a>. If these were separate commands (perhaps individual commands or perhaps a <code>static_analysis</code> command, they'd still rely on the same underlying compiler infrastructure and impose the same maintenance burden  it's not really reasonable to have these static analysis tools operate on plain text and reimplement the entire compiler toolchain necessary to get the point where they can do static analysis. They could be separate commands instead of bundled into <code>clang</code>, but they'd still take a dependency on the same machinery that's used for the compiler and either impose a maintenance and complexity burden on the compiler (which has to support non-breaking interfaces for the tools built on top) or they'd break all the time.</p>\n\n<p>Just make everything text so that it's simple makes for a nice soundbite, but in reality the textual representation of the data is often not what you want if you want to do actually useful work.</p>\n\n<p>And on clang in particular, whether you make it a monolithic command or thousands of smaller commands, clang simply does more than any compiler that existed in 1979 or even all compilers that existed in 1979 combined. It's easy to say that things were simpler in 1979 and that us modern programmers have lost our way. It's harder to actually propose a design that's actually much simpler and could really get adopted. It's impossible that such a design could maintain all of the existing functionality and configurability and be as simple as something from 1979.</p>\n <a class=\"footnote-return\" href=\"#fnref:C\"><sup>[return]</sup></a></li>\n<li id=\"fn:P\">Since its inception, curl has gone from supporting 3 protocols to 40. Does that mean it does 40 things and it would be more &quot;UNIX-y&quot; to split it up into 40 separate commands? Depends on who you ask. If each protocol were its own command, created and maintained by a different person, we'd be in the same situation we are with other commands. Inconsistent command line options, inconsistent output formats despite it all being text streams, etc. Would that be closer to the simplicity McIlroy advocates for? Depends on who you ask.\n <a class=\"footnote-return\" href=\"#fnref:P\"><sup>[return]</sup></a></li>\n</ol>\n</div>\n"]}]}]}]}